from pathlib import Path
from datetime import datetime
from tqdm import tqdm
from typing import List, Dict, Optional, Union
import numpy as np
import os
import sys
import torch
from torch.utils.tensorboard import SummaryWriter
from pogema import AStarAgent
project_root = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train"
if project_root not in sys.path:
    sys.path.insert(0, project_root)
from g2rl.environment import G2RLEnv
from g2rl.agent import DDQNAgent
from g2rl.network import CRNNModel
from g2rl import moving_cost, detour_percentage
from g2rl.curriculum import CurriculumScheduler
import time
from pathlib import Path
from collections import deque
import pandas as pd

MAP_SETTINGS_PATH = 'C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/g2rl/map_settings_generated_new.yaml'

import yaml


from g2rl.map_settings import predict_complexity  

# æ–°å¢
import argparse
import csv
# é¡¶éƒ¨è¡¥å……ï¼š
import inspect
import numpy as np
import yaml

# ...ï¼ˆä½ çš„å…¶ä»– import ä¿æŒä¸å˜ï¼‰

# è¯»å– YAMLï¼ˆä¿æŒä½ åŸæ¥çš„è·¯å¾„å˜é‡ï¼‰
# è¯»å– YAMLï¼ˆä¿æŒä½ åŸæœ‰çš„ MAP_SETTINGS_PATHï¼‰
with open(MAP_SETTINGS_PATH, "r", encoding="utf-8") as f:
    base_map_settings = yaml.safe_load(f)

# å…¼å®¹é¡¶å±‚ä¸º list çš„æƒ…å†µï¼šç”¨ name åš key è½¬æˆ dict
if isinstance(base_map_settings, list):
    base_map_settings = {
        (m.get("name") or f"map_{i}"): m
        for i, m in enumerate(base_map_settings)
    }

# å¦‚æœä½ å·²æœ‰ map_names çš„ç­›é€‰é€»è¾‘ï¼Œä¿ç•™ä½ çš„ï¼›å¦åˆ™ç”¨å…¨éƒ¨
map_names = list(base_map_settings.keys())

# ====== å…³é”®ä¿®æ­£ï¼šè¿‡æ»¤æ‰ G2RLEnv ä¸è®¤è¯†çš„å‚æ•° ======
import inspect
import numpy as np

# å– G2RLEnv.__init__ æ”¯æŒçš„å‚æ•°åï¼ˆæˆ–ä½ ä¹Ÿå¯ä»¥æ‰‹å†™ç™½åå•ï¼‰
_sig = inspect.signature(G2RLEnv.__init__)
_ctor_params = {
    p.name for p in _sig.parameters.values()
    if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY)
}
_ctor_params.discard("self")

# è‹¥ YAML é”®åä¸æ„é€ å‡½æ•°ä¸ä¸€è‡´ï¼Œå¯åœ¨è¿™é‡Œåšæ˜ å°„
# ä¾‹å¦‚ï¼šYAML ç”¨ 'size'ï¼Œæ„é€ å‡½æ•°å« 'map_size'ï¼Œå°±å†™ {'size':'map_size'}
_rename = {
    # 'size': 'map_size',
    # 'num_agents': 'n_agents',
}

def _build_ctor_cfg(raw: dict) -> dict:
    tmp = { _rename.get(k, k): v for k, v in raw.items() }
    return { k: v for k, v in tmp.items() if k in _ctor_params }

maps = []
for name in map_names:
    raw = base_map_settings[name]

    # 1) åªä¼  __init__ è®¤è¯†çš„é”®
    ctor_cfg = _build_ctor_cfg(raw)
    env = G2RLEnv(**ctor_cfg)

    # 2) å°† grid/starts/goals ç­‰â€œé¢å¤–ä¿¡æ¯â€ä½œä¸ºå±æ€§æŒ‚åœ¨ env ä¸Šï¼ˆä¸ä¼šä¼ ç»™ __init__ï¼‰
    if "grid" in raw:
        try:
            env.grid = (np.array(raw["grid"]) > 0).astype(np.uint8)
        except Exception:
            env.grid = None
    if "starts" in raw:
        env.starts = raw["starts"]  # list[list[int,int]] æˆ– None
    if "goals" in raw:
        env.goals = raw["goals"]

    maps.append(env)
# ====== ä¿®æ­£ç»“æŸ ======

# ===== Complexity-based Curriculum Scheduler =====
from typing import Iterable
from g2rl.complexity import compute_complexity
import numpy as np
import yaml, math, random

# --- ä½ çš„çº¿æ€§å…¬å¼ ---
INTERCEPT = 0.848
WEIGHTS = {
    'Size': 0.021,
    'Agents': -0.010,
    'Density': 0.077,
    'Density_actual': 0.132,
    'LDD': 0.027,
    'BN': -0.128,
    'MC': 0.039,
    'DLR': 0.002,
}
FEATURE_MEAN_STD = None  # è‹¥è®­ç»ƒæ—¶åšè¿‡æ ‡å‡†åŒ–ï¼ŒæŒ‰ {'Size':(mu, sigma), ...} ä¼ å…¥

# def _compute_complexities_for_settings(base_map_settings: Dict[str, dict],
#                                        size_mode: str = "max") -> pd.DataFrame:
#     """å¯¹å• YAML å†…æ‰€æœ‰åœ°å›¾é¡¹é€ä¸ªè®¡ç®— complexityï¼Œè¿”å› DataFrame."""
#     rows = []
#     for name, spec in base_map_settings.items():
#         try:
#             cpx, used, raw = compute_complexity(
#                 spec, intercept=INTERCEPT, weights=WEIGHTS,
#                 feature_mean_std=FEATURE_MEAN_STD, size_mode=size_mode
#             )
#             rows.append({
#                 "name": name,
#                 "complexity": float(cpx),
#                 "spec": spec,
#             })
#         except Exception as e:
#             rows.append({"name": name, "error": str(e), "spec": spec})
#     df = pd.DataFrame(rows)
#     if "error" in df.columns:
#         df = df[df["error"].isna()]
#     return df.sort_values("complexity").reset_index(drop=True)

def _build_stages_by_quantile_df(df: pd.DataFrame, n_stages: int = 5, min_per_stage: int = 5):
    if len(df) == 0:
        raise ValueError("æ²¡æœ‰å¯ç”¨åœ°å›¾ç”¨äº complexity è¯¾ç¨‹ã€‚")
    qs = np.linspace(0, 1, n_stages + 1)
    edges = np.quantile(df["complexity"].values, qs)
    stages = []
    for i in range(n_stages):
        lo, hi = float(edges[i]), float(edges[i+1]) + 1e-12
        sub = df[(df["complexity"] >= lo) & (df["complexity"] < hi)]
        if len(sub) < min_per_stage:
            need = min_per_stage - len(sub)
            center = (lo + hi) / 2.0
            extra = df.iloc[(df["complexity"] - center).abs().argsort()[:need]]
            sub = pd.concat([sub, extra]).drop_duplicates(subset=["name"])
        stages.append({
            "stage": i,
            "cpx_min": lo,
            "cpx_max": hi,
            "items": sub.to_dict("records"),  # æ¯æ¡å« name/spec/complexity
        })
    return stages


# ======== Robust complexity building (single source of truth) ========
import math, random, numpy as np, pandas as pd
from typing import Dict, Tuple

# ç»Ÿä¸€é”®åæ˜ å°„ï¼ˆYAMLé‡Œå¸¸è§å¤§å°å†™/åˆ«åï¼‰
_KEY_RENAME = {
    "map_size": "size",
    "Size": "size",
    "Agents": "num_agents",
    "n_agents": "num_agents",
    "N_agents": "num_agents",
    "Density": "density",
    "obs_density": "density",
}

def _normalize_spec(raw: dict) -> Tuple[dict, list]:
    """æŠŠä¸€æ¡ map spec ç»Ÿä¸€æˆ {size, num_agents, density, ...} å¹¶è¿”å›(è§„èŒƒåŒ–spec, ç¼ºå¤±å­—æ®µåˆ—è¡¨)"""
    spec = {}
    missing = []
    # å…ˆ rename
    for k, v in raw.items():
        spec[_KEY_RENAME.get(k, k)] = v
    # å¿…è¦å­—æ®µ
    for k in ("size", "num_agents", "density"):
        if k not in spec:
            missing.append(k)
    return spec, missing

def _fallback_complexity(spec: dict) -> float:
    """compute_complexity å¤±è´¥æ—¶çš„åå¤‡ç®€å¼ï¼Œè‡³å°‘ä¿è¯æœ‰æ•°"""
    size = int(spec.get("size", 8))
    na   = int(spec.get("num_agents", 2))
    dens = float(spec.get("density", 0.10))
    # ä½ å¯ä»¥æ”¹è¿™å…¬å¼ï¼›å…ˆç”¨ä¸€ä¸ªæ¸©å’Œä¸Šå‡çš„åŸºçº¿
    return 0.45 * (na / max(1, size)) + 0.45 * dens + 0.10 * math.log2(max(2, size))

def _compute_complexities_for_settings(base_map_settings: Dict[str, dict],
                                       size_mode: str = "max") -> pd.DataFrame:
    rows, err_rows = [], []
    for name, raw in base_map_settings.items():
        spec, missing = _normalize_spec(raw if isinstance(raw, dict) else {})
        if missing:
            err_rows.append((name, f"missing keys: {missing}"))
            # ä»ç„¶ç”¨ fallback è®¡ç®—ï¼Œé¿å…æ•´è¡¨æ—  complexity
            cpx = _fallback_complexity(spec)
            rows.append({"name": name, "complexity": float(cpx), "spec": spec, "note": "fallback_missing"})
            continue

        # ä¼˜å…ˆå°è¯•ä½ çš„ learn-based å¤æ‚åº¦
        try:
            # æ³¨æ„ï¼šcompute_complexity çš„ spec éœ€è¦æ˜¯â€œä½ å®ç°æ‰€éœ€çš„é”®â€ï¼Œ
            # è¿™é‡Œä¼ è§„èŒƒåŒ–åçš„ specï¼›å¦‚å®ƒè¿˜éœ€è¦å…¶ä»–å­—æ®µï¼Œè¯·åœ¨ _normalize_spec é‡Œè¡¥é½/rename
            cpx, used, raw_feat = compute_complexity(
                spec, intercept=INTERCEPT, weights=WEIGHTS,
                feature_mean_std=FEATURE_MEAN_STD, size_mode=size_mode
            )
            rows.append({"name": name, "complexity": float(cpx), "spec": spec})
        except Exception as e:
            # å›é€€åˆ°ç®€å¼ï¼Œè®°å½•é”™è¯¯åŸå› 
            err_rows.append((name, str(e)))
            cpx = _fallback_complexity(spec)
            rows.append({"name": name, "complexity": float(cpx), "spec": spec, "note": "fallback_error"})

    df = pd.DataFrame(rows)

    # æœ€ç»ˆä¿é™©ï¼šå¦‚æœä¾ç„¶æ²¡æœ‰ 'complexity' åˆ—ï¼Œç›´æ¥æŠ¥å¸¦é¢„è§ˆçš„é”™
    if "complexity" not in df.columns:
        raise RuntimeError(f"No 'complexity' column produced. Preview:\n{df.head()}")

    # æ¸…æ´—éæ³•å€¼
    df = df.dropna(subset=["complexity"])
    df = df[np.isfinite(df["complexity"])].copy()
    df = df.sort_values("complexity").reset_index(drop=True)

    # æ‰“å°é”™è¯¯ç»Ÿè®¡ï¼Œå¸®åŠ©ä½ æ’æŸ¥ä¸ºä½•ä½¿ç”¨äº† fallback
    if err_rows:
        print("âš ï¸  compute_complexity() errors (fallback used):")
        for nm, msg in err_rows[:10]:
            print(f"  - {nm}: {msg}")
        if len(err_rows) > 10:
            print(f"  ... and {len(err_rows)-10} more.")

    print(f"âœ… Complexity DF ready: {len(df)} maps, columns={list(df.columns)}")
    return df
# ======== end robust builder ========


class ComplexityScheduler:
    """
    ä¸ä½  train() ä¸­ä½¿ç”¨çš„ scheduler æ¥å£å…¼å®¹ï¼š
      - properties: current_stage, max_stage, episodes_per_stage, threshold
      - methods: get_updated_map_settings(), add_episode_result(x), ready_to_advance(),
                 advance(pbar), repeat_stage(pbar), is_done(), current_window_sr()
    """
    def __init__(self,
                 base_map_settings: Dict[str, dict],
                 n_stages: int = 5,
                 min_per_stage: int = 5,
                 episodes_per_stage: int = 100,
                 threshold: float = 0.70,
                 window_size: int = 100,
                 shuffle_each_stage: bool = True,
                 seed: int = 0,
                 size_mode: str = "max"):
        self.episodes_per_stage = episodes_per_stage
        self.threshold = threshold
        self.window_size = window_size

        self._rng = random.Random(seed)
        df = _compute_complexities_for_settings(base_map_settings, size_mode=size_mode)
        stages = _build_stages_by_quantile_df(df, n_stages=n_stages, min_per_stage=min_per_stage)

        # build iterable per stage
        self._stage_items = []
        for st in stages:
            items = list(st["items"])
            if shuffle_each_stage:
                self._rng.shuffle(items)
            self._stage_items.append(items)

        self._stage_edges = [(st["cpx_min"], st["cpx_max"]) for st in stages]
        self.current_stage = 0
        self.max_stage = len(self._stage_items) - 1

        self._idx_in_stage = 0
        self._win = deque(maxlen=self.window_size)
        self._stage_ep_cnt = 0
        self._stage_succ_cnt = 0

    def get_updated_map_settings(self) -> Dict[str, dict]:
        """è¿”å› {map_name: spec}ï¼ˆä½  train() é‡ŒåŸæ ·æ¥æ”¶ï¼‰"""
        if self.current_stage > self.max_stage:
            return {}
        items = self._stage_items[self.current_stage]
        if not items:
            raise RuntimeError(f"Stage {self.current_stage} æ²¡æœ‰åœ°å›¾ã€‚")
        item = items[self._idx_in_stage]
        self._idx_in_stage = (self._idx_in_stage + 1) % len(items)
        return {item["name"]: item["spec"]}

    def add_episode_result(self, success: int):
        self._win.append(1 if success else 0)
        self._stage_ep_cnt += 1
        if success:
            self._stage_succ_cnt += 1

    def current_window_sr(self) -> float:
        if len(self._win) == 0:
            return 0.0
        return float(sum(self._win) / len(self._win))

    def ready_to_advance(self) -> bool:
        # å¯é€‰ï¼šç”¨æ»‘çª—ç›´æ¥åˆ¤å®š
        return self.current_window_sr() >= self.threshold and self._stage_ep_cnt >= self.episodes_per_stage//2

    def advance(self, pbar=None):
        if pbar:
            pbar.write(f"âœ… è¿›å…¥ä¸‹ä¸€é˜¶æ®µï¼š{self.current_stage} -> {self.current_stage + 1}")
        self.current_stage += 1
        self._idx_in_stage = 0
        self._win.clear()
        self._stage_ep_cnt = 0
        self._stage_succ_cnt = 0

    def repeat_stage(self, pbar=None):
        if pbar:
            pbar.write(f"ğŸ” é˜¶æ®µ {self.current_stage} æœªè¾¾æ ‡ï¼Œé‡å¤è®­ç»ƒè¯¥é˜¶æ®µ")
        # é‡ç½®è®¡æ•°ï¼Œä½†ä¸å˜æ›´é˜¶æ®µ
        self._idx_in_stage = 0
        self._win.clear()
        self._stage_ep_cnt = 0
        self._stage_succ_cnt = 0

    def is_done(self) -> bool:
        return self.current_stage > self.max_stage

# åç»­é€»è¾‘ä¿æŒä¸å˜ï¼ˆæŠŠ maps ä¼ ç»™ä½ çš„è®­ç»ƒ/è°ƒåº¦æµç¨‹ï¼‰

def append_result_row(csv_path, row_dict, header_order):
    file_exists = os.path.exists(csv_path)
    with open(csv_path, "a", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=header_order)
        if not file_exists:
            writer.writeheader()
        writer.writerow({k: row_dict.get(k, "") for k in header_order})


import pandas as pd
os.makedirs("plots", exist_ok=True)

# è®¾ç½®æ­£ç¡®è·¯å¾„
correct_path = "C:/Users/MSc_SEIoT_1/MAPF_G2RL-main"
if correct_path not in sys.path:
    sys.path.insert(0, correct_path)

# ç¡®è®¤å¯¼å…¥çš„æ˜¯ä½ ä¿®æ”¹çš„æ–‡ä»¶

# with open("g2rl/map_settings_generated.yaml", "r", encoding="utf-8") as f:
#     base_map_settings = yaml.safe_load(f)
# if isinstance(base_map_settings, list):
#     base_map_settings = {
#         (m.get("name") or f"map_{i}"): m
#         for i, m in enumerate(base_map_settings)
#     }
# scheduler = CurriculumScheduler(
#     base_map_settings=base_map_settings,
#     agent_range=[4, 6, 8, 16],  # æ¯ä¸ªé˜¶æ®µçš„ agent æ•°é‡
#     episodes_per_stage=100,     # æ¯ 100 episodes æå‡ä¸€é˜¶æ®µ
# )

# è¯» YAMLï¼ˆä½ å·²æœ‰ï¼‰
with open(MAP_SETTINGS_PATH, "r", encoding="utf-8") as f:
    base_map_settings = yaml.safe_load(f)

# é¡¶å±‚æ˜¯ list çš„æƒ…å†µè½¬æˆ {name: spec}
if isinstance(base_map_settings, list):
    base_map_settings = { (m.get("name") or f"map_{i}"): m for i, m in enumerate(base_map_settings) }

# ç”¨ complexity åˆ’åˆ†è¯¾ç¨‹
scheduler = ComplexityScheduler(
    base_map_settings=base_map_settings,
    n_stages=5,
    min_per_stage=10,
    episodes_per_stage=100,
    threshold=0.70,
    window_size=100,
    shuffle_each_stage=True,
    seed=0,
    size_mode="max",  # æ³¨æ„ä¸è®­ç»ƒæ—¶çš„ Size å®šä¹‰ä¸€è‡´
)







def evaluate_model_inline(model, env_config, num_episodes=20, device='cuda'):
    from pogema import AStarAgent
    from g2rl.environment import G2RLEnv
    from g2rl.agent import DDQNAgent
    from g2rl import moving_cost, detour_percentage

    model.eval()
    env = G2RLEnv(**env_config)
    num_actions = len(env.get_action_space())

    agent = DDQNAgent(model=model, action_space=list(range(num_actions)), device=device)

    success_rates, moving_costs, detour_rates = [], [], []

    for episode in range(num_episodes):
        obs, _ = env.reset()
        target_idx = np.random.randint(env.num_agents)
        goal = tuple(env.goals[target_idx])
        state = obs[target_idx]
        opt_path = [state['global_xy']] + env.global_guidance[target_idx]
        agents = [agent if i == target_idx else AStarAgent() for i in range(env.num_agents)]

        success = False
        for t in range(50 + 10 * episode):
            actions = [a.act(o) for a, o in zip(agents, obs)]
            obs, _, terminated, _, _ = env.step(actions)
            pos = tuple(obs[target_idx]['global_xy'])
            if pos == goal:
                success = True
                moving_costs.append(moving_cost(t + 1, opt_path[0], opt_path[-1]))
                detour_rates.append(detour_percentage(t + 1, len(opt_path) - 1))
                break

        success_rates.append(1 if success else 0)

    return {
        'Success Rate': np.mean(success_rates),
        'Avg Moving Cost': np.mean(moving_costs) if moving_costs else float('inf'),
        'Avg Detour %': np.mean(detour_rates) if detour_rates else float('inf'),
    }


import matplotlib.pyplot as plt

def plot_training_results(logs, save_path=None):
    episodes = [log['Episode'] for log in logs]
    success = [log['Success'] for log in logs]
    moving_cost = [log['Moving Cost'] for log in logs]
    detour = [log['Detour Percentage'] for log in logs]
    loss = [log['Average Loss'] for log in logs]
    epsilon = [log['Average Epsilon'] for log in logs]

    plt.figure(figsize=(12, 8))

    plt.subplot(2, 2, 1)
    plt.plot(episodes, success, label='Success')
    plt.title("Success per Episode")
    plt.xlabel("Episode")
    plt.ylabel("Success")
    plt.grid(True)

    plt.subplot(2, 2, 2)
    plt.plot(episodes, moving_cost, label='Moving Cost')
    plt.title("Moving Cost")
    plt.xlabel("Episode")
    plt.ylabel("Steps")
    plt.grid(True)

    plt.subplot(2, 2, 3)
    plt.plot(episodes, detour, label='Detour %')
    plt.title("Detour Percentage")
    plt.xlabel("Episode")
    plt.ylabel("Detour %")
    plt.grid(True)

    plt.subplot(2, 2, 4)
    plt.plot(episodes, loss, label='Loss')
    plt.plot(episodes, epsilon, label='Epsilon')
    plt.title("Loss & Epsilon")
    plt.xlabel("Episode")
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path)
        print(f"ğŸ“ˆ å›¾åƒä¿å­˜è‡³ï¼š{save_path}")
    plt.show()


def get_timestamp() -> str:
    now = datetime.now()
    timestamp = now.strftime('%H-%M-%d-%m-%Y')
    return timestamp


def get_normalized_probs(x: Union[List[float], None], size: int) -> np.ndarray:
    x = [1] * size if x is None else x + [0] * (size - len(x))
    e_x = np.exp(x - np.max(x))
    return e_x / e_x.sum(axis=0)


def train(
        # model: torch.nn.Module,
        # scheduler: CurriculumScheduler,
        # map_settings: Dict[str, dict],
        # map_probs: Union[List[float], None],
        # num_episodes: int = 300,
        # batch_size: int = 32,
        # decay_range: int = 1000,
        # log_dir = 'logs',
        # lr: float = 0.001,
        # replay_buffer_size: int = 1000,
        # device: str = 'cuda'
        model: torch.nn.Module,
        map_settings: Dict[str, dict],
        map_probs: Union[List[float], None],
        num_episodes: int = 300,
        batch_size: int = 32,
        decay_range: int = 1000,
        log_dir='logs',
        lr: float = 0.001,
        replay_buffer_size: int = 1000,
        device: str = 'cuda',
        scheduler: Optional[CurriculumScheduler] = None,  # â† å¯é€‰+é»˜è®¤
        max_episode_seconds: int = 30
    ) -> DDQNAgent:
    timestamp = get_timestamp()
    writer = SummaryWriter(log_dir=Path(log_dir) / timestamp)
    # maps = [G2RLEnv(**args) for _, args in map_settings.items()]
    # maps = [G2RLEnv(**map_settings[name]) for name in map_settings]
    maps = []
    
    for name in map_settings:
        env = G2RLEnv(**map_settings[name])
        # print(f"âœ… åˆå§‹åŒ– envï¼š{env}")
        # print(f"âœ… env.reset: {env.reset}")
        maps.append(env)
    # map_probs = get_normalized_probs(map_probs, len(maps))
    map_probs = map_probs or [1.0 / len(maps)] * len(maps)
    agent = DDQNAgent(
        model,
        maps[0].get_action_space(),
        lr=lr,
        decay_range=decay_range,
        device=device,
        replay_buffer_size=replay_buffer_size,
    )

    pbar = tqdm(range(num_episodes), desc='Episodes', dynamic_ncols=True)
    
    
    # Curriculum Learning é˜¶æ®µç»Ÿè®¡å™¨
    episode = 0
    success_count = 0
    stage_success_count = 0
    stage_episode_count = 0

    # è®°å½•æ¯ä¸ªé˜¶æ®µçš„è¯„ä¼°ç»“æœ


    episode_logs = []  # ğŸ‘ˆ æ”¾åœ¨ train() æœ€å‰é¢



    # for episode in pbar:
    while scheduler.current_stage <= scheduler.max_stage:
        # scheduler.step(episode)


        # æ‰‹åŠ¨åœ°å›¾é€‰æ‹©ï¼ˆåªè®­ç»ƒè¯¥åœ°å›¾ï¼‰
        # if episode == 0:
        # # âœ… åªåœ¨é¦–ä¸ª episode é€‰æ‹©åœ°å›¾
        #     map_settings = scheduler.get_updated_map_settings()
        #     pbar.write(f"ğŸ“¶ å½“å‰è®­ç»ƒé˜¶æ®µï¼šStage {scheduler.current_stage}")
        #     env, map_type = G2RLEnv.select_map_env(map_settings)
        #     pbar.write(f"ğŸŸ¢ é¦–æ¬¡é€‰æ‹©åœ°å›¾ï¼š{map_type}")
        #     pbar.write(f"ğŸ‘¥ å½“å‰ Agent æ•°é‡ï¼š{env.num_agents}")
            

        # else:
        #     pbar.write(f"ğŸ” ç»§ç»­ä½¿ç”¨åœ°å›¾ï¼š{map_type}")
        map_settings = map_settings
        map_type, cfg = next(iter(map_settings.items()))   # ç›´æ¥å–ç¬¬ä¸€é¡¹
        env = G2RLEnv(**cfg)
        pbar.write(f"ğŸŸ¢ ä½¿ç”¨åœ°å›¾ï¼š{map_type}")
        maps = [env]
        # torch.save(model.state_dict(), f'models/{timestamp}.pt')
        stage = scheduler.current_stage
        num_agents = env.num_agents  # å½“å‰ç¯å¢ƒä¸­çš„ agent æ•°é‡
        model_name = f"models/g2rl_cl_stage{stage}_agents{num_agents}.pt"
        torch.save(model.state_dict(), model_name)
        # env, map_type = G2RLEnv.select_map_env(map_settings)
        # print(f"[DEBUG] env ç±»å‹: {type(env)}")
        # print(f"[DEBUG] env.reset å‡½æ•°ï¼š{env.reset}")
        # print(f"âœ… å½“å‰ environment.py è·¯å¾„: {G2RLEnv.__module__} æ¥è‡ª {__import__('g2rl.environment').__file__}")
        
        obs, info = env.reset()
        pbar.write(f"ğŸ—ºï¸ Episode {episode} using map: {map_type}")
        
        # print("âœ… æ˜¯å¦æœ‰ goalsï¼Ÿ", hasattr(env, 'goals'))
        # print("âœ… env.goals:", getattr(env, 'goals', None))
        target_idx = np.random.randint(env.num_agents)
        agents = [agent if i == target_idx else AStarAgent() for i in range(env.num_agents)]
        goal = tuple(env.goals[target_idx]) 
        state = obs[target_idx]
        # opt_path = [state['global_xy']] + env.global_guidance[target_idx]
         # è·å–çœŸå®ç›®æ ‡
        opt_path = [state['global_xy']] + env.global_guidance[target_idx]

        success_flag = False
        
        retrain_count = 0
        scalars = {
            'Reward': 0,
            'Moving Cost': 0,
            'Detour Percentage': 0,
            'Average Loss': 0,
            'Average Epsilon': 0,
        }

        timesteps_per_episode = 50 + 10 * episode
        episode_start_time = time.time()
        for timestep in range(timesteps_per_episode):
            if time.time() - episode_start_time > 30:
                pbar.write(f"â° Episode {episode} è¶…æ—¶ï¼ˆ>30ç§’ï¼‰ï¼Œå¼ºåˆ¶ç»ˆæ­¢æœ¬ episode")
                break
            actions = [agent.act(ob) for agent, ob in zip(agents, obs)]
            obs, reward, terminated, truncated, info = env.step(actions)
            # terminated[target_idx] = obs[target_idx]['global_xy'] == opt_path[-1]
            agent_pos = tuple(obs[target_idx]['global_xy'])
            
            terminated[target_idx] = agent_pos == goal
            # print(f"[EP {episode}] Agent pos: {agent_pos}, goal: {goal}, success: {terminated[target_idx]}")


            # if the target agent has finished or FOV does not contain the global guidance

            if terminated[target_idx]:
                success_flag = True
                scalars['Success'] = 1
                scalars['Moving Cost'] = moving_cost(timestep + 1, opt_path[0], opt_path[-1])
                scalars['Detour Percentage'] = detour_percentage(timestep + 1, len(opt_path) - 1)
                break
            
            # æå–çŠ¶æ€ç‰¹å¾çŸ©é˜µ
            if isinstance(state, dict) and 'obs' in state:
                obs_dict = state['obs']
            else:
                obs_dict = state

# å¸¸è§è¾“å…¥æ˜¯ 'view_cache'
            if 'view_cache' in obs_dict:
                state_obs = obs_dict['view_cache']  # shape é€šå¸¸ä¸º [T, H, W, C]
            else:
                raise ValueError(f"âŒ state['obs'] ç¼ºå°‘ 'view_cache'ï¼Œç›®å‰å†…å®¹ä¸º: {obs_dict.keys()}")

# è½¬æ¢æˆ tensorï¼Œshape: [1, T, H, W, C] â†’ è¾“å…¥ CRNN
            state_tensor = torch.tensor(state_obs[..., :512], dtype=torch.float32).unsqueeze(0).to(device)

            agent.store(
                state,
                actions[target_idx],
                reward[target_idx],
                obs[target_idx],
                terminated[target_idx],
            )
            state = obs[target_idx]
            scalars['Reward'] += reward[target_idx]

            if len(agent.replay_buffer) >= batch_size:
                retrain_count += 1
                scalars['Average Loss'] += agent.retrain(batch_size)
                scalars['Average Epsilon'] += round(agent.epsilon, 4)

        
        # if success_flag:
        #     stage_success_count += scalars['Success']
        #     stage_episode_count += 1
        #     episode += 1
        # else:
        #     scalars['Success'] = 0  # âŒ æ²¡æœ‰æˆåŠŸ

        if not success_flag:
            scalars['Success'] = 0
        else:
            success_count += 1
            stage_success_count += 1

        stage_episode_count += 1
        episode += 1
        pbar.update(1)

        

        for name in scalars.keys():
            if 'Average' in name and retrain_count > 0:
                scalars[name] /= retrain_count

        # logging
        for name, value in scalars.items():
            writer.add_scalar(name, value, episode)
        # pbar.update(1)
        pbar.set_postfix(scalars)

        if scalars['Success'] == 1:
            pbar.write(f"[EP {episode}] âœ… æˆåŠŸï¼šAgent pos: {tuple(map(int, state['global_xy']))}, goal: {goal}")
        else:
            pbar.write(f"[EP {episode}] âŒ å¤±è´¥ï¼šAgent pos: {tuple(map(int, state['global_xy']))}, goal: {goal}")
        pbar.write(f"â­ å½“å‰ç´¯è®¡æˆåŠŸç‡ï¼š{success_count / (episode + 1) * 100:.2f}% ({success_count}/{episode + 1})")  #ä»è®­ç»ƒå¼€å§‹åˆ°å½“å‰ episode ä¸ºæ­¢çš„æ€»æˆåŠŸç‡ 
        # pbar.write(f"ğŸ“Š å½“å‰é˜¶æ®µæˆåŠŸç‡: {success_rate:.2f}")  #åªç»Ÿè®¡å½“å‰ Curriculum Learning é˜¶æ®µï¼ˆStageï¼‰ä¸­çš„æˆåŠŸç‡
        
                # âœ… è®°å½•å½“å‰ episode çš„è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
#         episode_logs.append({
#     'Episode': episode,
#     'Size': env.grid_config.size,
#     'Agents': env.num_agents,
#     'Density': env.grid_config.density,
#     'Success': scalars['Success'],
#     'Reward': scalars['Reward'],
#     'Moving Cost': scalars['Moving Cost'],
#     'Detour Percentage': scalars['Detour Percentage'],
#     'Average Loss': scalars['Average Loss'],
#     'Average Epsilon': scalars['Average Epsilon'],
# })
#         episode_logs.append({
#     'Episode': episode,
#     'Size': env.grid_config.size,
#     'Agents': env.num_agents,
#     'Density': env.grid_config.density,
#     'Success': scalars['Success'],
#     'Reward': scalars['Reward'],
#     'Moving Cost': scalars['Moving Cost'],
#     'Detour Percentage': scalars['Detour Percentage'],
#     'Average Loss': scalars['Average Loss'],
#     'Average Epsilon': scalars['Average Epsilon'],
#     'Complexity': predict_complexity(env)  # âœ… æ–°å¢
# })





        if stage_episode_count >= scheduler.episodes_per_stage:
            success_rate = stage_success_count / stage_episode_count
            # pbar.write(f"â­ å½“å‰ç´¯è®¡æˆåŠŸç‡ï¼š{success_count / (episode + 1) * 100:.2f}% ({success_count}/{episode + 1})")  #ä»è®­ç»ƒå¼€å§‹åˆ°å½“å‰ episode ä¸ºæ­¢çš„æ€»æˆåŠŸç‡ 
            
            previous_stage = scheduler.current_stage
            scheduler.update(success_rate, pbar)
            

            user_input = input(f"ğŸš¦ Stage {scheduler.current_stage - 1} å®Œæˆã€‚æ˜¯å¦ç»§ç»­è®­ç»ƒï¼Ÿ(y/n): ")
            if user_input.lower() != 'y':
                pbar.write("ğŸ›‘ ç”¨æˆ·é€‰æ‹©ç»ˆæ­¢è®­ç»ƒ")
                torch.save(model.state_dict(), f'models/stage{scheduler.current_stage - 1}_final.pt')
                plot_training_results(episode_logs, save_path='training_plot.png')
                writer.close()
                return agent
            # é‡ç½®é˜¶æ®µå†…è®¡æ•°å™¨
            stage_success_count = 0
            stage_episode_count = 0


            if scheduler.current_stage == 4:
                pbar.write("ğŸ›‘ å·²å®Œæˆ Stage 3ï¼Œè®­ç»ƒè‡ªåŠ¨ç»ˆæ­¢")
                torch.save(model.state_dict(), f'models/stage3_final.pt')
                writer.close()
                return agent
    

            if scheduler.current_stage != previous_stage:
                map_settings = scheduler.get_updated_map_settings()
                env, map_type = G2RLEnv.select_map_env(map_settings)
                pbar.write(f"ğŸ†• æ™‹çº§åé‡æ–°é€‰æ‹©åœ°å›¾ï¼š{map_type}")
                pbar.write(f"ğŸ‘¥ å½“å‰ Agent æ•°é‡ï¼š{env.num_agents}")


            if scheduler.current_stage > scheduler.max_stage:
                pbar.write("ğŸ‰ æ‰€æœ‰é˜¶æ®µå®Œæˆï¼Œè®­ç»ƒç»“æŸï¼")
                break

    # plot_training_results(episode_logs, save_path='training_plot.png')
    # df_logs = pd.DataFrame(episode_logs)
    # df_logs.to_csv('logs/episode_logs.csv', index=False, encoding='utf-8-sig')
    # pbar.write("ğŸ“„ episode_logs å·²ä¿å­˜ä¸º logs/episode_logs.csv")

    writer.close()
    return agent


# def train(
#         model: torch.nn.Module,
#         map_settings: Dict[str, dict],
#         map_probs: Union[List[float], None],
#         num_episodes: int = 300,
#         batch_size: int = 32,
#         decay_range: int = 1000,
#         log_dir='logs',
#         lr: float = 0.001,
#         replay_buffer_size: int = 1000,
#         device: str = 'cuda',
#         scheduler: Optional[CurriculumScheduler] = None,  # è‡ªåŠ¨æ™‹çº§/é‡å¤ç”¨
#         max_episode_seconds: int = 30
#     ) -> DDQNAgent:

#     timestamp = get_timestamp()
#     writer = SummaryWriter(log_dir=Path(log_dir) / timestamp)

#     # ç¯å¢ƒ/agent
#     maps = []
#     for name in map_settings:
#         env = G2RLEnv(**map_settings[name])
#         maps.append(env)
#     map_probs = map_probs or [1.0 / len(maps)] * len(maps)

#     # ç”¨ç¬¬ä¸€ä¸ª env çš„åŠ¨ä½œç©ºé—´åˆå§‹åŒ– agent
#     agent = DDQNAgent(
#         model,
#         maps[0].get_action_space(),
#         lr=lr,
#         decay_range=decay_range,
#         device=device,
#         replay_buffer_size=replay_buffer_size,
#     )

#     pbar = tqdm(range(num_episodes), desc='Episodes', dynamic_ncols=True)

#     # è®­ç»ƒè®¡æ•°å™¨
#     episode = 0
#     success_count_total = 0

#     # é˜¶æ®µè®¡æ•°å™¨ï¼ˆç”¨äºâ€œåˆ°è¾¾é˜¶æ®µä¸Šé™ä½†æœªè¾¾æ ‡â†’é‡å¤â€ï¼‰
#     stage_success_count = 0
#     stage_episode_count = 0

#     # â€”â€” é˜ˆå€¼ï¼šä¼˜å…ˆç”¨ scheduler.thresholdï¼Œå¦åˆ™å›é€€åˆ° 0.8
#     stage_threshold = getattr(scheduler, "threshold", 0.8)

#     # ä¸»å¾ªç¯ï¼šç›´åˆ°æ‰€æœ‰é˜¶æ®µå®Œæˆæˆ–è¾¾åˆ°æ€» episodes ä¸Šé™
#     while (scheduler is None) or (scheduler.current_stage <= scheduler.max_stage):
#         if episode >= num_episodes:
#             break

#         # 1) è·å–â€œå½“å‰é˜¶æ®µâ€çš„åœ°å›¾é…ç½®ï¼Œé‡å»º env
#         cur_map_cfg = scheduler.get_updated_map_settings() if scheduler else map_settings
#         map_type, cfg = next(iter(cur_map_cfg.items()))
#         env = G2RLEnv(**cfg)
#         pbar.write(f"ğŸŸ¢ ä½¿ç”¨åœ°å›¾ï¼š{map_type} | Stage {scheduler.current_stage if scheduler else '-'} | Agents={env.num_agents}")

#         # 2) reset & å‡†å¤‡ä¸€é›†
#         obs, info = env.reset()
#         target_idx = np.random.randint(env.num_agents)
#         agents = [agent if i == target_idx else AStarAgent() for i in range(env.num_agents)]
#         goal = tuple(env.goals[target_idx])
#         state = obs[target_idx]
#         opt_path = [state['global_xy']] + env.global_guidance[target_idx]

#         success_flag = False
#         retrain_count = 0
#         scalars = {
#             'Reward': 0.0,
#             'Moving Cost': 0.0,
#             'Detour Percentage': 0.0,
#             'Average Loss': 0.0,
#             'Average Epsilon': 0.0,
#             'Success': 0
#         }

#         # 3) è·‘ä¸€é›†
#         timesteps_per_episode = 50 + 10 * episode
#         episode_start_time = time.time()

#         for timestep in range(timesteps_per_episode):
#             # è¶…æ—¶ä¿æŠ¤
#             if time.time() - episode_start_time > max_episode_seconds:
#                 pbar.write(f"â° Episode {episode} è¶…æ—¶ï¼ˆ>{max_episode_seconds}sï¼‰ï¼Œå¼ºåˆ¶ç»ˆæ­¢æœ¬ episode")
#                 break

#             actions = [ag.act(o) for ag, o in zip(agents, obs)]
#             obs, reward, terminated, truncated, info = env.step(actions)

#             # åˆ°è¾¾ç›®æ ‡åˆ¤å®š
#             agent_pos = tuple(obs[target_idx]['global_xy'])
#             done = (agent_pos == goal)
#             terminated[target_idx] = done

#             if done:
#                 success_flag = True
#                 scalars['Success'] = 1
#                 scalars['Moving Cost'] = moving_cost(timestep + 1, opt_path[0], opt_path[-1])
#                 scalars['Detour Percentage'] = detour_percentage(timestep + 1, len(opt_path) - 1)
#                 break

#             # é‡‡æ ·&å­¦ä¹ 
#             # â€”â€” ä½ çš„çŠ¶æ€æå–é€»è¾‘ï¼ˆå°½é‡ä½¿ç”¨ obs[target_idx]ï¼Œä¸è¦ç”¨æ—§ state é‡Œçš„ 'obs'ï¼‰â€”â€”
#             # è¿™é‡Œä¿ç•™ä½ çš„åŸå®ç°ï¼Œæ³¨æ„ state çš„æ›´æ–°æ”¾åœ¨æœ€å
#             agent.store(
#                 state,
#                 actions[target_idx],
#                 reward[target_idx],
#                 obs[target_idx],
#                 terminated[target_idx],
#             )
#             state = obs[target_idx]
#             scalars['Reward'] += float(reward[target_idx])

#             if len(agent.replay_buffer) >= batch_size:
#                 retrain_count += 1
#                 scalars['Average Loss'] += float(agent.retrain(batch_size))
#                 scalars['Average Epsilon'] += float(agent.epsilon)

#         # 4) ç»Ÿè®¡æœ¬é›†
#         if retrain_count > 0:
#             scalars['Average Loss'] /= retrain_count
#             scalars['Average Epsilon'] /= retrain_count

#         if success_flag:
#             success_count_total += 1
#             stage_success_count += 1
#         scalars['Success'] = 1 if success_flag else 0

#         stage_episode_count += 1
#         episode += 1
#         pbar.update(1)

#         # logging
#         for name, value in scalars.items():
#             writer.add_scalar(name, value, episode)

#         pbar.set_postfix(
#             Stage=(scheduler.current_stage if scheduler else "-"),
#             SR_total=f"{success_count_total / max(1, episode):.2f}",
#             R=f"{scalars['Reward']:.2f}",
#         )

#         # 5) æŠŠç»“æœå†™è¿› scheduler çš„æ»‘çª—ï¼Œå¹¶åˆ¤å®šæ™‹çº§/é‡å¤
#         if scheduler is not None:
#             scheduler.add_episode_result(scalars['Success'])
#             window_sr = scheduler.current_window_sr()

#             # åˆ°è¾¾é˜¶æ®µä¸Šé™ï¼šæ ¹æ®é˜¶æ®µæˆåŠŸç‡ï¼ˆè€Œéæ»‘çª—ï¼‰åšä¸€æ¬¡ç¡¬åˆ¤å®š
#             if stage_episode_count >= scheduler.episodes_per_stage:
#                 stage_sr = stage_success_count / max(1, stage_episode_count)
#                 if stage_sr >= stage_threshold:
#                     scheduler.advance(pbar)
#                     # é‡ç½®é˜¶æ®µè®¡æ•°å™¨
#                     stage_success_count = 0
#                     stage_episode_count = 0
#                     # æœ€åä¸€é˜¶æ®µä¸”å·²è¾¾æ ‡å¯æ—©åœ
#                     if scheduler.is_done():
#                         break
#                     # è¿›å…¥ä¸‹ä¸€å¾ªç¯ä¼šæŒ‰æ–°é˜¶æ®µé…ç½®é‡å»º env
#                     continue
#                 else:
#                     # æœªè¾¾æ ‡ï¼šé‡å¤å½“å‰ stage
#                     scheduler.repeat_stage(pbar)
#                     stage_success_count = 0
#                     stage_episode_count = 0
#                     # ç»§ç»­åœ¨å½“å‰ stage è®­ç»ƒ
#                     continue

#             # å¯é€‰ï¼šå¦‚æœä½ æ›´åå¥½â€œæ»‘çª—è¾¾æ ‡ç«‹åˆ»æ™‹çº§â€ï¼Œä¹Ÿä¿ç•™è¿™æ¡å¿«é€Ÿé€šé“
#             if scheduler.ready_to_advance():
#                 scheduler.advance(pbar)
#                 stage_success_count = 0
#                 stage_episode_count = 0
#                 if scheduler.is_done():
#                     break
#                 continue

#     # ç»“æŸï¼šè®°å½•æœ€ç»ˆæˆåŠŸç‡ï¼ˆç”¨æ»‘çª—æˆ–å…¨å±€ï¼‰
#     # âœ… åªç”¨å…¨å±€ï¼šæ‰€æœ‰ episode æˆåŠŸæ•° / æ‰€æœ‰ episode æ•°
#     final_sr = success_count_total / max(1, episode)
#     agent.final_success_rate = float(final_sr)

# # ï¼ˆå¯é€‰ï¼‰è°ƒè¯•ä¸€ä¸‹ï¼Œç¡®è®¤å£å¾„
#     print(f"[train] final_sr(global) = {success_count_total}/{episode} = {agent.final_success_rate:.6f}")


#     writer.close()
#     return agent


    

if __name__ == '__main__':
    import os
    from collections import deque  # é¿å… deque æ ‡é»„
    import yaml
    import torch
    import pandas as pd
    import numpy as np

    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    os.makedirs('models', exist_ok=True)

    # 1) è¯»å–åœ°å›¾é…ç½®ï¼ˆä½ çš„ç”Ÿæˆæ–‡ä»¶ï¼‰
    MAP_SETTINGS_PATH = 'C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/g2rl/map_settings_generated.yaml'
    with open(MAP_SETTINGS_PATH, "r", encoding="utf-8") as f:
        base_map_settings = yaml.safe_load(f)

    # é¡¶å±‚ä¸º list æ—¶ï¼Œè½¬æˆ {name: spec}
    if isinstance(base_map_settings, list):
        base_map_settings = { (m.get("name") or f"map_{i}"): m for i, m in enumerate(base_map_settings) }

    # 2) æ„å»ºåŸºäº complexity çš„ schedulerï¼ˆç”¨ä½ å‰é¢è´´çš„ ComplexitySchedulerï¼‰
    scheduler = ComplexityScheduler(
        base_map_settings=base_map_settings,
        n_stages=5,
        min_per_stage=10,
        episodes_per_stage=100,
        threshold=0.70,
        window_size=100,
        shuffle_each_stage=True,
        seed=0,
        size_mode="max",  # å’Œä½ è®­ç»ƒå…¬å¼çš„ Size å®šä¹‰ä¿æŒä¸€è‡´
    )

    # 3) è®¾å¤‡ & æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CRNNModel().to(device)

    # 4) è®­ç»ƒ
    trained_agent = train(
        model=model,
        scheduler=scheduler,                           # âœ… ç”¨æ–°çš„ scheduler
        map_settings=scheduler.get_updated_map_settings(),  # åˆå§‹ä¸€å¼ ï¼›train å†…æ¯é›†ä¼šå†å–
        map_probs=None,
        num_episodes=300,
        batch_size=32,
        replay_buffer_size=500,
        decay_range=10_000,
        log_dir='logs',
        device=device,
        # å¦‚ä½ çš„ train ç­¾åé‡Œè¿˜æœ‰ max_episode_seconds ç­‰å‚æ•°ï¼Œä¹Ÿä¸€èµ·ä¼ ï¼š
        # max_episode_seconds=30,
    )

    # 5) ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'models/best_model.pt')
    print('âœ… æ¨¡å‹å·²ä¿å­˜åˆ° models/best_model.pt')


