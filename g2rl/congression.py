# -*- coding: utf-8 -*-
"""
éçº¿æ€§å»ºæ¨¡ï¼ˆç¥ç»ç½‘ç»œ MLPï¼‰â€” é€‚ç”¨äº MAPF/CL æˆåŠŸç‡æˆ–å¤æ‚åº¦å›å½’
åŠŸèƒ½ï¼š
1) è¯»CSV â†’ è‡ªåŠ¨æ‰¾ç›®æ ‡åˆ— â†’ åˆ«åæ˜ å°„ç‰¹å¾ â†’ æ¸…æ´— â†’ æ ‡å‡†åŒ–
2) MLP è®­ç»ƒï¼ˆå«Dropout + L2æ­£åˆ™ + æ—©åœï¼‰ + éªŒè¯æŒ‡æ ‡ï¼ˆR/R2/MAE/RMSE/MAPEï¼‰
3) å†™å›é¢„æµ‹/æ®‹å·®åˆ—åˆ°CSVï¼›ä¿å­˜è¯Šæ–­å›¾ï¼›ä¿å­˜æ¨¡å‹ä¸æ ‡å‡†åŒ–å™¨
"""

import os, json, math, copy, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from typing import List, Dict
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.preprocessing import StandardScaler

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

warnings.filterwarnings("ignore")

# ============ é…ç½®ï¼ˆæŒ‰éœ€æ”¹ï¼‰ ============
SAVE_DIR = r"C:/Users/dell/Desktop/CL_1/0902"  # â† ä½ æƒ³ä¿å­˜çš„ç›®å½•
os.makedirs(SAVE_DIR, exist_ok=True)  # è‹¥ä¸å­˜åœ¨åˆ™åˆ›å»º

CSV_CANDIDATES = [
    r"C:/Users/dell/Desktop/CL_1/train_gray3d-Copy-FDA.csv",
]
TARGET_COL = None  # None = è‡ªåŠ¨è¯†åˆ«ï¼ˆsuccess_rate / Complexity / complexity / target / yï¼‰

CANDIDATE_FEATURES = [
    "size", "num_agents", "density", "density_actual",
    "LDD", "BN", "MC", "DLR", "FRA", "FDA"
]

# è®­ç»ƒç›¸å…³
RANDOM_STATE      = 42
TEST_SIZE         = 0.2
BATCH_SIZE        = 128
MAX_EPOCHS        = 500
PATIENCE          = 30           # æ—©åœå®¹å¿è½®æ•°ï¼ˆæŒ‰ val RMSEï¼‰
LEARNING_RATE     = 1e-3
WEIGHT_DECAY      = 1e-4         # L2 æ­£åˆ™
HIDDEN_DIM        = 128
HIDDEN_LAYERS     = 2            # éšè—å±‚å±‚æ•°ï¼ˆä¸å«è¾“å…¥/è¾“å‡ºå±‚ï¼‰
DROPOUT           = 0.10
USE_SIGMOID_HEAD  = True         # å¦‚æœç›®æ ‡æ˜¯[0,1]ï¼ˆæˆåŠŸç‡ï¼‰ï¼Œå»ºè®® True

# è¾“å‡ºå‘½å
OUTPUT_SUFFIX   = "_nn_with_pred.csv"
FIG_PRED_ACT    = "_nn_pred_vs_actual.png"
FIG_RESID_HIST  = "_nn_residual_hist.png"
FIG_RESID_FIT   = "_nn_residuals_vs_fitted.png"
MODEL_PATH      = "_nn_model.pt"
SCALER_PATH     = "_nn_scaler.json"
# ======================================


def _find_csv_path(cands: List[str]) -> str:
    for p in cands:
        if os.path.exists(p):
            return p
    raise FileNotFoundError(f"æ‰¾ä¸åˆ° CSVã€‚è¯·æŠŠ CSV æ”¾åœ¨è¿™äº›è·¯å¾„ä¹‹ä¸€ï¼š\n{cands}")


def _pick_target_col(df: pd.DataFrame, pref=None) -> str:
    if pref and pref in df.columns:
        return pref
    for cand in ["success_rate", "Complexity", "complexity", "target", "y"]:
        if cand in df.columns:
            return cand
    raise ValueError("æœªæ‰¾åˆ°ç›®æ ‡åˆ—ï¼ˆsuccess_rate/Complexity/complexity/target/yï¼‰ã€‚è¯·è®¾ç½® TARGET_COLã€‚")


def _build_feature_mapping(df_cols):
    alias = {
        "size":           ["size", "Size"],
        "num_agents":     ["num_agents", "Agents", "agents", "n_agents"],
        "density":        ["density", "Density"],
        "density_actual": ["density_actual", "Density_actual", "actual_density"],
        "LDD":            ["LDD", "ldd"],
        "BN":             ["BN", "bn", "Bottleneck", "bottleneck"],
        "MC":             ["MC", "mc", "MapConnectivity", "map_connectivity"],  # é¿å…è¯¯æ˜ å°„åˆ° moving_cost
        "DLR":            ["DLR", "dlr"],
        "FRA":            ["FRA", "fra"],
        "FDA":            ["FDA", "fda"],
        # å¦‚ä½ æœ‰ moving_costï¼Œè¯·å¦èµ·è§„èŒƒåï¼Œé¿å…å’Œ MC æ··æ·†
        "moving_cost":    ["MovingCost", "moving_cost", "MCost"],
    }
    lower_cols = {c.lower(): c for c in df_cols}
    mapping = {}
    for canonical, cands in alias.items():
        for c in cands:
            if c in df_cols:
                mapping[canonical] = c
                break
            if c.lower() in lower_cols:
                mapping[canonical] = lower_cols[c.lower()]
                break
    return mapping


class TabularDataset(Dataset):
    def __init__(self, X: np.ndarray, y: np.ndarray):
        self.X = torch.tensor(X, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]


class MLP(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int, hidden_layers: int, dropout: float, use_sigmoid_head: bool):
        super().__init__()
        layers = []
        dim = in_dim
        for _ in range(hidden_layers):
            layers += [
                nn.Linear(dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
            ]
            dim = hidden_dim
        layers.append(nn.Linear(dim, 1))
        self.backbone = nn.Sequential(*layers)
        self.use_sigmoid = use_sigmoid_head

    def forward(self, x):
        out = self.backbone(x).squeeze(-1)
        if self.use_sigmoid:
            out = torch.sigmoid(out)
        return out


def metrics_block(y_true: np.ndarray, y_pred: np.ndarray) -> Dict[str, float]:
    # clipåˆ°[0,1]ï¼ˆè‹¥æ˜¯æˆåŠŸç‡ç±»ç›®æ ‡ï¼‰
    yt = y_true.astype(float)
    yp = y_pred.astype(float)
    if USE_SIGMOID_HEAD:
        yp = np.clip(yp, 0.0, 1.0)

    R = float(np.corrcoef(yt, yp)[0, 1]) if (len(yt) >= 2 and np.std(yt) > 0 and np.std(yp) > 0) else float("nan")
    R2 = float(r2_score(yt, yp))
    MAE = float(mean_absolute_error(yt, yp))
    RMSE = float(np.sqrt(mean_squared_error(yt, yp)))
    denom = np.where(np.abs(yt) < 1e-12, 1e-12, np.abs(yt))
    MAPE = float(np.mean(np.abs((yt - yp) / denom)) * 100.0)
    return {"R": R, "R2": R2, "MAE": MAE, "RMSE": RMSE, "MAPE%": MAPE}


def main():
    # 1) è¯»CSV
    csv_path = _find_csv_path(CSV_CANDIDATES)
    base, _ = os.path.splitext(csv_path)
    print("ğŸ“„ CSV:", csv_path)
    df = pd.read_csv(csv_path)
    print("Columns:", list(df.columns), "Shape:", df.shape)

    # 2) ç›®æ ‡åˆ—
    target_col = _pick_target_col(df, TARGET_COL)
    print("ğŸ¯ ç›®æ ‡åˆ—:", target_col)

    # 3) ç‰¹å¾æ˜ å°„ï¼ˆåˆ«åï¼‰
    mapping = _build_feature_mapping(df.columns)
    used_features = [f for f in CANDIDATE_FEATURES if f in mapping]
    if not used_features:
        raise ValueError("âŒ æ²¡æ‰¾åˆ°ä»»ä½•å¯ç”¨ç‰¹å¾ï¼Œè¯·æ£€æŸ¥åˆ—åæˆ– CANDIDATE_FEATURES")

    print("ğŸ§© ä½¿ç”¨ç‰¹å¾ï¼ˆè§„èŒƒåï¼‰:", used_features)
    print("â†³ åˆ—æ˜ å°„:", {k: mapping[k] for k in used_features})

    # 4) ä¸¢ NaN
    cols_need = [mapping[f] for f in used_features] + [target_col]
    dff = df[cols_need].dropna().copy()
    if dff.empty:
        raise ValueError("âŒ æœ‰æ•ˆæ ·æœ¬ä¸º 0ã€‚è¯·å…ˆæ¸…æ´—æ•°æ®ã€‚")
    print("âœ… å¯ç”¨æ ·æœ¬æ•°:", len(dff))

    # 5) å–X/y
    # X_all = dff[[mapping[f] for f in used_features]].astype(float).values


    cols = [mapping[f.strip()] for f in used_features]  # æ³¨æ„ f.strip() å»æ‰ ' FDA' å‰çš„ç©ºæ ¼é—®é¢˜

# é’ˆå¯¹æ¯ä¸€åˆ—åšæ¸…æ´—
    clean = dff[cols].apply(
        lambda col: col.astype(str)  # è½¬æˆå­—ç¬¦ä¸²
                   .str.replace(',', '', regex=False)
                   .str.replace('%', '', regex=False)
                   .str.replace(r'[^0-9eE+\-\.]', '', regex=True)  # ä¿ç•™æ•°å€¼ç›¸å…³å­—ç¬¦
    )

# è½¬æ¢æˆæ•°å€¼ï¼Œéæ³•çš„è½¬ NaN
    clean = clean.apply(pd.to_numeric, errors='coerce')

# æ£€æŸ¥åè¡Œ
    bad_rows = clean.isna().any(axis=1)
    if bad_rows.any():
        print(f"[æ¸…æ´—] æœ‰ {bad_rows.sum()} è¡ŒåŒ…å«éæ•°å€¼ç‰¹å¾ï¼š")
        print(dff.loc[bad_rows, cols].head())

# ä¸¢å¼ƒåè¡Œ
    clean = clean.dropna(axis=0, how='any')

    X_all = clean.values


    y_all = dff[target_col].astype(float).values

    # 6) åˆ’åˆ†è®­ç»ƒ/éªŒè¯
    X_tr, X_va, y_tr, y_va = train_test_split(X_all, y_all, test_size=TEST_SIZE, random_state=RANDOM_STATE)

    # 7) æ ‡å‡†åŒ–ï¼ˆä»… Xï¼‰
    x_scaler = StandardScaler().fit(X_tr)
    X_tr_s = x_scaler.transform(X_tr)
    X_va_s = x_scaler.transform(X_va)

    # 8) æ•°æ®é›†ä¸åŠ è½½å™¨
    ds_tr = TabularDataset(X_tr_s, y_tr)
    ds_va = TabularDataset(X_va_s, y_va)
    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)
    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)

    # 9) è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("ğŸ–¥ è®¾å¤‡:", device)

    # 10) æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±
    model = MLP(in_dim=X_tr_s.shape[1],
                hidden_dim=HIDDEN_DIM,
                hidden_layers=HIDDEN_LAYERS,
                dropout=DROPOUT,
                use_sigmoid_head=USE_SIGMOID_HEAD).to(device)

    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)
    loss_fn = nn.MSELoss()  # å›å½’

    # 11) è®­ç»ƒï¼ˆæ—©åœåŸºäº val RMSEï¼‰
    best_state = None
    best_val_rmse = float("inf")
    no_improve = 0

    for epoch in range(1, MAX_EPOCHS + 1):
        model.train()
        train_loss = 0.0
        for xb, yb in dl_tr:
            xb, yb = xb.to(device), yb.to(device)
            pred = model(xb)
            loss = loss_fn(pred, yb)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += loss.item() * xb.size(0)
        train_loss /= len(ds_tr)

        # éªŒè¯
        model.eval()
        with torch.no_grad():
            preds = []
            for xb, yb in dl_va:
                xb = xb.to(device)
                preds.append(model(xb).cpu().numpy())
            y_pred_va = np.concatenate(preds, axis=0)
        val_rmse = float(np.sqrt(mean_squared_error(y_va, y_pred_va)))

        if val_rmse + 1e-9 < best_val_rmse:
            best_val_rmse = val_rmse
            best_state = copy.deepcopy(model.state_dict())
            no_improve = 0
        else:
            no_improve += 1

        if epoch % 10 == 0 or epoch == 1:
            print(f"[Epoch {epoch:4d}] train_loss={train_loss:.6f}  val_RMSE={val_rmse:.6f}  best={best_val_rmse:.6f}")
        if no_improve >= PATIENCE:
            print(f"â¹ï¸ æ—©åœï¼š{epoch} / {MAX_EPOCHS}ï¼Œæœ€ä½³ val_RMSE={best_val_rmse:.6f}")
            break

    # åŠ è½½æœ€ä½³å‚æ•°
    if best_state is not None:
        model.load_state_dict(best_state)

    # 12) è¯„ä¼°ï¼ˆtrain/valï¼‰
    model.eval()
    with torch.no_grad():
        y_pred_tr = model(torch.tensor(X_tr_s, dtype=torch.float32, device=device)).cpu().numpy()
        y_pred_va = model(torch.tensor(X_va_s, dtype=torch.float32, device=device)).cpu().numpy()

    m_tr = metrics_block(y_tr, y_pred_tr)
    m_va = metrics_block(y_va, y_pred_va)

    print("\n==== Train Metrics ====")
    for k, v in m_tr.items():
        print(f"{k:>7s}: {v: .6f}")
    print("==== Valid Metrics ====")
    for k, v in m_va.items():
        print(f"{k:>7s}: {v: .6f}")

    # 13) å…¨è¡¨é¢„æµ‹ï¼ˆå¯¹åŸ dfï¼›ç¼ºå¤±ä¿ç•™ NaNï¼‰
    #    ä»…å¯¹åŸ df ä¸­â€œç‰¹å¾å®Œæ•´â€çš„è¡Œåšé¢„æµ‹
    mask_ok = ~df[[mapping[f] for f in used_features]].isna().any(axis=1)
    # X_full = df.loc[mask_ok, [mapping[f] for f in used_features]].astype(float).values
    # ==== ç»Ÿä¸€ä¿®æ­£ç‰¹å¾åç©ºæ ¼ç­‰é—®é¢˜ ====
    used_features = [f.strip() for f in used_features]
    cols = [mapping[f] for f in used_features]

# ==== å…ˆå¯¹å­è¡¨åšæ•°å€¼æ¸…æ´—ï¼ˆé€åˆ— .str æ“ä½œï¼‰====
    raw_feat = df.loc[:, cols].copy()

    def _clean_series(s):
        return (s.astype(str)
                .str.replace(',', '', regex=False)      # å»æ‰åƒåˆ†ä½
                .str.replace('%', '', regex=False)      # å»æ‰ç™¾åˆ†å·
                .str.replace(r'[^0-9eE+\.-]', '', regex=True))  # ä»…ä¿ç•™æ•°å­—/æ­£è´Ÿå·/å°æ•°ç‚¹/eE

    clean_feat = raw_feat.apply(_clean_series)
    num_feat = clean_feat.apply(pd.to_numeric, errors='coerce')

# ==== è®°å½•å¹¶å¤„ç†åè¡Œ ====
    bad_rows = num_feat.isna().any(axis=1)
    if bad_rows.any():
        print(f"[æ¸…æ´—] å‘ç° {bad_rows.sum()} è¡Œå«éæ•°å€¼ç‰¹å¾ï¼Œå°†è¢«å‰”é™¤ã€‚ç¤ºä¾‹ï¼š")
    # æ‰“å°å‰ 5 è¡Œé—®é¢˜æ ·æœ¬ï¼Œä¾¿äºå®šä½å“ªä¸€åˆ—è„æ•°æ®
        print(raw_feat[bad_rows].head(5))

# å¦‚æœä½ å·²æœ‰ mask_okï¼ˆæ¯”å¦‚ç›®æ ‡åˆ—ç­‰è§„åˆ™ï¼‰ï¼ŒæŠŠä¸¤ç§æ©ç éƒ½ç”¨ä¸Š
    mask_good = (~bad_rows)
    if 'mask_ok' in locals():
        mask_good = mask_good & mask_ok

# ==== æœ€ç»ˆç‰¹å¾çŸ©é˜µ ====
    X_full = num_feat.loc[mask_good, :].values

# ==== è‹¥å·²æœ‰ yï¼Œä¹Ÿéœ€åŒæ­¥æ©ç ï¼ˆç¤ºä¾‹ï¼‰====
# y_full = y_series.loc[mask_good].values

    X_full_s = x_scaler.transform(X_full)
    with torch.no_grad():
        pred_full = model(torch.tensor(X_full_s, dtype=torch.float32, device=device)).cpu().numpy().reshape(-1)

    pred_col  = f"nn_pred_{target_col}"
    resid_col = f"nn_residual_{target_col}"
    df[pred_col] = np.nan
    df.loc[mask_ok, pred_col] = pred_full
    # ä»…å¯¹ç›®æ ‡éç¼ºå¤±è®¡ç®—æ®‹å·®
    mask_resid = mask_ok & df[target_col].notna()
    df[resid_col] = np.nan
    df.loc[mask_resid, resid_col] = df.loc[mask_resid, target_col].astype(float).values - df.loc[mask_resid, pred_col].astype(float).values

    # 14) ä¿å­˜æ–°CSV
    out_csv = os.path.join(SAVE_DIR, "nn_results_with_pred.csv")
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å†™å‡ºï¼š{out_csv}")

    # 15) è¯Šæ–­å›¾ï¼ˆåŸºäºéªŒè¯é›†ï¼‰
    #   å›¾1ï¼šPred vs Actual
    fig1       = os.path.join(SAVE_DIR, "nn_pred_vs_actual.png")
    plt.figure()
    plt.scatter(y_va, y_pred_va, alpha=0.7)
    minv = float(min(np.min(y_va), np.min(y_pred_va)))
    maxv = float(max(np.max(y_va), np.max(y_pred_va)))
    plt.plot([minv, maxv], [minv, maxv], linestyle="--")
    plt.xlabel("Actual (Valid)")
    plt.ylabel("Predicted (Valid)")
    plt.title(f"NN Pred vs Actual (Valid)\nR={m_va['R']:.3f}, RÂ²={m_va['R2']:.3f}, MAE={m_va['MAE']:.3f}, RMSE={m_va['RMSE']:.3f}")
    plt.tight_layout()
    plt.savefig(fig1, dpi=160); plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig1}")

    #   å›¾2ï¼šResidual Histogram
    fig2       = os.path.join(SAVE_DIR, "nn_residual_hist.png")
    plt.figure()
    plt.hist(y_va - y_pred_va, bins=30)
    plt.xlabel("Residual (y - y_pred) [Valid]")
    plt.ylabel("Count")
    plt.title("NN Residuals Histogram (Valid)")
    plt.tight_layout()
    plt.savefig(fig2, dpi=160); plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig2}")

    #   å›¾3ï¼šResiduals vs Fitted
    fig3       = os.path.join(SAVE_DIR, "nn_residuals_vs_fitted.png")
    plt.figure()
    plt.scatter(y_pred_va, y_va - y_pred_va, alpha=0.7)
    plt.axhline(0.0, linestyle="--")
    plt.xlabel("Fitted (Predicted) [Valid]")
    plt.ylabel("Residual [Valid]")
    plt.title("NN Residuals vs Fitted (Valid)")
    plt.tight_layout()
    plt.savefig(fig3, dpi=160); plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig3}")

    # 16) ä¿å­˜æ¨¡å‹ä¸æ ‡å‡†åŒ–å™¨
    model_path = os.path.join(SAVE_DIR, "nn_model.pt")
    torch.save({
        "state_dict": model.state_dict(),
        "in_dim": X_tr_s.shape[1],
        "hidden_dim": HIDDEN_DIM,
        "hidden_layers": HIDDEN_LAYERS,
        "dropout": DROPOUT,
        "use_sigmoid_head": USE_SIGMOID_HEAD,
        "scaler_means": x_scaler.mean_.tolist(),
        "scaler_stds": x_scaler.scale_.tolist(),
        "features_used": used_features,
        "feature_columns": [mapping[f] for f in used_features],
        "target": target_col,
    }, model_path)
    print(f"ğŸ§  æ¨¡å‹å·²ä¿å­˜ï¼š{model_path}")

    scaler_meta = {
        "means": x_scaler.mean_.tolist(),
        "stds": x_scaler.scale_.tolist(),
        "feature_order": [mapping[f] for f in used_features],
        "standardize": True
    }
    scaler_path= os.path.join(SAVE_DIR, "nn_scaler.json")
    with open(scaler_path, "w", encoding="utf-8") as f:
        json.dump(scaler_meta, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ æ ‡å‡†åŒ–å™¨ä¿¡æ¯å·²ä¿å­˜ï¼š{scaler_path}")

if __name__ == "__main__":
    main()
