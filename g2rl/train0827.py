#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pathlib import Path
from datetime import datetime
from typing import Dict, Optional, Union, List
from collections import deque
import os, sys, time, inspect, random

import numpy as np
import pandas as pd
import torch
from torch.utils.tensorboard import SummaryWriter
import yaml
from tqdm import tqdm
from pogema import AStarAgent

# ===================== é…ç½®åŒºï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ =====================
# 1) YAML åœ°å›¾é…ç½®
MAP_SETTINGS_PATH = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/g2rl/map_settings_generated_new.yaml"

# 2) å¤æ‚åº¦ CSVï¼ˆç”± infer_complexity.py ç”Ÿæˆï¼‰
COMPLEXITY_CSV = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main-nn/0827result/maps_features_with_complexity.csv"

# 3) å¤æ‚åº¦CSVé‡Œç”¨äºåŒ¹é…åœ°å›¾IDçš„åˆ—ï¼ˆcsvï¼‰ä¸ YAML çš„å¯¹åº”å­—æ®µï¼ˆyamlï¼‰
#    ä¾‹å¦‚ï¼šCSV é‡Œæ˜¯ config_idï¼ŒYAML é‡Œæ˜¯ nameï¼Œåˆ™å¯¹åº”å¦‚ä¸‹ï¼š
MAP_ID_COL_IN_CSV  = "config_id"   # ä¹Ÿå¯æ¢æˆ "grid_hash"
MAP_ID_COL_IN_YAML = "name"        # YAML é‡Œåœ°å›¾çš„å”¯ä¸€æ ‡è¯†å­—æ®µ

# 4) è®­ç»ƒè¶…å‚
N_STAGES = 5
MIN_PER_STAGE = 10
MIN_EPISODES_PER_STAGE = 300
THRESHOLD = 0.80
WINDOW_SIZE = 100
NUM_EPISODES = 300
BATCH_SIZE = 32
REPLAY_BUFFER_SIZE = 500
DECAY_RANGE = 10_000
MAX_EPISODE_SECONDS = 30
LOG_DIR = "logs"
RUN_DIR = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/final_trainig_1"
MODEL_OUT = "models/best_model.pt"
# ============================================================


# -------- æŸ¥æ‰¾è§‚æµ‹ä¸­çš„æ•°ç»„ï¼Œå¹¶ç»Ÿä¸€åˆ° [C, D, H, W] --------
def _find_array(obj):
    import numpy as np
    try:
        import torch
        is_tensor = True
    except Exception:
        is_tensor = False

    if isinstance(obj, np.ndarray):
        return obj
    if is_tensor and hasattr(obj, "detach") and hasattr(obj, "cpu") and hasattr(obj, "numpy"):
        try:
            return obj.detach().cpu().numpy()
        except Exception:
            pass
    if isinstance(obj, (list, tuple)):
        for v in obj:
            arr = _find_array(v); 
            if arr is not None: 
                return arr
        return None
    if isinstance(obj, dict):
        preferred = ("obs","observation","view","state","tensor","grid","image","local_obs","global_obs")
        for k in preferred:
            if k in obj:
                arr = _find_array(obj[k])
                if arr is not None:
                    return arr
        for v in obj.values():
            arr = _find_array(v)
            if arr is not None:
                return arr
        return None
    try:
        arr = np.array(obj)
        if arr.ndim > 0:
            return arr
    except Exception:
        pass
    return None
def _obs_to_tensor_CDHW(s, device, expected_c: int):
    """
    ä»ä»»æ„ç»“æ„çš„å•ä¸ªæ™ºèƒ½ä½“è§‚æµ‹ s æ„é€  [1, C, D, H, W] çš„ float32 tensorï¼Œ
    å¹¶æŠŠâ€œé€šé“ç»´â€å¯¹é½åˆ° expected_cï¼š
      - C==expected_cï¼šåŸæ ·
      - C==1 ä¸” expected_c>1ï¼šrepeat åˆ° expected_c
      - 1<C<expected_cï¼šé›¶å¡«å……åˆ° expected_c
      - C>expected_cï¼šè£å‰ªå‰ expected_c ä¸ªé€šé“
    """
    import numpy as np
    import torch

    arr = _find_array(s)
    if arr is None:
        raise ValueError("æ— æ³•ä»è§‚æµ‹ä¸­æå–æ•°ç»„/å¼ é‡ã€‚")

    arr = np.array(arr)

    # å…ˆå½’ä¸€åˆ° 4 ç»´ [C,D,H,W]
    if arr.ndim == 5:          # [N,?, ?, ?, ?]
        if arr.shape[1] == expected_c:
            arr = arr[0]
        elif arr.shape[-1] == expected_c:
            arr = np.transpose(arr, (0,4,1,2,3))[0]
        else:
            arr = arr[0]
    if arr.ndim == 4:          # [C,D,H,W] / [D,H,W,C] / [D,C,H,W]
        if arr.shape[0] == expected_c:
            pass
        elif arr.shape[-1] == expected_c:
            arr = np.transpose(arr, (3,0,1,2))     # DHWC -> CDHW
        elif arr.shape[1] == expected_c:
            arr = np.transpose(arr, (1,0,2,3))     # DCHW -> CDHW
    elif arr.ndim == 3:        # [D,H,W]
        arr = arr[None, ...]
    elif arr.ndim == 2:        # [H,W]
        arr = arr[None, None, ...]
    else:
        raise ValueError(f"è§‚æµ‹ç»´åº¦ä¸æ”¯æŒï¼šshape={arr.shape}, ndim={arr.ndim}")

    if arr.ndim != 4:
        raise ValueError(f"é¢„å¤„ç†åä¸æ˜¯ 4 ç»´ CDHWï¼šshape={arr.shape}")

    C, D, H, W = arr.shape

    # é€šé“ä¿®æ­£ä¸º expected_c
    if C == expected_c:
        arr_fixed = arr
    elif C == 1 and expected_c > 1:
        arr_fixed = np.repeat(arr, expected_c, axis=0)
    elif C < expected_c:
        pad = np.zeros((expected_c - C, D, H, W), dtype=arr.dtype)
        arr_fixed = np.concatenate([arr, pad], axis=0)
    else:  # C > expected_c
        arr_fixed = arr[:expected_c]

    x = torch.tensor(arr_fixed[None, ...], dtype=torch.float32, device=device)  # [1,C,D,H,W]
    return x


# ============ é¡¹ç›®æ ¹è·¯å¾„ï¼ˆç¡®ä¿èƒ½ import g2rl.*ï¼‰ ============
project_root = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main-nn"
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ============ é¡¹ç›®æ¨¡å— ============ (è¦æ±‚ä½ çš„é¡¹ç›®ä¸­å­˜åœ¨è¿™äº›æ¨¡å—)
from g2rl.environment import G2RLEnv
from g2rl.agent import DDQNAgent
from g2rl.network import CRNNModel

# ============ å®‰å…¨æ„é€ å™¨ï¼šåªä¼  __init__ æ”¯æŒçš„å‚æ•° ============

from types import SimpleNamespace

def _safe_get_action_space(env):
    """
    å°½é‡ç¨³å¥åœ°æ‹¿åˆ°åŠ¨ä½œç©ºé—´ï¼š
    1) ä¼˜å…ˆç”¨ env.action_space.n
    2) å†è¯• env.get_action_space() çš„è¿”å›
    3) å†è¯• env.actions çš„é•¿åº¦
    4) å¦‚æœè¿˜æ²¡æœ‰ï¼Œå…ˆ reset ä¸€æ¬¡å†é‡å¤ 1-3
    5) æœ€åå…œåº•è¿”å› 5ï¼ˆå¸¸è§çš„ stay+UDLRï¼‰
    """
    # 1) gym é£æ ¼
    asp = getattr(env, "action_space", None)
    if asp is not None and hasattr(asp, "n"):
        return asp

    # 2) é¡¹ç›®è‡ªå®šä¹‰
    gas = getattr(env, "get_action_space", None)
    if callable(gas):
        try:
            out = gas()
            if hasattr(out, "n"):
                return out
            if isinstance(out, (list, tuple)):
                return SimpleNamespace(n=len(out))
            if isinstance(out, int) and out > 0:
                return SimpleNamespace(n=out)
        except Exception:
            pass

    # 3) ç›´æ¥çœ‹ env.actions
    if hasattr(env, "actions"):
        try:
            return SimpleNamespace(n=len(env.actions))
        except Exception:
            pass

    # 4) å…ˆ reset å†è¯•ä¸€è½®
    try:
        env.reset()
    except Exception:
        pass

    asp = getattr(env, "action_space", None)
    if asp is not None and hasattr(asp, "n"):
        return asp
    gas = getattr(env, "get_action_space", None)
    if callable(gas):
        try:
            out = gas()
            if hasattr(out, "n"):
                return out
            if isinstance(out, (list, tuple)):
                return SimpleNamespace(n=len(out))
            if isinstance(out, int) and out > 0:
                return SimpleNamespace(n=out)
        except Exception:
            pass
    if hasattr(env, "actions"):
        try:
            return SimpleNamespace(n=len(env.actions))
        except Exception:
            pass

    # 5) å…œåº•
    return SimpleNamespace(n=5)



def build_env_from_raw(raw_cfg: dict) -> G2RLEnv:
    sig = inspect.signature(G2RLEnv.__init__)
    allowed = {
        p.name for p in sig.parameters.values()
        if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY)
    }
    allowed.discard("self")

    rename = {
        # å¦‚éœ€è¦å¯åœ¨æ­¤åšé”®åæ˜ å°„: ä¾‹å¦‚ 'size': 'map_size'
    }

    ctor_cfg = {}
    for k, v in raw_cfg.items():
        kk = rename.get(k, k)
        if kk in allowed:
            ctor_cfg[kk] = v

    env = G2RLEnv(**ctor_cfg)

    # å°† grid/starts/goals ç­‰ä½œä¸ºå±æ€§æŒ‚è½½ï¼ˆå¦‚æœ YAML æœ‰è¿™äº›ï¼‰
    if "grid" in raw_cfg:
        try:
            env.grid = (np.array(raw_cfg["grid"]) > 0).astype(np.uint8)
        except Exception:
            env.grid = None
    if "starts" in raw_cfg:
        env.starts = raw_cfg["starts"]
    if "goals" in raw_cfg:
        env.goals = raw_cfg["goals"]

    return env

# ================== ç”¨ CSV åˆå¹¶å¤æ‚åº¦ ==================
def _merge_complexity_from_csv(base_map_settings: Dict[str, dict]) -> Dict[str, dict]:
    """
    ç¨³å¥åˆå¹¶ nn_complexityï¼š
    1) è‡ªåŠ¨å¯»æ‰¾ CSV ä¸ YAML çš„å…±åŒ ID åˆ—ï¼ˆä¼˜å…ˆï¼šconfig_idã€grid_hashã€nameã€map_idï¼‰
    2) è‹¥æ— å…±åŒ IDï¼Œåˆ™ä½¿ç”¨å¤åˆé”® (size,num_agents,density,obs_radius,max_episode_steps) åŒ¹é…
       - density ç­‰æµ®ç‚¹åˆ—ä¼šåšå››èˆäº”å…¥ä»¥é¿å…å¾®å°æ•°å€¼è¯¯å·®
    3) å¯¹åŒä¸€ ID çš„å¤šè¡Œå–å‡å€¼ï¼ˆéœ€è¦çš„è¯å¯ä»¥æ”¹æˆå–æœ€åä¸€æ¡ï¼‰
    4) æ‰“å°åŒ¹é…ç»Ÿè®¡
    """
    import numpy as np
    import pandas as pd
    import os

    if not os.path.exists(COMPLEXITY_CSV):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°å¤æ‚åº¦CSVï¼š{COMPLEXITY_CSV}")

    df = pd.read_csv(COMPLEXITY_CSV)

    # å…œåº•ï¼šæ²¡æœ‰ nn_complexity å°±ç”¨ 1 - nn_pred_success_rate
    if "nn_complexity" not in df.columns:
        if "nn_pred_success_rate" in df.columns:
            df["nn_complexity"] = 1.0 - pd.to_numeric(df["nn_pred_success_rate"], errors="coerce")
        else:
            raise ValueError("å¤æ‚åº¦CSVç¼ºå°‘ nn_complexity ä¸”æ²¡æœ‰ nn_pred_success_rate æ— æ³•æ¨å¯¼ã€‚")

    # è§„èŒƒæˆæ•°å€¼
    df["nn_complexity"] = pd.to_numeric(df["nn_complexity"], errors="coerce")

    # ===== 1) å¯»æ‰¾å…±åŒ ID åˆ— =====
    csv_id_candidates  = [c for c in ["config_id", "grid_hash", "name", "map_id"] if c in df.columns]
    yaml_id_candidates = set()
    for spec in base_map_settings.values():
        yaml_id_candidates.update(spec.keys())
    yaml_id_candidates = [c for c in ["config_id", "grid_hash", "name", "map_id"] if c in yaml_id_candidates]

    # å–ç¬¬ä¸€ä¸ªå…±åŒåˆ—ä½œä¸ºä¸»é”®
    common_ids = [c for c in csv_id_candidates if c in yaml_id_candidates]
    chosen_id = common_ids[0] if common_ids else None

    # ===== 2) å¦‚æœæ²¡æœ‰å…±åŒ IDï¼Œç”¨å¤åˆé”®åŒ¹é… =====
    use_composite = (chosen_id is None)

    # é¢„å¤„ç† DataFrameï¼ˆæŒ‰ chosen_id æˆ–å¤åˆé”®èšåˆï¼‰
    if not use_composite:
        # åŒä¸€ ID å¤šè¡Œ -> å‡å€¼
        agg = (
            df.groupby(chosen_id, dropna=False)["nn_complexity"]
              .mean()
              .reset_index()
              .rename(columns={"nn_complexity": "complexity"})
        )
        # æ˜ å°„ï¼šid -> complexity
        comp_map = dict(zip(agg[chosen_id].astype(str), agg["complexity"].astype(float)))
    else:
        # å¤åˆé”®ï¼šsize,num_agents,density,obs_radius,max_episode_steps
        needed = ["size", "num_agents", "density", "obs_radius", "max_episode_steps"]
        missing = [c for c in needed if c not in df.columns]
        if missing:
            raise ValueError(f"æ— æ³•ç”¨å¤åˆé”®åŒ¹é…ï¼ŒCSV ç¼ºå°‘åˆ—ï¼š{missing}")

        # æ•°å€¼è§„èŒƒï¼šå››èˆäº”å…¥é¿å…æµ®ç‚¹è¯¯å·®
        df["_size"] = pd.to_numeric(df["size"], errors="coerce").astype("Int64")
        df["_nag"]  = pd.to_numeric(df["num_agents"], errors="coerce").astype("Int64")
        df["_den"]  = pd.to_numeric(df["density"], errors="coerce").round(4)
        df["_obs"]  = pd.to_numeric(df["obs_radius"], errors="coerce").astype("Int64")
        df["_mep"]  = pd.to_numeric(df["max_episode_steps"], errors="coerce").astype("Int64")

        comp = (
            df.dropna(subset=["_size","_nag","_den","_obs","_mep"])
              .groupby(["_size","_nag","_den","_obs","_mep"])["nn_complexity"]
              .mean()
              .reset_index()
              .rename(columns={"nn_complexity":"complexity"})
        )
        # æ˜ å°„ï¼štuple -> complexity
        comp_map = { (int(r["_size"]), int(r["_nag"]), float(r["_den"]), int(r["_obs"]), int(r["_mep"])): float(r["complexity"])
                     for _, r in comp.iterrows() }

    # ===== 3) æŠŠ complexity å†™å› spec =====
    matched, unmatched = 0, 0
    out: Dict[str, dict] = {}

    for name, spec in base_map_settings.items():
        new_spec = dict(spec)
        cpx_val = np.nan

        if not use_composite:
            # ä¸»é”®åŒ¹é…
            key = spec.get(chosen_id, None)
            if key is None and chosen_id == "name":
                # é€€åŒ–ï¼šç”¨ dict çš„ keyï¼ˆnameï¼‰å½“ä½œ name
                key = name
            if key is not None:
                cpx_val = comp_map.get(str(key), np.nan)
        else:
            # å¤åˆé”®åŒ¹é…
            try:
                size  = int(spec.get("size"))
                nag   = int(spec.get("num_agents"))
                den   = float(spec.get("density"))
                obs   = int(spec.get("obs_radius"))
                mep   = int(spec.get("max_episode_steps"))
                tup   = (size, nag, round(den,4), obs, mep)
                cpx_val = comp_map.get(tup, np.nan)
            except Exception:
                cpx_val = np.nan

        if np.isfinite(cpx_val):
            matched += 1
            new_spec["complexity"] = float(cpx_val)
        else:
            unmatched += 1
            new_spec["complexity"] = np.nan

        out[name] = new_spec

    print(f"ğŸ§© å¤æ‚åº¦åŒ¹é…ç»Ÿè®¡ï¼šmatched={matched}, unmatched={unmatched} "
          f"| æ¨¡å¼={'å¤åˆé”®' if use_composite else f'IDåˆ—({chosen_id})'}")

    if unmatched > 0:
        # æ‰“å°å‰å‡ ä¸ªæœªåŒ¹é…æ ·æœ¬ï¼Œä¾¿äºä½ ç¡®è®¤ ID æ˜¯å¦ä¸€è‡´
        examples = [k for k,v in out.items() if not np.isfinite(v.get('complexity', np.nan))][:5]
        print(f"âš ï¸ æœ‰ {unmatched} å¼ åœ°å›¾æœªåŒ¹é…åˆ°å¤æ‚åº¦ï¼ˆæœªå‚ä¸é‡åŒ–ï¼‰ã€‚ç¤ºä¾‹ï¼š{examples}")

    return out


# ================== é˜¶æ®µåˆ‡åˆ†ï¼ˆæŒ‰å¤æ‚åº¦åˆ†ä½ï¼‰ ==================
def _build_stages_by_quantile_df(df: pd.DataFrame, n_stages: int = 5, min_per_stage: int = 5):
    df = df.dropna(subset=["complexity"]).copy()
    if len(df) == 0:
        raise ValueError("æ²¡æœ‰å¯ç”¨åœ°å›¾ç”¨äº complexity è¯¾ç¨‹ï¼ˆcomplexity å…¨ NaNï¼‰ã€‚")
    qs = np.linspace(0, 1, n_stages + 1)
    edges = np.quantile(df["complexity"].values, qs)
    stages = []
    for i in range(n_stages):
        lo, hi = float(edges[i]), float(edges[i+1]) + 1e-12
        sub = df[(df["complexity"] >= lo) & (df["complexity"] < hi)]
        if len(sub) < min_per_stage:
            need = min_per_stage - len(sub)
            center = (lo + hi) / 2.0
            extra = df.iloc[(df["complexity"] - center).abs().argsort()[:need]]
            sub = pd.concat([sub, extra]).drop_duplicates(subset=["name"])
        stages.append({
            "stage": i,
            "cpx_min": lo,
            "cpx_max": hi,
            "items": sub.to_dict("records"),  # æ¯æ¡å« name/spec/complexity
        })
    return stages

# ================== Scheduler ==================
class ComplexityScheduler:
    """
    - æ¯é˜¶æ®µæŒæœ‰ä¸€ç»„ mapsï¼ˆç”± complexity åˆ†ä½åˆ‡åˆ†ï¼‰
    - æ¯é˜¶æ®µè‡³å°‘è·‘ min_episodes_per_stage é›†
    - è¾¾åˆ°é˜ˆå€¼åæ™‹çº§
    """
    def __init__(self,
                 base_map_settings: Dict[str, dict],
                 n_stages: int = 5,
                 min_per_stage: int = 10,
                 min_episodes_per_stage: int = 100,
                 threshold: float = 0.70,
                 window_size: int = 100,
                 use_window_sr: bool = True,
                 shuffle_each_stage: bool = True,
                 seed: int = 0):
        self.min_episodes_per_stage = int(min_episodes_per_stage)
        self.threshold = float(threshold)
        self.window_size = int(window_size)
        self.use_window_sr = bool(use_window_sr)

        # æ„é€ åŒ…å« complexity çš„ DataFrame
        rows = []
        for name, spec in base_map_settings.items():
            rows.append({"name": name, "complexity": spec.get("complexity", np.nan), "spec": spec})
        df = pd.DataFrame(rows)
        stages = _build_stages_by_quantile_df(df, n_stages=n_stages, min_per_stage=min_per_stage)

        self._rng = random.Random(seed)
        self._stage_items = []
        self._stage_edges = []
        for st in stages:
            items = list(st["items"])
            if shuffle_each_stage:
                self._rng.shuffle(items)
            self._stage_items.append(items)
            self._stage_edges.append((st["cpx_min"], st["cpx_max"]))

        self.current_stage = 0
        self.max_stage = len(self._stage_items) - 1

        self._idx_in_stage = 0
        self._win = deque(maxlen=self.window_size)
        self._ep_in_stage = 0
        self._succ_in_stage = 0

    def get_updated_map_settings(self) -> Dict[str, dict]:
        if self.current_stage > self.max_stage:
            return {}
        items = self._stage_items[self.current_stage]
        if not items:
            raise RuntimeError(f"Stage {self.current_stage} æ²¡æœ‰åœ°å›¾ã€‚")
        item = items[self._idx_in_stage]
        self._idx_in_stage = (self._idx_in_stage + 1) % len(items)
        return {item["name"]: item["spec"]}

    def add_episode_result(self, success: int):
        s = 1 if success else 0
        self._win.append(s)
        self._ep_in_stage += 1
        self._succ_in_stage += s

    def window_sr(self) -> float:
        return float(sum(self._win) / len(self._win)) if len(self._win) else 0.0

    def stage_sr(self) -> float:
        return float(self._succ_in_stage / max(1, self._ep_in_stage))

    def should_advance(self) -> bool:
        if self._ep_in_stage < self.min_episodes_per_stage:
            return False
        sr = self.window_sr() if self.use_window_sr else self.stage_sr()
        return sr >= self.threshold

    def advance(self, pbar=None):
        if pbar:
            lo, hi = self._stage_edges[self.current_stage]
            pbar.write(
                f"âœ… é€šè¿‡ Stage {self.current_stage} | "
                f"SR(win)={self.window_sr():.2f} / SR(stage)={self.stage_sr():.2f} | "
                f"åŒºé—´[{lo:.4f}, {hi:.4f}] â†’ Stage {self.current_stage + 1}"
            )
        self.current_stage += 1
        self._reset_stage_stats()

    def repeat_stage(self, pbar=None):
        if pbar:
            pbar.write(f"ğŸ” æœªè¾¾æ ‡ï¼Œé‡å¤ Stage {self.current_stage}ï¼ˆå·²è®­ç»ƒ {self._ep_in_stage} epï¼ŒSR={self.stage_sr():.2f}ï¼‰")
        self._reset_stage_stats()

    def _reset_stage_stats(self):
        self._idx_in_stage = 0
        self._win.clear()
        self._ep_in_stage = 0
        self._succ_in_stage = 0

    def is_done(self) -> bool:
        return self.current_stage > self.max_stage

# ================== è®­ç»ƒ ==================
def get_timestamp() -> str:
    return datetime.now().strftime('%H-%M-%d-%m-%Y')

def train(
    model: torch.nn.Module,
    map_settings: Dict[str, dict],
    map_probs: Union[List[float], None],
    num_episodes: int = 300,
    batch_size: int = 32,
    decay_range: int = 1000,
    log_dir='logs',
    lr: float = 0.001,
    replay_buffer_size: int = 1000,
    device: str = 'cuda',
    scheduler: Optional[ComplexityScheduler] = None,
    max_episode_seconds: int = 30,
    run_dir: Optional[str] = None
) -> DDQNAgent:

    # è¾“å‡ºç›®å½•
    if run_dir is None:
        run_dir = Path(log_dir) / get_timestamp()
    else:
        run_dir = Path(run_dir)
    run_dir.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(run_dir))

    # åˆå§‹åŒ–ç¬¬ä¸€ä¸ª env ä»¥è·å–åŠ¨ä½œç©ºé—´
    from types import SimpleNamespace

    first_name = next(iter(map_settings))
    first_env = build_env_from_raw(map_settings[first_name])
    # ==== å·²æœ‰ï¼šfirst_env æ„å»º & é‡ç½® ====
    try:
        first_env.reset()
    except Exception:
        pass

# ==== æ¨æ–­ n_actions ====
    def _infer_n_actions(env) -> int:
        asp = getattr(env, "action_space", None)
        if asp is not None and hasattr(asp, "n"):
            return int(asp.n)
        if hasattr(env, "get_action_space") and callable(env.get_action_space):
            out = env.get_action_space()
            if hasattr(out, "n"):
                return int(out.n)
            if hasattr(out, "__len__"):
                return len(out)
            if isinstance(out, int):
                return out
        if hasattr(env, "actions") and env.actions is not None:
            return len(env.actions)
        return 5

    n_actions = _infer_n_actions(first_env)
    action_space_list = list(range(n_actions))
    print(f"âœ… n_actions = {n_actions}")

# ==== ç¡®å®š/æ¢æµ‹é€šé“æ•°ï¼ˆä½ ä¹Ÿå¯ä»¥å†™æ­» 11ï¼‰====
# å¦‚æœä½ ä¹‹å‰å·²ç»é€šè¿‡æ¢é’ˆæ‹¿åˆ°äº† in_channelsï¼Œå°±ç”¨é‚£ä¸ªï¼›å¦åˆ™å…ˆå–ä¸€ä¸ªè§‚æµ‹æ¢æµ‹
    probe_name = next(iter(map_settings))
    probe_env  = build_env_from_raw(map_settings[probe_name])
    obs0, _    = probe_env.reset()
    state0     = obs0[0] if isinstance(obs0, (list, tuple)) else obs0

# è¿™é‡Œå¯ç›´æ¥å›ºå®šï¼šin_channels = 11
# æˆ–è€…å¦‚æœä½ å·²ç»å®ç°äº†è‡ªåŠ¨æ¢æµ‹ï¼Œå°±æ›¿æ¢æˆæ¢æµ‹ç»“æœ
    in_channels = 11

# ==== ç»Ÿä¸€é¢„å¤„ç†å‡½æ•° ====
    def preprocess_batch1(s):
    # è¿”å› [1, C, D, H, W]ï¼Œå¹¶å¼ºåˆ¶ C = in_channels
        return _obs_to_tensor_CDHW(s, device, expected_c=in_channels)

    def preprocess_single(s):
    # è¿”å› [C, D, H, W]ï¼ˆå­˜ç»éªŒ/æ‹¼ batch ç”¨ï¼‰
        return preprocess_batch1(s).squeeze(0)

# ==== é¢„çƒ­ LazyConv3dï¼ˆå¿…é¡»åœ¨ä»»ä½• store()/retrain() ä¹‹å‰ï¼‰====
    with torch.no_grad():
        _ = model(preprocess_batch1(state0))
    print(f"ğŸ”¥ warmed model with in_channels={in_channels}")

# ==== åˆå§‹åŒ– Agentï¼ˆæ³¨æ„ï¼šå‰ä¸‰ä¸ªå¿…å¡«å‚æ•°é¡ºåºï¼šq_network, model, action_spaceï¼‰====
    agent = DDQNAgent(
    model,                 # q_network
    model,                 # modelï¼ˆä¸ä½ çš„ç­¾åä¸€è‡´ï¼Œä¸¤ä¸ªéƒ½ä¼ åŒä¸€ä¸ªå®ä¾‹ï¼‰
    action_space_list,     # action_space: [0..n_actions-1]
    lr=lr,
    decay_range=decay_range,
    device=device,
    replay_buffer_size=replay_buffer_size,
    # ä½ çš„ç±»è‹¥æ”¯æŒè¿™ä¸ªå…³é”®å­—å°±ä¼ ï¼›å¦åˆ™åˆ æ‰
    obs_preprocessor=preprocess_single,
    )

# ç»Ÿä¸€é¢„å¤„ç†ï¼šå’Œä½ åšåŠ¨ä½œé€‰æ‹©æ—¶åŒä¸€å‡½æ•°
    agent.obs_preprocessor = lambda s: _obs_to_tensor_CDHW(s, device, expected_c=in_channels)




    training_logs = []
    pbar = tqdm(range(num_episodes), desc='Episodes', dynamic_ncols=True)

    episode = 0
    success_count_total = 0

    while (scheduler is None) or (scheduler.current_stage <= scheduler.max_stage):
        if episode >= num_episodes:
            break

        # 1) å½“å‰é˜¶æ®µåœ°å›¾
        cur_map_cfg = scheduler.get_updated_map_settings() if scheduler else map_settings
        map_type, cfg = next(iter(cur_map_cfg.items()))
        env = build_env_from_raw(cfg)

        stage_id = (scheduler.current_stage if scheduler else -1)
        cpx_val = cfg.get("complexity", None)
        if cpx_val is not None and np.isfinite(cpx_val):
            pbar.write(f"ğŸŸ¢ åœ°å›¾ï¼š{map_type} | Stage {stage_id} | Agents={env.num_agents} | Complexity={float(cpx_val):.3f}")
        else:
            pbar.write(f"ğŸŸ¢ åœ°å›¾ï¼š{map_type} | Stage {stage_id} | Agents={env.num_agents}")

        # 2) reset
        try:
            obs, info = env.reset()
        except Exception:
            obs = env.reset()
            info = {}

        # ç›®æ ‡ä»£ç†ï¼ˆå…¶å®ƒç”¨ A* åšé˜Ÿå‹ï¼‰
        target_idx = np.random.randint(env.num_agents)
        teammates = [AStarAgent() if i != target_idx else None for i in range(env.num_agents)]
        goal = tuple(env.goals[target_idx])
        state = obs[target_idx]

        # 3) ä¸€é›†
        success_flag = False
        retrain_count = 0
        episode_start_time = time.time()

        # ä½ åŸæœ¬çš„æ­¥æ•°å¢é•¿ç­–ç•¥
        timesteps_per_episode = 50 + 10 * episode

        for t in range(timesteps_per_episode):
            if time.time() - episode_start_time > max_episode_seconds:
                pbar.write(f"â° Episode {episode} è¶…æ—¶ï¼ˆ>{max_episode_seconds}sï¼‰")
                break

            # ===== æ›¿æ¢å¼€å§‹ï¼šæˆ‘ä»¬æ™ºèƒ½ä½“çš„åŠ¨ä½œé€‰æ‹©ï¼ˆç¡®ä¿å–‚ç»™ç½‘ç»œçš„æ˜¯ [1, in_channels, D, H, W]ï¼‰ =====
            actions = []
            for i in range(env.num_agents):
                if i == target_idx:
        # å…³é”®ï¼šç»Ÿä¸€è§‚æµ‹åˆ° [1, in_channels, D, H, W]
                    x = _obs_to_tensor_CDHW(obs[i], device, expected_c=in_channels)

        # ä¸€æ¬¡æ€§é˜²å¾¡æ€§æ£€æŸ¥ï¼ˆæ¨èä¿ç•™ï¼Œå®šä½ç»´åº¦é—®é¢˜éå¸¸æœ‰ç”¨ï¼‰
                    if x.shape[1] != in_channels:
                        raise RuntimeError(f"[PrepMismatch] x.shape={tuple(x.shape)}, expected in_channels={in_channels}")

                    with torch.no_grad():
                        q = agent.q_network(x)
                        a = int(torch.argmax(q, dim=1).item())
                    actions.append(a)
                else:
                    try:
                        actions.append(int(teammates[i].act(obs[i])))
                    except Exception:
                        actions.append(0)
# ===== æ›¿æ¢ç»“æŸ =====



            obs, reward, terminated, truncated, info = env.step(actions)

            # åˆ°è¾¾åˆ¤å®š
            agent_pos = tuple(obs[target_idx]['global_xy'])
            done = (agent_pos == goal)
            terminated[target_idx] = done

            if done:
                success_flag = True
                break

            # ç»éªŒ & å­¦ä¹ 
            agent.store(
                state,
                actions[target_idx],
                reward[target_idx],
                obs[target_idx],
                terminated[target_idx],
            )
            state = obs[target_idx]

            if len(agent.replay_buffer) >= batch_size:
                retrain_count += 1
                _ = agent.retrain(batch_size)

        # 4) ç»Ÿè®¡ä¸æ—¥å¿—
        if success_flag:
            success_count_total += 1

        episode += 1
        success_rate = success_count_total / max(1, episode)
        writer.add_scalar('Success/episode', int(success_flag), episode)
        writer.add_scalar('SuccessRate/global', success_rate, episode)

        pbar.set_postfix(
            Stage=(stage_id if scheduler else "-"),
            success=int(success_flag),
            SR=f"{success_rate:.3f}",
        )
        pbar.update(1)  # â† åˆ«å¿˜äº†æ¨è¿›è¿›åº¦æ¡

        training_logs.append({
            "episode": episode,
            "stage": stage_id,
            "map": map_type,
            "agents": getattr(env, "num_agents", None),
            "complexity": (float(cpx_val) if cpx_val is not None else np.nan),
            "success": int(success_flag),
            "success_rate": float(success_rate),
        })

        # 5) è¯¾ç¨‹é€»è¾‘
        if scheduler is not None:
            scheduler.add_episode_result(int(success_flag))
            if scheduler.should_advance():
                scheduler.advance(pbar)
                if scheduler.is_done():
                    break
            else:
                if scheduler._ep_in_stage >= scheduler.min_episodes_per_stage:
                    scheduler.repeat_stage(pbar)

    # ä¿å­˜æ—¥å¿—
    df = pd.DataFrame(training_logs)
    run_dir.mkdir(parents=True, exist_ok=True)
    csv_path = run_dir / "episodes.csv"
    df.to_csv(csv_path, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ æ¯é›†æ—¥å¿—å·²ä¿å­˜ï¼š{csv_path}")

    writer.close()
    return agent

def _find_array(obj):
    """åœ¨ obj(å¯èƒ½æ˜¯dict/list/tuple/ndarray/torch.Tensor) ä¸­é€’å½’å¯»æ‰¾ç¬¬ä¸€ä¸ª ndarray æˆ–å¯è½¬ ndarray çš„å¯¹è±¡ã€‚"""
    import numpy as np
    try:
        import torch
        is_tensor = True
    except Exception:
        is_tensor = False

    # ç›´æ¥æ˜¯ ndarray
    if isinstance(obj, np.ndarray):
        return obj
    # torch tensor
    if is_tensor and hasattr(obj, "detach") and hasattr(obj, "cpu") and hasattr(obj, "numpy"):
        try:
            return obj.detach().cpu().numpy()
        except Exception:
            pass
    # åˆ—è¡¨/å…ƒç»„ï¼šä¾æ¬¡æ‰¾
    if isinstance(obj, (list, tuple)):
        for v in obj:
            arr = _find_array(v)
            if arr is not None:
                return arr
        return None
    # å­—å…¸ï¼šä¼˜å…ˆå¸¸ç”¨é”®ï¼Œå†å…¨é‡éå†
    if isinstance(obj, dict):
        preferred = ("obs", "observation", "view", "state", "tensor", "grid", "image", "local_obs", "global_obs")
        for k in preferred:
            if k in obj:
                arr = _find_array(obj[k])
                if arr is not None:
                    return arr
        # é€€åŒ–ï¼šéå†æ‰€æœ‰ value
        for v in obj.values():
            arr = _find_array(v)
            if arr is not None:
                return arr
        return None

    # æ ‡é‡/å…¶ä»–ï¼šå°è¯•ç›´æ¥è½¬
    try:
        arr = np.array(obj)
        if arr.ndim > 0:  # è‡³å°‘ 1 ç»´æ‰ç®—â€œåƒæ•°ç»„â€
            return arr
    except Exception:
        pass
    return None


def _to_CDHW(arr):
    """æŠŠ ndarray ç»Ÿä¸€æˆ [C, D, H, W]ã€‚å…è®¸è¾“å…¥ç»´åº¦ä¸º 2/3/4/5ã€‚"""
    import numpy as np
    if not isinstance(arr, np.ndarray):
        arr = np.array(arr)

    if arr.ndim == 5:
        # å¯èƒ½æ˜¯ [N,C,D,H,W] æˆ– [N,D,H,W,C]
        if arr.shape[1] in (1,2,3,4,5,8,11,16):  # çœ‹ç¬¬2ç»´åƒâ€œé€šé“â€
            arr = arr[0]  # -> [C,D,H,W]
        else:
            # å‡å®šæ˜¯ [N,D,H,W,C]
            arr = np.transpose(arr, (0,4,1,2,3))[0]  # -> [C,D,H,W]
    elif arr.ndim == 4:
        # å¯èƒ½æ˜¯ [C,D,H,W] æˆ– [D,H,W,C]
        if arr.shape[0] in (1,2,3,4,5,8,11,16):
            pass  # å·²æ˜¯ [C,D,H,W]
        elif arr.shape[-1] in (1,2,3,4,5,8,11,16):
            arr = np.transpose(arr, (3,0,1,2))  # DHWC -> CDHW
        else:
            # ä¸ç¡®å®šï¼ŒæŠŠæœ€åä¸€ç»´å½“é€šé“ï¼šH W C D -> C H W D å†è°ƒå› C D H Wï¼ˆæå°‘è§ï¼‰
            arr = np.transpose(arr, (3,0,1,2))
    elif arr.ndim == 3:
        # [D,H,W] -> [1,D,H,W]
        arr = arr[None, ...]
    elif arr.ndim == 2:
        # [H,W] -> [1,1,H,W]
        arr = arr[None, None, ...]
    else:
        raise ValueError(f"æ— æ³•å½’ä¸€åˆ° [C,D,H,W]ï¼šarr.ndim={arr.ndim}, shape={arr.shape}")
    return arr

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    os.makedirs('logs', exist_ok=True)
    os.makedirs('models', exist_ok=True)

    # 1) è¯» YAML
    with open(MAP_SETTINGS_PATH, "r", encoding="utf-8") as f:
        base_map_settings = yaml.safe_load(f)

    if isinstance(base_map_settings, list):
        base_map_settings = {(m.get("name") or f"map_{i}"): m for i, m in enumerate(base_map_settings)}

    # 2) åˆå¹¶å¤æ‚åº¦ï¼ˆæ¥è‡ª CSV çš„ nn_complexityï¼‰
    base_map_settings = _merge_complexity_from_csv(base_map_settings)

    # 3) æ„å»º Scheduler
    scheduler = ComplexityScheduler(
        base_map_settings=base_map_settings,
        n_stages=N_STAGES,
        min_per_stage=MIN_PER_STAGE,
        min_episodes_per_stage=MIN_EPISODES_PER_STAGE,
        threshold=THRESHOLD,
        window_size=WINDOW_SIZE,
        use_window_sr=True,
        shuffle_each_stage=True,
        seed=0,
    )



    # åœ¨åˆ›å»º scheduler ä¹‹åã€åˆ›å»º model ä¹‹å‰ï¼š
    first_name = next(iter(base_map_settings))
    probe_env = build_env_from_raw(base_map_settings[first_name])
    # ---------- è§‚æµ‹æå–ä¸æ•´å½¢å·¥å…· ----------
    probe_name = next(iter(base_map_settings))
    probe_env  = build_env_from_raw(base_map_settings[probe_name])
    obs_info   = probe_env.reset()
# å…¼å®¹ (obs, info) æˆ–ä»… obs
    if isinstance(obs_info, tuple) and len(obs_info) >= 1:
        obs = obs_info[0]
    else:
        obs = obs_info

    state0 = obs[0] if isinstance(obs, (list, tuple)) and len(obs) > 0 else obs

    arr_raw = _find_array(state0)
    if arr_raw is None:
    # æ‰“å°å¯ç”¨é”®å¸®åŠ©å®šä½
        if isinstance(state0, dict):
            print("âŒ åœ¨ state0 ä¸­æœªæ‰¾åˆ°å¯ç”¨æ•°ç»„ã€‚state0.keys():", list(state0.keys()))
        else:
            print("âŒ åœ¨ state0 ä¸­æœªæ‰¾åˆ°å¯ç”¨æ•°ç»„ã€‚type(state0):", type(state0))
        raise ValueError("æ— æ³•ä»è§‚æµ‹ä¸­æå–å¼ é‡ï¼Œè¯·å‘Šè¯‰æˆ‘ state ç»“æ„æˆ–è´´ä¸€æ®µæ ·ä¾‹ï¼Œæˆ‘æ¥é€‚é…ã€‚")

    arr_cdwh = _to_CDHW(arr_raw)     # -> [C,D,H,W]
    in_channels = 11
    print(f"ğŸ§ª æ¨æ–­è§‚æµ‹é€šé“æ•° in_channels={in_channels}, sample shape={arr_cdwh.shape}")




# æ³¨æ„ï¼šDDQNAgent é‡Œé€šå¸¸ä¼šä» action_space.n æ¨æ–­ num_actionsï¼Œ
# å¦‚æœéœ€è¦ä¼ å…¥ï¼Œä¹Ÿç”¨æˆ‘ä»¬ä¸Šä¸€æ­¥æ¨æ–­çš„åŠ¨ä½œæ•° n_actionsã€‚


    # 4) æ¨¡å‹ä¸è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CRNNModel().to(device)
    # é¢„çƒ­ LazyConv3dï¼Œè®©å®ƒä»¥ä½ æœŸæœ›çš„é€šé“æ•°å®šå‹ï¼ˆin_channelsï¼‰
    x0 = _obs_to_tensor_CDHW(state0, device, expected_c=in_channels)  # [1,C,D,H,W]
    with torch.no_grad():
        _ = model(x0)  # é¦–æ¬¡å‰å‘ï¼ŒLazyConv3d å›ºå®š in_channels=C


    # 5) è®­ç»ƒ
    agent = train(
        model=model,
        scheduler=scheduler,
        map_settings=scheduler.get_updated_map_settings(),  # åˆæ¬¡å–ä¸€å¼ ï¼›train å†…æ¯é›†ä¼šå†å–
        map_probs=None,
        num_episodes=NUM_EPISODES,
        batch_size=BATCH_SIZE,
        replay_buffer_size=REPLAY_BUFFER_SIZE,
        decay_range=DECAY_RANGE,
        log_dir=LOG_DIR,
        device=device,
        max_episode_seconds=MAX_EPISODE_SECONDS,
        run_dir=RUN_DIR,
    )

    # 6) ä¿å­˜æ¨¡å‹æƒé‡
    out_path = Path(MODEL_OUT)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    torch.save(model.state_dict(), out_path.as_posix())
    print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ° {out_path}")
