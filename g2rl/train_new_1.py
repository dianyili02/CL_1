#!/usr/bin/env python3
# -*- coding: utf-8 -*-

from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Union
from collections import defaultdict
import os, sys, time, inspect, random

import numpy as np
import pandas as pd
import torch
from torch.utils.tensorboard import SummaryWriter
import yaml
from tqdm import tqdm
import matplotlib.pyplot as plt
from pogema import AStarAgent

# ================== è·¯å¾„ä¸é¡¹ç›®æ¨¡å— ==================
project_root = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train"
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from g2rl.environment import G2RLEnv
from g2rl.agent import DDQNAgent
from g2rl.network import CRNNModel
from g2rl import moving_cost, detour_percentage  # è‹¥æœªç”¨åˆ°ä¹Ÿä¿ç•™ï¼Œä¾¿äºæ‰©å±•

# ================== é€šç”¨å·¥å…· ==================
def get_timestamp() -> str:
    return datetime.now().strftime('%H-%M-%d-%m-%Y')

def _np_grid(g):
    arr = np.array(g, dtype=np.uint8)
    if arr.ndim != 2:
        raise ValueError("grid must be 2D")
    return (arr > 0).astype(np.uint8)

# def build_env_from_raw(raw_cfg: dict) -> G2RLEnv:
#     """ä»…å°† G2RLEnv.__init__ æ”¯æŒçš„é”®ä¼ å…¥ï¼›grid/starts/goals ç­‰æŒ‚å±æ€§ã€‚"""
#     sig = inspect.signature(G2RLEnv.__init__)
#     allowed = {
#         p.name for p in sig.parameters.values()
#         if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY)
#     }
#     allowed.discard("self")
#     rename = {}  # å¦‚æœ YAML é”®åä¸ __init__ ä¸ä¸€è‡´ï¼Œå¯åœ¨è¿™é‡Œåšæ˜ å°„

#     ctor_cfg = {}
#     for k, v in raw_cfg.items():
#         kk = rename.get(k, k)
#         if kk in allowed:
#             ctor_cfg[kk] = v

#     env = G2RLEnv(**ctor_cfg)

#     # å°† grid/starts/goals ä½œä¸ºå±æ€§æŒ‚è½½ï¼ˆä¸ä¼ å…¥ __init__ï¼‰
#     if "grid" in raw_cfg:
#         try:
#             env.grid = _np_grid(raw_cfg["grid"])
#         except Exception:
#             env.grid = None
#     if "starts" in raw_cfg:
#         env.starts = raw_cfg["starts"]
#     if "goals" in raw_cfg:
#         env.goals = raw_cfg["goals"]
#     return env
#


# teståŠ¨ç”»ç”¨
def build_env_from_raw(raw_cfg: dict) -> G2RLEnv:
    sig = inspect.signature(G2RLEnv.__init__)
    allowed = {
        p.name for p in sig.parameters.values()
        if p.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD, inspect.Parameter.KEYWORD_ONLY)
    }
    allowed.discard("self")

    # è¿™é‡Œé»˜è®¤å…³é—­å¸¸è§â€œç›®æ ‡é‡ç”Ÿ/é‡åˆ†é…/é‡æ’â€å¼€å…³ï¼ˆå¦‚æœæ„é€ å™¨æ”¯æŒï¼‰
    defaults_to_disable = {
        "respawn_goals": False,
        "reassign_goals": False,
        "shuffle_agents": False,
        "reset_goals_each_step": False,
    }

    ctor_cfg = {}
    for k, v in raw_cfg.items():
        if k in allowed:
            ctor_cfg[k] = v
    for k, v in defaults_to_disable.items():
        if (k in allowed) and (k not in ctor_cfg):
            ctor_cfg[k] = v

    env = G2RLEnv(**ctor_cfg)

    # grid/starts/goals ä»æŒ‰åŸé€»è¾‘æŒ‚å±æ€§
    if "grid" in raw_cfg:
        try:
            env.grid = _np_grid(raw_cfg["grid"])
        except Exception:
            env.grid = None
    if "starts" in raw_cfg:
        env.starts = raw_cfg["starts"]
    if "goals" in raw_cfg:
        env.goals = raw_cfg["goals"]
    return env



# ================== ç‰¹å¾æŠ½å–ï¼ˆç”¨äºâ€œéš¾åº¦è¯„åˆ†â€ä¸åˆ†æï¼‰ ==================
def extract_features_from_spec(name: str, spec: dict) -> Dict[str, float]:
    """
    ä» YAML æ¡ç›®ä¸ grid ä¸­æŠ½å–ç‰¹å¾ã€‚
    ä¿ç•™ Size / Agents / Density / Density_actual + FRA/FDA/BN/LDD/MC/DLRï¼ˆè‹¥å­˜åœ¨ï¼‰ã€‚
    """
    feats = {}
    feats["map_name"]   = name
    feats["Size"]       = float(spec.get("size", spec.get("Size", np.nan)))
    feats["Agents"]     = float(spec.get("num_agents", spec.get("Agents", np.nan)))
    feats["Density"]    = float(spec.get("density", spec.get("Density", np.nan)))

    # density_actualï¼šæŒ‰ grid çœŸå®éšœç¢å æ¯”ï¼ˆæœ‰ grid æ—¶è®¡ç®—ï¼‰
    try:
        if "grid" in spec and spec["grid"] is not None:
            g = _np_grid(spec["grid"])
            feats["Density_actual"] = float(g.mean())
        else:
            feats["Density_actual"] = np.nan
    except Exception:
        feats["Density_actual"] = np.nan

    # å…¶ä»–ç‰¹å¾ï¼šå¦‚æœ YAML å·²ç®—å¥½å°±å¸¦ä¸Š
    for k in ["FRA", "FDA", "BN", "LDD", "MC", "DLR"]:
        if k in spec:
            try:
                feats[k] = float(spec[k])
            except Exception:
                feats[k] = np.nan
        else:
            feats[k] = np.nan
    return feats

# ================== æˆåŠŸåˆ¤å®šè¾…åŠ© ==================
def _bool_from(x, idx: int) -> bool:
    """æŠŠ terminated/truncated è§„æ•´ä¸ºç›®æ ‡ä»£ç†çš„å¸ƒå°”ã€‚"""
    if isinstance(x, (list, tuple, np.ndarray)):
        if len(x) == 0: 
            return False
        if 0 <= idx < len(x):
            return bool(x[idx])
        return bool(any(x))
    if isinstance(x, dict):
        cand_keys = [idx, "global", "any", "done", "terminated"]
        for key in cand_keys:
            if key in x:
                try:
                    return bool(x[key])
                except Exception:
                    pass
        try:
            return bool(next(iter(x.values())))
        except Exception:
            return False
    try:
        return bool(x)
    except Exception:
        return False

def _success_from_info(info, idx: int) -> Optional[bool]:
    """ä» info ä¸­æŠ½å–æˆåŠŸä¿¡å·ï¼ŒæŠ½ä¸åˆ°è¿”å› Noneã€‚"""
    if not isinstance(info, dict):
        return None
    cand = info.get(idx, info)  # æœ‰äº› env ä¸ºæ¯ä¸ª agent æä¾›å­ dict
    if isinstance(cand, dict):
        for k in ("success", "is_success", "solved", "reached_goal", "done"):
            if k in cand:
                try:
                    return bool(cand[k])
                except Exception:
                    pass
    return None

# ================== å•é›†è¯„ä¼°ï¼ˆå¸¦è¶…æ—¶ä¸æ­¥æ•°ä¸Šé™ï¼‰ ==================
# teståŠ¨ç”»
# === REPLACE your run_one_episode with this ===
def run_one_episode(
    env: G2RLEnv,
    agent: DDQNAgent,
    episode_idx: int,
    max_episode_seconds: int = 30,
    max_steps_cap: Optional[int] = None,
) -> Dict[str, float]:
    try:
        obs, info = env.reset()
    except Exception:
        obs = env.reset()
        info = {}

    # â‘  å›ºå®šåŒä¸€ä¸ª agent
    target_idx = np.random.randint(env.num_agents)
    target_id  = int(obs[target_idx].get('agent_id', target_idx))

    # â‘¡ åªåœ¨ reset åå–ä¸€æ¬¡â€œç»å¯¹ç»ˆç‚¹â€ï¼Œä½œä¸º fixed_goal
    try:
        fixed_goal = tuple(map(int, env.goals[target_idx]))
    except Exception:
        fixed_goal = None

    # â‘¢ å…¶ä»–é˜Ÿå‹ç”¨ A*
    teammates = [agent if i == target_idx else AStarAgent() for i in range(env.num_agents)]

    # å‚è€ƒæœ€çŸ­è·¯å¾„é•¿åº¦ï¼ˆå¯é€‰ï¼‰
    state = obs[target_idx]
    try:
        opt_path = [state['global_xy']] + env.global_guidance[target_idx]
        opt_len = max(1, len(opt_path) - 1)
    except Exception:
        opt_len = 1

    # æ­¥æ•°é¢„ç®—
    timesteps_per_episode = 50 + 10 * episode_idx
    max_steps_budget = min(timesteps_per_episode, max_steps_cap) if isinstance(max_steps_cap, int) and max_steps_cap > 0 else timesteps_per_episode

    t0 = time.time()
    success, steps, timeout_flag = 0, 0, 0

    for _ in range(max_steps_budget):
        steps += 1
        if time.time() - t0 > max_episode_seconds:
            timeout_flag = 1
            break

        # è‹¥ obs é¡ºåºä¼šå˜ï¼Œç”¨ agent_id æ‰¾å›åŒä¸€åä»£ç†
        if 'agent_id' in obs[0]:
            target_idx = next(i for i, o in enumerate(obs) if int(o.get('agent_id', -1)) == target_id)

        # A) åˆ°è¾¾â€œå‰â€åˆ¤å®šï¼ˆæœ‰äº› env ä¼šåœ¨ step() æ—¶ç«‹åˆ»æ¢ç›®æ ‡ï¼‰
        try:
            pos_before = tuple(map(int, obs[target_idx]['global_xy']))
        except Exception:
            pos_before = None
        if (fixed_goal is not None) and (pos_before is not None) and (pos_before == fixed_goal):
            success = 1
            break

        # è¡ŒåŠ¨
        actions = [ag.act(o) for ag, o in zip(teammates, obs)]
        out = env.step(actions)
        if len(out) == 5:
            obs, reward, terminated, truncated, info = out
        else:
            obs, reward, done, info = out
            terminated, truncated = done, False

        # B) åˆ°è¾¾â€œåâ€ç«‹å³åˆ¤å®šï¼Œç”¨ fixed_goalï¼ˆä¸è¦ç”¨ä¼šå˜çš„ global_target_xyï¼‰
        try:
            pos_after = tuple(map(int, obs[target_idx]['global_xy']))
        except Exception:
            pos_after = None
        if (fixed_goal is not None) and (pos_after is not None) and (pos_after == fixed_goal):
            success = 1
            break

        # C) ï¼ˆä»…ç”¨äºä½ çš„æ˜¾ç¤º/æ—¥å¿—ï¼‰å°è¯•æŠŠ env.goals é”å›å»ï¼Œé¿å…ä½ åé¢æ‰“å°çœ‹åˆ°â€œå˜äº†â€
        try:
            env.goals[target_idx] = fixed_goal
        except Exception:
            pass

        # ç»éªŒå›æ”¾/å­¦ä¹ 
        r_target = reward[target_idx] if isinstance(reward, (list, tuple, np.ndarray)) else reward
        agent.store(
            state,
            actions[target_idx],
            r_target,
            obs[target_idx],
            bool(terminated[target_idx] if isinstance(terminated, (list, tuple, np.ndarray)) else terminated),
        )
        state = obs[target_idx]

    return {
        "success": int(success),
        "steps": int(steps),
        "opt_len": float(opt_len),
        "timeout": int(timeout_flag or (steps >= max_steps_budget)),
    }
# === END REPLACE ===

# def run_one_episode(
#     env: G2RLEnv,
#     agent: DDQNAgent,
#     episode_idx: int,
#     max_episode_seconds: int = 30,
#     max_steps_cap: Optional[int] = None,
# ) -> Dict[str, float]:
#     try:
#         obs, info = env.reset()
#     except Exception:
#         obs = env.reset()
#         info = {}

#     # â‘  ç»‘å®šå›ºå®šçš„ç›®æ ‡ä»£ç†
#     target_idx = np.random.randint(env.num_agents)
#     target_id  = int(obs[target_idx].get('agent_id', target_idx))  # è‹¥æ—  agent_idï¼Œå°±ç”¨ç´¢å¼•å…œåº•

#     # â‘¡ å›ºå®šä¸€æ¬¡æ€§çš„â€œç»å¯¹ç›®æ ‡åæ ‡â€ï¼ˆç»ä¸å†æ”¹ï¼‰
#     try:
#         fixed_goal = tuple(map(int, env.goals[target_idx]))
#     except Exception:
#         fixed_goal = None

#     # â‘¢ å…¶ä½™é˜Ÿå‹ç”¨ A*
#     teammates = [agent if i == target_idx else AStarAgent() for i in range(env.num_agents)]

#     # å‚è€ƒæœ€ä¼˜è·¯å¾„é•¿åº¦ï¼ˆå¯é€‰ï¼‰
#     state = obs[target_idx]
#     try:
#         opt_path = [state['global_xy']] + env.global_guidance[target_idx]
#         opt_len = max(1, len(opt_path) - 1)
#     except Exception:
#         opt_len = 1

#     # æ­¥æ•°é¢„ç®—
#     timesteps_per_episode = 50 + 10 * episode_idx
#     max_steps_budget = min(timesteps_per_episode, max_steps_cap) if isinstance(max_steps_cap, int) and max_steps_cap > 0 else timesteps_per_episode

#     t0 = time.time()
#     success, steps, timeout_flag = 0, 0, 0

#     for _ in range(max_steps_budget):
#         steps += 1
#         if time.time() - t0 > max_episode_seconds:
#             timeout_flag = 1
#             break

#         # è‹¥ obs é¡ºåºå¯èƒ½å˜åŒ–ï¼šç”¨ agent_id æ‰¾å›åŒä¸€åä»£ç†çš„ç´¢å¼•
#         if 'agent_id' in obs[0]:
#             target_idx = next(i for i, o in enumerate(obs) if int(o.get('agent_id', -1)) == target_id)

#         actions = [ag.act(o) for ag, o in zip(teammates, obs)]
#         out = env.step(actions)
#         if len(out) == 5:
#             obs, reward, terminated, truncated, info = out
#         else:
#             obs, reward, done, info = out
#             terminated, truncated = done, False

#         # åªç”¨ç»å¯¹åæ ‡ä¸â€œå›ºå®šç›®æ ‡â€åˆ¤å®šæˆåŠŸ
#         try:
#             pos = tuple(map(int, obs[target_idx]['global_xy']))
#         except Exception:
#             pos = None

#         if (fixed_goal is not None) and (pos is not None) and (pos == fixed_goal):
#             success = 1
#             break

#         # å…œåº•ï¼šinfo è‹¥æ˜ç¡®ç»™æˆåŠŸä¿¡å·
#         if isinstance(info, dict):
#             sub = info.get(target_idx, info)
#             if isinstance(sub, dict) and any(k in sub and bool(sub[k]) for k in ('success', 'is_success', 'reached_goal', 'done')):
#                 success = 1
#                 break

#         # å…¼å®¹ done ä¿¡å·ï¼šå†å¯¹æ¯”ä¸€æ¬¡å›ºå®šç›®æ ‡
#         if (isinstance(terminated, (list, tuple, np.ndarray)) and 0 <= target_idx < len(terminated) and terminated[target_idx]) \
#            or (isinstance(truncated, (list, tuple, np.ndarray)) and 0 <= target_idx < len(truncated) and truncated[target_idx]):
#             if (fixed_goal is not None) and (pos is not None) and (pos == fixed_goal):
#                 success = 1
#             break

#         # ç»éªŒå›æ”¾ï¼ˆå¦‚éœ€åœ¨çº¿å­¦ä¹ ï¼‰
#         r_target = reward[target_idx] if isinstance(reward, (list, tuple, np.ndarray)) else reward
#         agent.store(state, actions[target_idx], r_target, obs[target_idx],
#                     bool(terminated[target_idx] if isinstance(terminated, (list, tuple, np.ndarray)) else terminated))
#         state = obs[target_idx]

#     return {"success": int(success), "steps": int(steps), "opt_len": float(opt_len),
#             "timeout": int(timeout_flag or (steps >= max_steps_budget))}

# ================== â€œéš¾åº¦è¯„åˆ†â€ & è¯¾ç¨‹æ„å»ºï¼ˆä¸ä½¿ç”¨ complexityï¼‰ ==================
def compute_difficulty_from_features(df_feats: pd.DataFrame) -> pd.DataFrame:
    """
    è®¡ç®—ä¸€ä¸ªå¯æ§çš„ difficulty_scoreï¼š
      Agentsâ†‘ã€Density/Density_actualâ†‘ã€Sizeâ†‘ => éš¾åº¦â†‘
      FRA/FDAâ†‘ï¼ˆæ›´æ˜“é€šè¡Œï¼‰ => éš¾åº¦â†“
    åš 0~1 å½’ä¸€åŒ–åçº¿æ€§åŠ æƒï¼Œä¾¿äºè°ƒæ•´ã€‚
    """
    d = df_feats.copy()

    dens_act = d["Density_actual"] if "Density_actual" in d else d["Density"]

    cols = {
        "Size": d.get("Size", pd.Series(np.nan, index=d.index)),
        "Agents": d.get("Agents", pd.Series(np.nan, index=d.index)),
        "DensityA": dens_act,
        "Density": d.get("Density", pd.Series(np.nan, index=d.index)),
        "FRA": d.get("FRA", pd.Series(np.nan, index=d.index)),
        "FDA": d.get("FDA", pd.Series(np.nan, index=d.index)),
    }
    X = pd.DataFrame(cols)

    def _minmax(s: pd.Series):
        s = s.astype(float)
        if s.notna().sum() <= 1:
            return s
        lo, hi = np.nanmin(s.values), np.nanmax(s.values)
        if not np.isfinite(lo) or not np.isfinite(hi) or (hi - lo) < 1e-12:
            return s
        return (s - lo) / (hi - lo)

    Xn = X.apply(_minmax)

    # æƒé‡ï¼ˆå¯æŒ‰éœ€å¾®è°ƒï¼‰
    w_agents   = 0.50
    w_density  = 0.30
    w_size     = 0.20
    w_fra_good = 0.10
    w_fda_good = 0.10

    dens_for_score = Xn["DensityA"].fillna(Xn["Density"])

    diff = (
        w_agents  * Xn["Agents"].fillna(0.5) +
        w_density * dens_for_score.fillna(0.5) +
        w_size    * Xn["Size"].fillna(0.5) -
        w_fra_good * Xn["FRA"].fillna(0.0) -
        w_fda_good * Xn["FDA"].fillna(0.0)
    )

    d["difficulty_score"] = diff.values
    med = np.nanmedian(d["difficulty_score"])
    d["difficulty_score"] = d["difficulty_score"].fillna(med if np.isfinite(med) else 0.5)
    return d

def build_curriculum_buckets(maps_cfg: Dict[str, dict], n_stages: int = 5, min_per_stage: int = 1):
    """
    1) æå–æ¯å›¾ç‰¹å¾ â†’ 2) è®¡ç®— difficulty_score â†’ 3) æ’åº â†’ 4) åˆ†ä½æ•°åˆ‡å‰²æˆ n ä¸ª stageã€‚
    è¿”å›ï¼šList[List[(map_name, spec, feature_row_dict)]] ä¸ df_scoredï¼ˆå« difficulty_scoreï¼‰ã€‚
    """
    rows = []
    for name, spec in maps_cfg.items():
        feats = extract_features_from_spec(name, spec)
        rows.append(feats)
    df_feats = pd.DataFrame(rows)

    df_scored = compute_difficulty_from_features(df_feats).sort_values("difficulty_score").reset_index(drop=True)

    qs = np.linspace(0, 1, n_stages + 1)
    edges = np.quantile(df_scored["difficulty_score"].values, qs)
    buckets = []
    for i in range(n_stages):
        lo, hi = float(edges[i]), float(edges[i+1]) + 1e-12
        sub = df_scored[(df_scored["difficulty_score"] >= lo) & (df_scored["difficulty_score"] < hi)]
        if len(sub) < min_per_stage and len(df_scored) >= min_per_stage:
            need = min_per_stage - len(sub)
            center = (lo + hi) / 2.0
            extra = df_scored.iloc[(df_scored["difficulty_score"] - center).abs().argsort()[:need]]
            sub = pd.concat([sub, extra]).drop_duplicates(subset=["map_name"])

        bucket = []
        for _, r in sub.iterrows():
            name = str(r["map_name"])
            bucket.append((name, maps_cfg[name], r.to_dict()))

        # å»é‡
        seen, uniq = set(), []
        for item in bucket:
            if item[0] not in seen:
                uniq.append(item)
                seen.add(item[0])
        buckets.append(uniq)

    return buckets, df_scored

# ================== å¯è§†åŒ–ï¼šFeatures vs Success ==================
def make_feature_success_plots(out_dir: Union[str, Path], df: pd.DataFrame):
    """
    ç”Ÿæˆï¼š
    - success_rate by map çš„æŸ±çŠ¶å›¾
    - æ¯ä¸ªç‰¹å¾ vs success_rate çš„æ•£ç‚¹å›¾
    - ç‰¹å¾+æˆåŠŸç‡çš„ç›¸å…³æ€§çƒ­å›¾
    """
    out_dir = Path(out_dir)
    out_dir.mkdir(parents=True, exist_ok=True)

    # 1) æ¯å›¾æˆåŠŸç‡æŸ±çŠ¶å›¾
    try:
        df_sorted = df.sort_values("success_rate")
        plt.figure(figsize=(max(8, 0.35*len(df_sorted)), 5))
        plt.bar(df_sorted["map_name"].astype(str), df_sorted["success_rate"].values)
        plt.xticks(rotation=60, ha="right")
        plt.ylabel("Success Rate")
        plt.title("Success Rate by Map")
        plt.tight_layout()
        plt.savefig(out_dir / "sr_by_map.png", dpi=150)
        plt.close()
    except Exception as e:
        print(f"[Plot] sr_by_map.png å¤±è´¥: {e}")

    # 2) æ¯ä¸ªæ•°å€¼ç‰¹å¾ vs success_rate æ•£ç‚¹å›¾
    numeric_cols = []
    for c in df.columns:
        if c in ["map_name", "success_rate", "stage", "difficulty_score"]:
            continue
        try:
            if pd.api.types.is_numeric_dtype(df[c]):
                numeric_cols.append(c)
        except Exception:
            pass

    for c in numeric_cols:
        try:
            d = df.dropna(subset=[c, "success_rate"])
            if len(d) == 0:
                continue
            plt.figure(figsize=(6, 4))
            plt.scatter(d[c].values, d["success_rate"].values, s=18)
            plt.xlabel(c)
            plt.ylabel("Success Rate")
            plt.title(f"Success Rate vs {c}")
            plt.grid(True, alpha=0.3)
            plt.tight_layout()
            plt.savefig(out_dir / f"success_vs_{c}.png", dpi=150)
            plt.close()
        except Exception as e:
            print(f"[Plot] success_vs_{c}.png å¤±è´¥: {e}")

    # 3) ç›¸å…³æ€§çƒ­å›¾
    try:
        # ç”¨æ¯å›¾èšåˆåçš„æ•°æ®ï¼ˆé¿å…æ¯é˜¶æ®µé‡å¤ï¼‰
        corr_cols = ["success_rate"] + [c for c in numeric_cols if c in df.columns]
        dnum = df[corr_cols].copy()
        dnum = dnum.apply(pd.to_numeric, errors="coerce")
        cm = dnum.corr().values
        labels = dnum.columns.tolist()

        plt.figure(figsize=(1.2*len(labels), 1.0*len(labels)))
        im = plt.imshow(cm, cmap="coolwarm", vmin=-1, vmax=1)
        plt.colorbar(im, fraction=0.046, pad=0.04)
        plt.xticks(range(len(labels)), labels, rotation=60, ha="right")
        plt.yticks(range(len(labels)), labels)
        plt.title("Correlation (numeric features & success_rate)")
        for i in range(len(labels)):
            for j in range(len(labels)):
                plt.text(j, i, f"{cm[i,j]:.2f}", ha="center", va="center", fontsize=8)
        plt.tight_layout()
        plt.savefig(out_dir / "correlation_heatmap.png", dpi=150)
        plt.close()
    except Exception as e:
        print(f"[Plot] correlation_heatmap.png å¤±è´¥: {e}")

# ================== è¯¾ç¨‹è®­ç»ƒï¼ˆç”±æ˜“åˆ°éš¾ï¼Œè¦†ç›–å…¨éƒ¨åœ°å›¾ï¼‰ ==================
def train_with_curriculum(
    model: torch.nn.Module,
    buckets: List[List[tuple]],
    episodes_per_map_stage: int = 100,  # æ¯é˜¶æ®µã€æ¯å›¾çš„é›†æ•°
    batch_size: int = 64,
    replay_buffer_size: int = 100_000,
    decay_range: int = 50_000,
    lr: float = 5e-4,
    device: str = "cuda",
    log_dir: str = "logs",
    run_dir: Optional[str] = None,
    max_episode_seconds: int = 60,
    max_steps_cap: Optional[int] = 3000,
) -> pd.DataFrame:
    """
    Stage 0(ç®€å•) â†’ ... â†’ Stage N-1(å›°éš¾)ï¼›
    æ¯é˜¶æ®µå¯¹è¯¥æ¡¶å†…æ‰€æœ‰åœ°å›¾å„è·‘ episodes_per_map_stage é›†ã€‚
    """
    run_dir = Path(run_dir or (Path(log_dir) / get_timestamp()))
    run_dir.mkdir(parents=True, exist_ok=True)
    writer = SummaryWriter(log_dir=str(run_dir))

    # ç”¨ç¬¬ä¸€å¼ å›¾çš„åŠ¨ä½œç©ºé—´åˆå§‹åŒ– agent
    first_name, first_spec, _ = buckets[0][0]
    first_env = build_env_from_raw(first_spec)
    agent = DDQNAgent(
        model,
        first_env.get_action_space(),
        lr=lr,
        decay_range=decay_range,
        device=device,
        replay_buffer_size=replay_buffer_size,
    )

    global_rows = []

    for stage_id, bucket in enumerate(buckets):
        print(f"\n====== Stage {stage_id}ï¼ˆ{len(bucket)} mapsï¼‰======")
        for map_name, spec, feats in bucket:
            env = build_env_from_raw(spec)
            pbar = tqdm(range(episodes_per_map_stage), desc=f"Stage{stage_id}[{map_name}]", dynamic_ncols=True)
            succ = 0; timeouts = 0
            for ep in pbar:
                out = run_one_episode(
                    env, agent, ep,
                    max_episode_seconds=max_episode_seconds,
                    max_steps_cap=max_steps_cap
                )
                succ += out["success"]; timeouts += out["timeout"]

                # åœ¨çº¿å­¦ä¹ 
                if len(agent.replay_buffer) >= batch_size:
                    _ = agent.retrain(batch_size)

                # TB
                writer.add_scalar(f"stage_{stage_id}/{map_name}_success", out["success"], ep)
                writer.add_scalar(f"stage_{stage_id}/{map_name}_steps", out["steps"], ep)
                writer.add_scalar(f"stage_{stage_id}/{map_name}_timeout", out["timeout"], ep)

                pbar.set_postfix(sr=f"{succ/(ep+1):.2f}")

            # è®°å½•è¯¥å›¾åœ¨æœ¬é˜¶æ®µè¡¨ç°
            row = dict(feats)
            row.update({
                "stage": stage_id,
                "map_name": map_name,
                "success_rate": succ / max(1, episodes_per_map_stage),
                "timeout_rate": timeouts / max(1, episodes_per_map_stage),
            })
            global_rows.append(row)

    df_all = pd.DataFrame(global_rows)
    out_csv = run_dir / "curriculum_features_vs_success_1.csv"
    df_all.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"ğŸ“ å·²ä¿å­˜ï¼š{out_csv}")

    # èšåˆåˆ°â€œæ¯å›¾ä¸€è¡Œâ€ç”¨äºå¯è§†åŒ–ï¼ˆæˆåŠŸç‡å–å‡å€¼ï¼Œå…¶å®ƒç‰¹å¾å–ç¬¬ä¸€æ¡ï¼‰
    keep_cols = ["Size","Agents","Density","Density_actual","FRA","FDA","BN","LDD","MC","DLR","difficulty_score"]
    agg_dict = {"success_rate":"mean", "timeout_rate":"mean"}
    for c in keep_cols:
        if c in df_all.columns:
            agg_dict[c] = "first"
    df_map = df_all.groupby("map_name", as_index=False).agg(agg_dict)

    try:
        make_feature_success_plots(run_dir, df_map)
        print(f"ğŸ“Š å¯è§†åŒ–è¾“å‡ºåˆ°ï¼š{run_dir}")
    except Exception as e:
        print(f"âš ï¸ å¯è§†åŒ–å¤±è´¥ï¼š{e}")

    # æ‰“å°å…¨å±€å‡å€¼ï¼ˆæŒ‰æ¯å›¾å‡å€¼å†å–å¹³å‡ï¼‰
    global_sr = df_map["success_rate"].mean() if len(df_map) else 0.0
    print(f"ğŸŒ Global Success Rate (mean of per-map SR): {global_sr:.3f}")

    writer.close()
    return df_all

# ================== å…¥å£ ==================
if __name__ == "__main__":
    os.makedirs('logs', exist_ok=True)
    os.makedirs('models', exist_ok=True)

    # è¯»å– YAML
    MAP_SETTINGS_PATH = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/g2rl/map_settings_generated_new.yaml"
    with open(MAP_SETTINGS_PATH, "r", encoding="utf-8") as f:
        base_map_settings = yaml.safe_load(f)

    # é¡¶å±‚ list â†’ dict
    if isinstance(base_map_settings, list):
        base_map_settings = {
            (m.get("name") or f"map_{i}"): m
            for i, m in enumerate(base_map_settings)
        }

    # æ„å»ºè¯¾ç¨‹ï¼ˆç”±æ˜“åˆ°éš¾ï¼Œä¸ç”¨ complexityï¼‰
    buckets, df_scored = build_curriculum_buckets(
        base_map_settings,
        n_stages=5,       # éš¾åº¦é˜¶æ®µæ•°
        min_per_stage=1,  # æ¯é˜¶æ®µè‡³å°‘å‡ å¼ å›¾
    )
    print("ğŸ“š Curriculum bucketsï¼ˆç”±æ˜“åˆ°éš¾ï¼‰:")
    for i, bk in enumerate(buckets):
        names = [n for (n, _, __) in bk]
        print(f"  Stage {i}: {names}")

    # è®¾å¤‡ & æ¨¡å‹
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = CRNNModel().to(device)

    # ç”±æ˜“åˆ°éš¾è¿›è¡Œè¯¾ç¨‹è®­ç»ƒï¼ˆè¦†ç›–å…¨éƒ¨åœ°å›¾ï¼‰
    df_all = train_with_curriculum(
        model=model,
        buckets=buckets,
        episodes_per_map_stage=256,                     # æ¯é˜¶æ®µã€æ¯å¼ å›¾çš„ episode æ•°
        batch_size=64,
        replay_buffer_size=100_000,
        decay_range=50_000,
        lr=5e-4,
        device=device,
        log_dir='logs',
        run_dir=r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/curriculum_run",
        max_episode_seconds=60,                         # å¢™é’Ÿè¶…æ—¶
        max_steps_cap=3000,                             # æ­¥æ•°ä¸Šé™ï¼ˆNone å…³é—­ï¼‰
    )

    # ä¿å­˜æ¨¡å‹
    torch.save(model.state_dict(), 'models/best_model_1.pt')
    print('âœ… æ¨¡å‹å·²ä¿å­˜åˆ° models/best_model_1.pt')


