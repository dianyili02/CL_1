# #!/usr/bin/env python3
# # -*- coding: utf-8 -*-

# """
# Constrained linear regression with sign constraints (cvxpy).
# - è‡ªåŠ¨è¯»å– CSVï¼ˆé»˜è®¤ï¼š/mnt/data/map_complexity.csv æˆ– /mnt/data/features_vs_success.csvï¼‰
# - è‡ªåŠ¨æ¨æ–­ç›®æ ‡åˆ—ï¼ˆä¼˜å…ˆï¼šsuccess_rate â†’ Complexity â†’ complexity â†’ target â†’ yï¼‰
# - è‡ªåŠ¨åŒ¹é…ç‰¹å¾åˆ—ï¼ˆå«åˆ«åï¼šSize/sizeã€Agents/num_agentsã€Density/densityã€Density_actual/density_actual ç­‰ï¼‰
# - å¯¹ç³»æ•°æ–½åŠ â€œæœ‰ç¬¦å·çº¦æŸâ€ï¼ˆå¦‚ num_agents<=0ã€FRA/FDA<=0ã€Size>=0 ç­‰ï¼‰
# - æ‹ŸåˆæˆåŠŸåï¼ŒæŠŠé¢„æµ‹å€¼ä¸æ®‹å·®ä¸¤åˆ—å†™å›åˆ°æ–° CSVï¼š*_with_pred.csvï¼ˆä¸è¦†ç›–åŸè¡¨ï¼‰

# éœ€è¦ä¾èµ–ï¼šcvxpyã€numpyã€pandasï¼ˆè‹¥ OSQP ä¸å¯ç”¨ä¼šè‡ªåŠ¨é€€å› ECOS/ECOS_BBï¼‰
# """

# import os
# import sys
# import json
# import numpy as np
# import pandas as pd

# # ----- é…ç½®åŒºåŸŸï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ -----
# # ä¼˜å…ˆå°è¯•çš„ CSV è·¯å¾„ï¼ˆæ‰¾åˆ°ä¸€ä¸ªå°±ç”¨å®ƒï¼‰
# CSV_CANDIDATES = [
#     "C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/train_gray3d-Copy-FDA.csv",
# ]

# # ç›®æ ‡åˆ—ï¼ˆNone è¡¨ç¤ºè‡ªåŠ¨æ¨æ–­ï¼šsuccess_rateâ†’Complexityâ†’complexityâ†’targetâ†’yï¼‰
# TARGET_COL = None

# # å€™é€‰ç‰¹å¾ï¼ˆç”¨â€œè§„èŒƒåâ€è¡¨ç¤ºï¼‰ï¼›è„šæœ¬ä¼šè‡ªåŠ¨åœ¨ CSV ä¸­æ‰¾åˆ«ååˆ—
# CANDIDATE_FEATURES = [
#     "size", "num_agents", "density", "density_actual",
#     "LDD", "BN", "MC", "DLR", "FRA", "FPA"
# ]

# # ç‰¹å¾çš„ç¬¦å·çº¦æŸï¼ˆé”®ç”¨â€œè§„èŒƒåâ€ï¼‰
# # ">=0": ç³»æ•°éè´Ÿï¼›"<=0": ç³»æ•°éæ­£ï¼›"free": ä¸çº¦æŸ
# SIGN_CONSTRAINT = {
#     "size":            ">=0",
#     "num_agents":      ">=0",
#     "density":         ">=0",
#     "density_actual":  ">=0",
#     "LDD":             ">=0",
#     "BN":              ">=0",
#     "MC":              ">=0",
#     "DLR":             ">=0",
#     "FRA":             ">=0",
#     "FPA":             ">=0",
# }

# # L2 æ­£åˆ™å¼ºåº¦ï¼ˆ0 è¡¨ç¤ºä¸åŠ æ­£åˆ™ï¼‰
# L2_LAMBDA = 1e-3

# # è¾“å‡ºæ–‡ä»¶ååç¼€
# OUTPUT_SUFFIX = "_with_pred.csv"
# WEIGHTS_JSON   = "_weights.json"
# # --------------------------------


# def _find_csv_path(candidates):
#     for p in candidates:
#         if os.path.exists(p):
#             return p
#     raise FileNotFoundError(
#         f"æ‰¾ä¸åˆ° CSVã€‚è¯·æŠŠä½ çš„ CSV æ”¾åœ¨è¿™äº›è·¯å¾„ä¹‹ä¸€ï¼š\n{candidates}"
#     )


# def _import_cvxpy():
#     try:
#         import cvxpy as cp
#         return cp
#     except Exception as e:
#         print("âŒ æœªèƒ½å¯¼å…¥ cvxpyï¼Œè¯·å…ˆå®‰è£…ï¼š pip install cvxpy")
#         raise


# def _pick_target_col(df: pd.DataFrame, target_pref=None) -> str:
#     if target_pref and target_pref in df.columns:
#         return target_pref
#     for cand in ["success_rate", "Complexity", "complexity", "target", "y"]:
#         if cand in df.columns:
#             return cand
#     raise ValueError(
#         "è‡ªåŠ¨é€‰æ‹©ç›®æ ‡åˆ—å¤±è´¥ï¼šæœªæ‰¾åˆ° success_rate/Complexity/complexity/target/y ä¸­ä»»æ„ä¸€ä¸ªã€‚"
#         "è¯·æŠŠ TARGET_COL æ”¹æˆè¡¨å†…å­˜åœ¨çš„åˆ—åã€‚"
#     )


# def _build_feature_mapping(df_cols):
#     """
#     æ„é€ â€œè§„èŒƒåâ†’CSVå®é™…åˆ—åâ€çš„æ˜ å°„ï¼ˆå¤§å°å†™ä¸å¸¸è§åˆ«åå‡å¯ï¼‰ã€‚
#     è§„èŒƒåï¼ˆå³ï¼‰: å¯èƒ½çš„åˆ«åï¼ˆå·¦ï¼‰
#     """
#     alias = {
#         "size":           ["size", "Size"],
#         "num_agents":     ["num_agents", "Agents", "agents", "n_agents"],
#         "density":        ["density", "Density"],
#         "density_actual": ["density_actual", "Density_actual", "actual_density"],
#         "LDD":            ["LDD", "ldd"],
#         "BN":             ["BN", "bn", "Bottleneck", "bottleneck"],
#         "MC":             ["MC", "mc", "MovingCost", "moving_cost"],
#         "DLR":            ["DLR", "dlr"],
#         "FRA":            ["FRA", "fra"],
#         "FPA":            ["FPA", "fpa"],
#     }

#     # å¤§å°å†™ä¸æ•æ„ŸåŒ¹é…
#     lower_cols = {c.lower(): c for c in df_cols}
#     mapping = {}
#     for canonical, cands in alias.items():
#         hit = None
#         for c in cands:
#             if c in df_cols:
#                 hit = c
#                 break
#             if c.lower() in lower_cols:
#                 hit = lower_cols[c.lower()]
#                 break
#         if hit is not None:
#             mapping[canonical] = hit
#     return mapping


# def _solve_with_constraints(X, y, feature_names, sign_constraint, l2_lambda=0.0):
#     cp = _import_cvxpy()

#     n, d = X.shape
#     w = cp.Variable(d)   # ç³»æ•°
#     b = cp.Variable()    # æˆªè·

#     # æ®‹å·® & æŸå¤±
#     resid = X @ w + b - y
#     loss = cp.sum_squares(resid)
#     if l2_lambda and l2_lambda > 0:
#         loss += l2_lambda * cp.sum_squares(w)

#     # æ–½åŠ ç¬¦å·çº¦æŸ
#     constraints = []
#     for j, name in enumerate(feature_names):
#         rule = sign_constraint.get(name, "free")
#         if rule == ">=0":
#             constraints += [w[j] >= 0]
#         elif rule == "<=0":
#             constraints += [w[j] <= 0]
#         # "free" ä¸åŠ ä»»ä½•çº¦æŸ

#     prob = cp.Problem(cp.Minimize(loss), constraints)

#     # ä¼˜å…ˆ OSQPï¼Œä¸è¡Œå°±é€€å› ECOS/ECOS_BB
#     solvers_by_pref = [cp.OSQP, cp.ECOS, cp.ECOS_BB]
#     last_err = None
#     for solver in solvers_by_pref:
#         try:
#             prob.solve(solver=solver, verbose=True, max_iters=10000)
#             if prob.status in ("optimal", "optimal_inaccurate"):
#                 return w.value, b.value, prob.status
#         except Exception as e:
#             last_err = e
#             continue

#     raise RuntimeError(f"æ±‚è§£å¤±è´¥ï¼ˆå·²å°è¯• OSQP/ECOS/ECOS_BBï¼‰ã€‚æœ€åé”™è¯¯ï¼š{last_err}")


# def main():
#     # 1) é€‰ CSV
#     csv_path = _find_csv_path(CSV_CANDIDATES)
#     print("ğŸ“„ è¯»å– CSV:", csv_path)
#     df = pd.read_csv(csv_path)
#     print("Columns:", list(df.columns))
#     print("Shape:", df.shape)

#     # 2) é€‰ç›®æ ‡åˆ—
#     target_col = _pick_target_col(df, TARGET_COL)
#     print("ğŸ¯ ä½¿ç”¨ç›®æ ‡åˆ—:", target_col)
#     y = df[target_col].astype(float).values  # shape (n,)

#     # 3) ç‰¹å¾é€‰æ‹©ï¼ˆå¸¦åˆ«åï¼‰
#     mapping = _build_feature_mapping(df.columns)
#     # åªå–è¡¨ä¸­çœŸæ­£å­˜åœ¨çš„ç‰¹å¾
#     used_features = [f for f in CANDIDATE_FEATURES if f in mapping]
#     if not used_features:
#         raise ValueError("âŒ æ²¡æœ‰ä»»ä½•å€™é€‰ç‰¹å¾åœ¨è¡¨ä¸­æ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥åˆ—åæˆ– CANDIDATE_FEATURESã€‚")
#     print("ğŸ§© ä½¿ç”¨ç‰¹å¾ï¼ˆè§„èŒƒåï¼‰:", used_features)
#     print("â†³ åˆ—æ˜ å°„ï¼ˆè§„èŒƒåâ†’CSVåˆ—åï¼‰:", {k: mapping[k] for k in used_features})

#     # 4) ä¸¢å¼ƒå« NaN çš„è¡Œ
#     cols_need = [mapping[f] for f in used_features] + [target_col]
#     dff = df[cols_need].dropna().copy()
#     if dff.empty:
#         raise ValueError("âŒ æœ‰æ•ˆè¡Œæ•°ä¸º 0ï¼ˆç‰¹å¾æˆ–ç›®æ ‡å­˜åœ¨ NaNï¼‰ã€‚è¯·æ¸…æ´—æ•°æ®åå†è¯•ã€‚")
#     print("âœ… å¯ç”¨æ ·æœ¬æ•°:", len(dff))

#     # 5) ç»„è£… X/y
#     X = dff[[mapping[f] for f in used_features]].astype(float).values   # (n,d)
#     y_fit = dff[target_col].astype(float).values                        # (n,)

#     # 6) æ±‚è§£å¸¦ç¬¦å·çº¦æŸçš„çº¿æ€§å›å½’
#     print("ğŸš€ å¼€å§‹æ‹Ÿåˆï¼ˆå¸¦ç¬¦å·çº¦æŸï¼‰...")
#     w_val, b_val, status = _solve_with_constraints(
#         X, y_fit, used_features, SIGN_CONSTRAINT, L2_LAMBDA
#     )
#     print("Solver status:", status)
#     print("w:", w_val)
#     print("b:", float(b_val))

#     # 7) é¢„æµ‹ä¸æ®‹å·®ï¼ˆåœ¨åŸè¡¨ç»´åº¦ä¸Šè®¡ç®—ï¼Œå¯¹ NaN åšè·³è¿‡ï¼‰
#     #    å…ˆæŒ‰åŸè¡¨çš„åˆ—é¡ºåºæ„é€  X_allï¼ˆç¼ºå¤±è¡Œä¼šæ˜¯ NaNï¼‰
#     X_all = df[[mapping[f] for f in used_features]].astype(float)
#     pred = np.full(len(df), np.nan, dtype=float)

#     # åªå¯¹å®Œæ•´è¡Œåšé¢„æµ‹
#     mask_ok = ~X_all.isna().any(axis=1)
#     pred[mask_ok.values] = X_all.loc[mask_ok].values @ w_val + float(b_val)

#     # æ®‹å·®ï¼ˆåªåœ¨ y å­˜åœ¨ä¸” pred å·²è®¡ç®—å‡ºæ¥çš„è¡Œæœ‰æ•ˆï¼‰
#     residual = np.full(len(df), np.nan, dtype=float)
#     mask_resid = mask_ok & df[target_col].notna()
#     residual[mask_resid.values] = df.loc[mask_resid, target_col].astype(float).values - pred[mask_resid.values]

#     # å†™å›ä¸¤åˆ—
#     pred_col = f"pred_{target_col}"
#     resid_col = f"residual_{target_col}"
#     df[pred_col] = pred
#     df[resid_col] = residual

#     # 8) ä¿å­˜è¾“å‡º
#     base, ext = os.path.splitext(csv_path)
#     out_csv = base + OUTPUT_SUFFIX
#     df.to_csv(out_csv, index=False, encoding="utf-8-sig")
#     print(f"âœ… å·²å†™å‡ºï¼š{out_csv}")

#     # 9) é¡ºä¾¿æŠŠç³»æ•°ä¹Ÿå­˜ä¸€ä»½ JSON
#     weights = {
#         "intercept": float(b_val),
#         "weights": {feat: float(w_val[i]) for i, feat in enumerate(used_features)},
#         "target": target_col,
#         "features_used": used_features,
#         "sign_constraint": {k: SIGN_CONSTRAINT.get(k, "free") for k in used_features},
#         "l2_lambda": L2_LAMBDA,
#         "status": status,
#     }
#     weights_path = base + WEIGHTS_JSON
#     with open(weights_path, "w", encoding="utf-8") as f:
#         json.dump(weights, f, ensure_ascii=False, indent=2)
#     print(f"ğŸ“ ç³»æ•°å·²ä¿å­˜ï¼š{weights_path}")

#     # 10) æ‰“å°ä¸€ç‰ˆå¯è¯»çš„ç³»æ•°è¡¨
#     print("\n==== Learned Coefficients ====")
#     for k in used_features:
#         print(f"{k:>16s} : {weights['weights'][k]: .6f}")
#     print(f"{'intercept':>16s} : {weights['intercept']: .6f}")
#     print("==============================")

# if __name__ == "__main__":
#     main()


#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Constrained linear regression with sign constraints (cvxpy).
- è‡ªåŠ¨è¯»å– CSVï¼ˆé»˜è®¤ï¼š/mnt/data/map_complexity.csv æˆ– /mnt/data/features_vs_success.csvï¼‰
- è‡ªåŠ¨æ¨æ–­ç›®æ ‡åˆ—ï¼ˆä¼˜å…ˆï¼šsuccess_rate â†’ Complexity â†’ complexity â†’ target â†’ yï¼‰
- è‡ªåŠ¨åŒ¹é…ç‰¹å¾åˆ—ï¼ˆå«åˆ«åï¼šSize/sizeã€Agents/num_agentsã€Density/densityã€Density_actual/density_actual ç­‰ï¼‰
- å¯¹ç³»æ•°æ–½åŠ â€œæœ‰ç¬¦å·çº¦æŸâ€ï¼ˆå¦‚ num_agents<=0ã€FRA/FDA<=0ã€Size>=0 ç­‰ï¼‰
- æ‹ŸåˆæˆåŠŸåï¼ŒæŠŠé¢„æµ‹å€¼ä¸æ®‹å·®ä¸¤åˆ—å†™å›åˆ°æ–° CSVï¼š*_with_pred.csvï¼ˆä¸è¦†ç›–åŸè¡¨ï¼‰
- è¯„ä¼°ï¼šPearson Rã€R^2ã€MAEã€RMSEã€MAPEï¼Œå¹¶ç”Ÿæˆè¯Šæ–­å›¾

éœ€è¦ä¾èµ–ï¼šcvxpyã€numpyã€pandasã€matplotlibã€scikit-learnï¼ˆè‹¥ OSQP ä¸å¯ç”¨ä¼šè‡ªåŠ¨é€€å› ECOS/ECOS_BBï¼‰
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# ----- é…ç½®åŒºåŸŸï¼ˆæŒ‰éœ€ä¿®æ”¹ï¼‰ -----
CSV_CANDIDATES = [
    "C:/Users/MSc_SEIoT_1/MAPF_G2RL-main - train/train_gray3d-Copy-FDA.csv",
]

TARGET_COL = None

CANDIDATE_FEATURES = [
    "size", "num_agents", "density", "density_actual",
    "LDD", "BN", "MC", "DLR", "FRA", "FPA"
]

SIGN_CONSTRAINT = {
    "size":            ">=0",
    "num_agents":      ">=0",
    "density":         ">=0",
    "density_actual":  ">=0",
    "LDD":             ">=0",
    "BN":              ">=0",
    "MC":              ">=0",
    "DLR":             ">=0",
    "FRA":             ">=0",
    "FPA":             ">=0",
}

L2_LAMBDA = 1e-3

OUTPUT_SUFFIX = "_with_pred.csv"
WEIGHTS_JSON   = "_weights.json"
FIG_PRED_ACT   = "_pred_vs_actual.png"
FIG_RESID_HIST = "_residual_hist.png"
FIG_RESID_FIT  = "_residuals_vs_fitted.png"
# --------------------------------


def _find_csv_path(candidates):
    for p in candidates:
        if os.path.exists(p):
            return p
    raise FileNotFoundError(
        f"æ‰¾ä¸åˆ° CSVã€‚è¯·æŠŠä½ çš„ CSV æ”¾åœ¨è¿™äº›è·¯å¾„ä¹‹ä¸€ï¼š\n{candidates}"
    )


def _import_cvxpy():
    try:
        import cvxpy as cp
        return cp
    except Exception as e:
        print("âŒ æœªèƒ½å¯¼å…¥ cvxpyï¼Œè¯·å…ˆå®‰è£…ï¼š pip install cvxpy")
        raise


def _pick_target_col(df: pd.DataFrame, target_pref=None) -> str:
    if target_pref and target_pref in df.columns:
        return target_pref
    for cand in ["success_rate", "Complexity", "complexity", "target", "y"]:
        if cand in df.columns:
            return cand
    raise ValueError(
        "è‡ªåŠ¨é€‰æ‹©ç›®æ ‡åˆ—å¤±è´¥ï¼šæœªæ‰¾åˆ° success_rate/Complexity/complexity/target/y ä¸­ä»»æ„ä¸€ä¸ªã€‚"
        "è¯·æŠŠ TARGET_COL æ”¹æˆè¡¨å†…å­˜åœ¨çš„åˆ—åã€‚"
    )


def _build_feature_mapping(df_cols):
    alias = {
        "size":           ["size", "Size"],
        "num_agents":     ["num_agents", "Agents", "agents", "n_agents"],
        "density":        ["density", "Density"],
        "density_actual": ["density_actual", "Density_actual", "actual_density"],
        "LDD":            ["LDD", "ldd"],
        "BN":             ["BN", "bn", "Bottleneck", "bottleneck"],
        "MC":             ["MC", "mc", "MovingCost", "moving_cost"],
        "DLR":            ["DLR", "dlr"],
        "FRA":            ["FRA", "fra"],
        "FPA":            ["FPA", "fpa"],
    }
    lower_cols = {c.lower(): c for c in df_cols}
    mapping = {}
    for canonical, cands in alias.items():
        hit = None
        for c in cands:
            if c in df_cols:
                hit = c
                break
            if c.lower() in lower_cols:
                hit = lower_cols[c.lower()]
                break
        if hit is not None:
            mapping[canonical] = hit
    return mapping


def _solve_with_constraints(X, y, feature_names, sign_constraint, l2_lambda=0.0):
    cp = _import_cvxpy()

    n, d = X.shape
    w = cp.Variable(d)   # ç³»æ•°
    b = cp.Variable()    # æˆªè·

    resid = X @ w + b - y
    loss = cp.sum_squares(resid)
    if l2_lambda and l2_lambda > 0:
        loss += l2_lambda * cp.sum_squares(w)

    constraints = []
    for j, name in enumerate(feature_names):
        rule = sign_constraint.get(name, "free")
        if rule == ">=0":
            constraints += [w[j] >= 0]
        elif rule == "<=0":
            constraints += [w[j] <= 0]

    prob = cp.Problem(cp.Minimize(loss), constraints)

    solvers_by_pref = [cp.OSQP, cp.ECOS, cp.ECOS_BB]
    last_err = None
    for solver in solvers_by_pref:
        try:
            prob.solve(solver=solver, verbose=False, max_iters=10000)
            if prob.status in ("optimal", "optimal_inaccurate"):
                return w.value, b.value, prob.status
        except Exception as e:
            last_err = e
            continue

    raise RuntimeError(f"æ±‚è§£å¤±è´¥ï¼ˆå·²å°è¯• OSQP/ECOS/ECOS_BBï¼‰ã€‚æœ€åé”™è¯¯ï¼š{last_err}")


def main():
    # 1) é€‰ CSV
    csv_path = _find_csv_path(CSV_CANDIDATES)
    print("ğŸ“„ è¯»å– CSV:", csv_path)
    df = pd.read_csv(csv_path)
    print("Columns:", list(df.columns))
    print("Shape:", df.shape)

    # 2) é€‰ç›®æ ‡åˆ—
    target_col = _pick_target_col(df, TARGET_COL)
    print("ğŸ¯ ä½¿ç”¨ç›®æ ‡åˆ—:", target_col)
    y = df[target_col].astype(float).values  # shape (n,)

    # 3) ç‰¹å¾é€‰æ‹©ï¼ˆå¸¦åˆ«åï¼‰
    mapping = _build_feature_mapping(df.columns)
    used_features = [f for f in CANDIDATE_FEATURES if f in mapping]
    if not used_features:
        raise ValueError("âŒ æ²¡æœ‰ä»»ä½•å€™é€‰ç‰¹å¾åœ¨è¡¨ä¸­æ‰¾åˆ°ï¼Œè¯·æ£€æŸ¥åˆ—åæˆ– CANDIDATE_FEATURESã€‚")
    print("ğŸ§© ä½¿ç”¨ç‰¹å¾ï¼ˆè§„èŒƒåï¼‰:", used_features)
    print("â†³ åˆ—æ˜ å°„ï¼ˆè§„èŒƒåâ†’CSVåˆ—åï¼‰:", {k: mapping[k] for k in used_features})

    # 4) ä¸¢å¼ƒå« NaN çš„è¡Œ
    cols_need = [mapping[f] for f in used_features] + [target_col]
    dff = df[cols_need].dropna().copy()
    if dff.empty:
        raise ValueError("âŒ æœ‰æ•ˆè¡Œæ•°ä¸º 0ï¼ˆç‰¹å¾æˆ–ç›®æ ‡å­˜åœ¨ NaNï¼‰ã€‚è¯·æ¸…æ´—æ•°æ®åå†è¯•ã€‚")
    print("âœ… å¯ç”¨æ ·æœ¬æ•°:", len(dff))

    # 5) ç»„è£… X/y
    X = dff[[mapping[f] for f in used_features]].astype(float).values   # (n,d)
    y_fit = dff[target_col].astype(float).values                        # (n,)

    # 6) æ±‚è§£å¸¦ç¬¦å·çº¦æŸçš„çº¿æ€§å›å½’
    print("ğŸš€ å¼€å§‹æ‹Ÿåˆï¼ˆå¸¦ç¬¦å·çº¦æŸï¼‰...")
    w_val, b_val, status = _solve_with_constraints(
        X, y_fit, used_features, SIGN_CONSTRAINT, L2_LAMBDA
    )
    print("Solver status:", status)
    print("w:", w_val)
    print("b:", float(b_val))

    # 7) é¢„æµ‹ä¸æ®‹å·®ï¼ˆåœ¨åŸè¡¨ç»´åº¦ä¸Šè®¡ç®—ï¼Œå¯¹ NaN åšè·³è¿‡ï¼‰
    X_all = df[[mapping[f] for f in used_features]].astype(float)
    pred = np.full(len(df), np.nan, dtype=float)
    mask_ok = ~X_all.isna().any(axis=1)
    pred[mask_ok.values] = X_all.loc[mask_ok].values @ w_val + float(b_val)

    residual = np.full(len(df), np.nan, dtype=float)
    mask_resid = mask_ok & df[target_col].notna()
    residual[mask_resid.values] = df.loc[mask_resid, target_col].astype(float).values - pred[mask_resid.values]

    pred_col = f"pred_{target_col}"
    resid_col = f"residual_{target_col}"
    df[pred_col] = pred
    df[resid_col] = residual

    # 8) ä¿å­˜è¾“å‡º CSV
    base, ext = os.path.splitext(csv_path)
    out_csv = base + OUTPUT_SUFFIX
    df.to_csv(out_csv, index=False, encoding="utf-8-sig")
    print(f"âœ… å·²å†™å‡ºï¼š{out_csv}")

    # ======== 9) === è¯„ä¼°ä¸å¯è§†åŒ–ï¼ˆåŸºäºâ€œå‚ä¸æ‹Ÿåˆçš„æ ·æœ¬â€ï¼‰ ========
    # ä»…å¯¹ dffï¼ˆè®­ç»ƒå‚ä¸çš„æ— ç¼ºå¤±æ ·æœ¬ï¼‰è®¡ç®—è¯„ä¼°æŒ‡æ ‡å’Œä½œå›¾
    y_pred_fit = X @ w_val + float(b_val)

    # æŒ‡æ ‡
    # Pearson ç›¸å…³ç³»æ•° R
    if len(y_fit) >= 2 and np.std(y_fit) > 0 and np.std(y_pred_fit) > 0:
        R = float(np.corrcoef(y_fit, y_pred_fit)[0, 1])
    else:
        R = float("nan")
    R2 = float(r2_score(y_fit, y_pred_fit))
    MAE = float(mean_absolute_error(y_fit, y_pred_fit))
    RMSE = float(np.sqrt(mean_squared_error(y_fit, y_pred_fit)))
    # ä¸ºé¿å…é™¤é›¶ï¼Œå¯¹æ¥è¿‘ 0 çš„ y è¿›è¡Œä¿æŠ¤
    denom = np.where(np.abs(y_fit) < 1e-12, 1e-12, np.abs(y_fit))
    MAPE = float(np.mean(np.abs((y_fit - y_pred_fit) / denom)) * 100.0)

    print("\n==== Metrics (on training/fit subset) ====")
    print(f"Pearson R : {R: .6f}")
    print(f"R^2       : {R2: .6f}")
    print(f"MAE       : {MAE: .6f}")
    print(f"RMSE      : {RMSE: .6f}")
    print(f"MAPE (%)  : {MAPE: .2f}")
    print("==========================================\n")

    # å›¾ 1ï¼šPredicted vs. Actualï¼ˆå¸¦ y=x å‚è€ƒçº¿ï¼‰
    fig1 = base + FIG_PRED_ACT
    plt.figure()
    plt.scatter(y_fit, y_pred_fit, alpha=0.7)
    minv = float(min(np.min(y_fit), np.min(y_pred_fit)))
    maxv = float(max(np.max(y_fit), np.max(y_pred_fit)))
    plt.plot([minv, maxv], [minv, maxv], linestyle="--")
    plt.xlabel("Actual")
    plt.ylabel("Predicted")
    plt.title(f"Predicted vs. Actual\nR={R:.3f}, RÂ²={R2:.3f}, MAE={MAE:.3f}, RMSE={RMSE:.3f}")
    plt.tight_layout()
    plt.savefig(fig1, dpi=160)
    plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig1}")

    # å›¾ 2ï¼šResidual Histogram
    fig2 = base + FIG_RESID_HIST
    plt.figure()
    plt.hist(y_fit - y_pred_fit, bins=30)
    plt.xlabel("Residual (y - y_pred)")
    plt.ylabel("Count")
    plt.title("Residuals Histogram")
    plt.tight_layout()
    plt.savefig(fig2, dpi=160)
    plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig2}")

    # å›¾ 3ï¼šResiduals vs. Fitted
    fig3 = base + FIG_RESID_FIT
    plt.figure()
    plt.scatter(y_pred_fit, y_fit - y_pred_fit, alpha=0.7)
    plt.axhline(0.0, linestyle="--")
    plt.xlabel("Fitted (Predicted)")
    plt.ylabel("Residual")
    plt.title("Residuals vs. Fitted")
    plt.tight_layout()
    plt.savefig(fig3, dpi=160)
    plt.close()
    print(f"ğŸ–¼ å·²ä¿å­˜å›¾ï¼š{fig3}")

    # 10) ä¿å­˜ç³»æ•° JSONï¼ˆé™„åŠ è¯„ä¼°æŒ‡æ ‡ï¼‰
    weights = {
        "intercept": float(b_val),
        "weights": {feat: float(w_val[i]) for i, feat in enumerate(used_features)},
        "target": target_col,
        "features_used": used_features,
        "sign_constraint": {k: SIGN_CONSTRAINT.get(k, "free") for k in used_features},
        "l2_lambda": L2_LAMBDA,
        "status": status,
        "metrics_on_fit": {
            "pearson_r": R,
            "r2": R2,
            "mae": MAE,
            "rmse": RMSE,
            "mape_percent": MAPE
        },
        "diagnostic_figures": {
            "pred_vs_actual": fig1,
            "residual_hist": fig2,
            "residuals_vs_fitted": fig3
        }
    }
    weights_path = base + WEIGHTS_JSON
    with open(weights_path, "w", encoding="utf-8") as f:
        json.dump(weights, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ ç³»æ•°ä¸æŒ‡æ ‡å·²ä¿å­˜ï¼š{weights_path}")

    # 11) æ‰“å°ä¸€ç‰ˆå¯è¯»çš„ç³»æ•°è¡¨
    print("\n==== Learned Coefficients ====")
    for k in used_features:
        print(f"{k:>16s} : {weights['weights'][k]: .6f}")
    print(f"{'intercept':>16s} : {weights['intercept']: .6f}")
    print("==============================")

if __name__ == "__main__":
    main()

