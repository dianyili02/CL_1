# -*- coding: utf-8 -*-
"""
infer_complexity.py â€” å›ºå®šé…ç½®ç‰ˆï¼ˆæ— éœ€å‘½ä»¤è¡Œå‚æ•°ï¼‰
åŠŸèƒ½ï¼š
- åŠ è½½è®­ç»ƒå¥½çš„ NN æ¨¡å‹ï¼ˆnn_model.ptï¼‰
- è¯»å–åœ°å›¾ç‰¹å¾ CSV
- é¢„æµ‹ success_rateï¼Œå¹¶è®¡ç®— complexity = 1 - success_rateï¼ˆå¯è£å‰ªåˆ°[0,1]ï¼‰
- å†™å›åˆ°æ–°çš„ CSV

ç›´æ¥è¿è¡Œï¼š python infer_complexity.py
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn

# ============ ä½ çš„å›ºå®šé…ç½® ============
MODEL_PATH  = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main-nn/0827/nn_model.pt"
INPUT_CSV   = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main-nn/train_gray3d-Copy-FDA.csv"
OUTPUT_CSV  = r"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main-nn/0827result/maps_features_with_complexity.csv"
CLIP        = True          # æ˜¯å¦è£å‰ªåˆ° [0,1]
ID_COL      = None          # å¦‚æœ‰ map ååˆ—ï¼ˆå¦‚ 'map_name'ï¼‰ï¼Œå¡«åˆ—åï¼›å¦åˆ™ None
DEVICE      = "cuda" if torch.cuda.is_available() else "cpu"
# =====================================

# ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„ MLP ç»“æ„
class MLP(nn.Module):
    def __init__(self, in_dim: int, hidden_dim: int, hidden_layers: int, dropout: float, use_sigmoid_head: bool):
        super().__init__()
        layers = []
        dim = in_dim
        for _ in range(hidden_layers):
            layers += [
                nn.Linear(dim, hidden_dim),
                nn.ReLU(inplace=True),
                nn.Dropout(dropout),
            ]
            dim = hidden_dim
        layers.append(nn.Linear(dim, 1))
        self.backbone = nn.Sequential(*layers)
        self.use_sigmoid = use_sigmoid_head

    def forward(self, x):
        out = self.backbone(x).squeeze(-1)
        if self.use_sigmoid:
            out = torch.sigmoid(out)
        return out

def safe_standardize(X: np.ndarray, means: np.ndarray, stds: np.ndarray) -> np.ndarray:
    stds_safe = np.where(stds == 0, 1.0, stds)
    return (X - means) / stds_safe

def main():
    # 1) åŠ è½½ checkpoint
    if not os.path.exists(MODEL_PATH):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ï¼š{MODEL_PATH}")
    ckpt = torch.load(MODEL_PATH, map_location="cpu")

    in_dim       = ckpt["in_dim"]
    hidden_dim   = ckpt["hidden_dim"]
    hidden_layers= ckpt["hidden_layers"]
    dropout      = ckpt["dropout"]
    use_sigmoid  = ckpt.get("use_sigmoid_head", True)

    feature_columns = ckpt.get("feature_columns", ckpt.get("features_used"))
    if feature_columns is None:
        raise ValueError("æ¨¡å‹ä¸­ç¼ºå°‘ feature_columns / features_usedï¼Œæ— æ³•ç¡®å®šç‰¹å¾åˆ—é¡ºåºã€‚")

    scaler_means = np.array(ckpt["scaler_means"], dtype=float)
    scaler_stds  = np.array(ckpt["scaler_stds"], dtype=float)
    if len(feature_columns) != in_dim or scaler_means.shape[0] != in_dim:
        raise ValueError(f"in_dim({in_dim}) ä¸ç‰¹å¾/æ ‡å‡†åŒ–å™¨ç»´åº¦ä¸ä¸€è‡´ã€‚")

    # 2) è®¾å¤‡ä¸æ¨¡å‹
    device = torch.device(DEVICE)
    print(f"ğŸ–¥ è®¾å¤‡: {device}")

    model = MLP(in_dim=in_dim,
                hidden_dim=hidden_dim,
                hidden_layers=hidden_layers,
                dropout=dropout,
                use_sigmoid_head=use_sigmoid).to(device)
    model.load_state_dict(ckpt["state_dict"])
    model.eval()

    # 3) è¯»å–è¾“å…¥ CSV
    # if not os.path.exists(INPUT_CSV):
    #     raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥ CSVï¼š{INPUT_CSV}")
    # df = pd.read_csv(INPUT_CSV)
    # print(f"ğŸ“„ è¾“å…¥CSV: {INPUT_CSV}  |  å½¢çŠ¶: {df.shape}")

    # missing = [c for c in feature_columns if c not in df.columns]
    # if missing:
    #     raise ValueError(f"è¾“å…¥CSVç¼ºå°‘ä»¥ä¸‹ç‰¹å¾åˆ—ï¼ˆéœ€ä¸è®­ç»ƒä¸€è‡´ï¼‰: {missing}")

    # # 4) é€‰æ‹©å¯æ¨ç†çš„è¡Œï¼ˆç‰¹å¾é NaNï¼‰
    # mask_ok = ~df[feature_columns].isna().any(axis=1)
    # num_ok = int(mask_ok.sum())
    # print(f"âœ… å¯æ¨ç†æ ·æœ¬æ•°: {num_ok} / {len(df)}")

    # X  = df.loc[mask_ok, feature_columns].astype(float).values
    # Xs = safe_standardize(X, scaler_means, scaler_stds)
    # 3) è¯»å–è¾“å…¥ CSV
    if not os.path.exists(INPUT_CSV):
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥ CSVï¼š{INPUT_CSV}")
    df = pd.read_csv(INPUT_CSV)
    print(f"ğŸ“„ è¾“å…¥CSV: {INPUT_CSV}  |  å½¢çŠ¶: {df.shape}")

    missing = [c for c in feature_columns if c not in df.columns]
    if missing:
        raise ValueError(f"è¾“å…¥CSVç¼ºå°‘ä»¥ä¸‹ç‰¹å¾åˆ—ï¼ˆéœ€ä¸è®­ç»ƒä¸€è‡´ï¼‰: {missing}")

# === 3.1 æ¸…æ´—ï¼šæŠŠç‰¹å¾åˆ—é‡Œçš„éæ•°å­—å­—ç¬¦æ¸…æ‰ï¼Œå†è½¬ä¸º float ===
    import re
    def _coerce_numeric_series(s: pd.Series) -> pd.Series:
    # ä¿ç•™æ•°å­—ã€æ­£è´Ÿå·ã€å°æ•°ç‚¹ã€ç§‘å­¦è®¡æ•°æ³• e/Eï¼›å…¶ä½™éƒ½å»æ‰
        cleaned = s.astype(str).str.replace(r"[^0-9eE\+\-\.]", "", regex=True)
    # å°†ç©ºå­—ç¬¦ä¸²æ›¿æ¢ä¸º NaN
        cleaned = cleaned.replace({"": np.nan})
    # å®‰å…¨è½¬æ¢ä¸ºæ•°å€¼ï¼Œéæ³•çš„è®°ä¸º NaN
        return pd.to_numeric(cleaned, errors="coerce")

    before_na = df[feature_columns].isna().sum()
    for col in feature_columns:
        df[col] = _coerce_numeric_series(df[col])

    after_na = df[feature_columns].isna().sum()
    added_na = (after_na - before_na)
    added_na = added_na[added_na > 0]
    if not added_na.empty:
        print("âš ï¸ æ¸…æ´—åè½¬ä¸º NaN çš„æ¡ç›®ï¼ˆè¯´æ˜åŸæœ¬å«æœ‰éæ•°å­—å­—ç¬¦ï¼‰:")
        for c, n in added_na.items():
            print(f"   - {c}: +{int(n)}")

# 4) é€‰æ‹©å¯æ¨ç†çš„è¡Œï¼ˆç‰¹å¾é NaNï¼‰
    mask_ok = ~df[feature_columns].isna().any(axis=1)
    num_ok = int(mask_ok.sum())
    num_all = len(df)
    print(f"âœ… å¯æ¨ç†æ ·æœ¬æ•°: {num_ok} / {num_all}")
    if num_ok == 0:
    # æ‰“å°å‡ è¡Œç¤ºä¾‹ï¼Œå¸®åŠ©ä½ å®šä½é—®é¢˜æ•°æ®
        print("âŒ æ¸…æ´—åæ²¡æœ‰å¯ç”¨æ ·æœ¬ã€‚ç¤ºä¾‹å‰5è¡Œï¼š")
        print(df[feature_columns].head())
        raise ValueError("æ‰€æœ‰ç‰¹å¾è¡Œå­˜åœ¨ NaNï¼Œæ— æ³•æ¨ç†ã€‚è¯·æ£€æŸ¥åŸå§‹æ•°æ®æ ¼å¼ã€‚")

# 5) æ ‡å‡†åŒ–å¹¶æ¨ç†
    X  = df.loc[mask_ok, feature_columns].values.astype(float)
    Xs = safe_standardize(X, scaler_means, scaler_stds)


    # 5) æ¨ç†
    with torch.no_grad():
        x_tensor = torch.tensor(Xs, dtype=torch.float32, device=device)
        pred_sr = model(x_tensor).cpu().numpy().reshape(-1)

    if CLIP or use_sigmoid:
        pred_sr = np.clip(pred_sr, 0.0, 1.0)

    complexity = 1.0 - pred_sr
    if CLIP or use_sigmoid:
        complexity = np.clip(complexity, 0.0, 1.0)

    # 6) å›å¡« & ä¿å­˜
    pred_col = "nn_pred_success_rate"
    comp_col = "nn_complexity"

    df[pred_col] = np.nan
    df.loc[mask_ok, pred_col] = pred_sr
    df[comp_col] = np.nan
    df.loc[mask_ok, comp_col] = complexity

    out_dir = os.path.dirname(OUTPUT_CSV)
    if out_dir and not os.path.exists(out_dir):
        os.makedirs(out_dir, exist_ok=True)

    df.to_csv(OUTPUT_CSV, index=False, encoding="utf-8-sig")
    print(f"ğŸ’¾ å·²ä¿å­˜: {OUTPUT_CSV}")

    # 7) å¯é€‰ï¼šæ‰“å° Top-5ï¼ˆä»…å½“æä¾› ID_COL ä¸”å­˜åœ¨æ—¶ï¼‰
    if ID_COL and ID_COL in df.columns:
        sub = df.loc[mask_ok, [ID_COL, comp_col]].dropna()
        if not sub.empty:
            worst = sub.sort_values(comp_col, ascending=False).head(5)
            best  = sub.sort_values(comp_col, ascending=True).head(5)
            print("\nğŸ´â€â˜ ï¸ Top-5 æœ€å¤æ‚ï¼š")
            for _, r in worst.iterrows():
                print(f"  {r[ID_COL]}  -> complexity={r[comp_col]:.4f}")
            print("\nğŸƒ Top-5 æœ€ç®€å•ï¼š")
            for _, r in best.iterrows():
                print(f"  {r[ID_COL]}  -> complexity={r[comp_col]:.4f}")
    else:
        print("â„¹ï¸ æœªé…ç½® ID_COL æˆ–è¾“å…¥CSVä¸­ä¸å­˜åœ¨è¯¥åˆ—ï¼Œè·³è¿‡ Top-5 æ‰“å°ã€‚")

    print("âœ… æ¨ç†å®Œæˆã€‚")

if __name__ == "__main__":
    main()
