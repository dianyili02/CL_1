{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5efb18e",
   "metadata": {},
   "source": [
    "你的推理过程（我帮你理清一下）\n",
    "\n",
    "目标变量\n",
    "\n",
    "你最终关心的是 地图复杂度 Complexity\n",
    "\n",
    "但 Complexity 不能直接观测（它只是一个潜在概念/指标）\n",
    "\n",
    "我们能观测到的结果是 success_rate（训练表现）\n",
    "\n",
    "核心假设\n",
    "\n",
    "success_rate 越低 → 地图越难（Complexity 越高）\n",
    "\n",
    "success_rate 越高 → 地图越简单（Complexity 越低）\n",
    "\n",
    "所以：Complexity 是 success_rate 的函数。\n",
    "\n",
    "学习流程\n",
    "\n",
    "Step 1. 用 XGBoost 学习 success_rate ~ 特征(size, agents, density, LDD, BN, MC, DLR, ...) 的非线性关系\n",
    "\n",
    "Step 2. 提取特征重要性（说明哪些特征最影响 success_rate → 哪些特征决定复杂度）\n",
    "\n",
    "Step 3. 把 XGBoost 的非线性结果转化成线性权重公式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d150322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 特征重要性 (对 success_rate 的影响)：\n",
      "          feature  importance\n",
      "2         density    0.367306\n",
      "6              MC    0.200028\n",
      "3  density_actual    0.128360\n",
      "4             LDD    0.107434\n",
      "5              BN    0.062293\n",
      "0            size    0.046418\n",
      "7             DLR    0.044426\n",
      "1      num_agents    0.043736\n",
      "\n",
      "📊 线性权重 (可解释公式)：\n",
      "          feature    weight\n",
      "3  density_actual  0.131938\n",
      "2         density  0.077375\n",
      "6              MC  0.039424\n",
      "4             LDD  0.026888\n",
      "0            size  0.021092\n",
      "7             DLR  0.002133\n",
      "1      num_agents -0.010119\n",
      "5              BN -0.127655\n",
      "\n",
      "公式：\n",
      "Complexity = 0.848 + (0.021 * size) + (-0.010 * num_agents) + (0.077 * density) + (0.132 * density_actual) + (0.027 * LDD) + (-0.128 * BN) + (0.039 * MC) + (0.002 * DLR)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 读数据\n",
    "df = pd.read_csv(\"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/logs/train_gray3d-Copy.csv\")\n",
    "\n",
    "# 特征 & 目标\n",
    "features = [\"size\", \"num_agents\", \"density\", \"density_actual\", \"LDD\", \"BN\", \"MC\", \"DLR\"]\n",
    "X = df[features]\n",
    "y = df[\"success_rate\"]\n",
    "\n",
    "# Step 1: 用 XGBoost 学 success_rate\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Step 2: 特征重要性\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"📊 特征重要性 (对 success_rate 的影响)：\")\n",
    "print(importance)\n",
    "\n",
    "# Step 3: 转化成线性权重（拟合可解释公式）\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lr = LinearRegression().fit(X_scaled, y)\n",
    "\n",
    "weights = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"weight\": lr.coef_\n",
    "}).sort_values(\"weight\", ascending=False)\n",
    "\n",
    "print(\"\\n📊 线性权重 (可解释公式)：\")\n",
    "print(weights)\n",
    "\n",
    "# Step 4: Complexity = 1 - success_rate\n",
    "# 你也可以直接用 1 - success_rate 或者把 success_rate 回归结果反转\n",
    "df[\"Complexity\"] = 1 - y\n",
    "\n",
    "print(\"\\n公式：\")\n",
    "print(\"Complexity = {:.3f} + \".format(lr.intercept_) +\n",
    "      \" + \".join(f\"({w:.3f} * {f})\" for f, w in zip(features, lr.coef_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target = success_rate\n",
      "R^2 train = 0.930, R^2 test = 0.713, RMSE test = 0.0557, MAE test = 0.0455\n",
      "\n",
      "=== Complexity 权重（基于 Permutation Importance 归一化） ===\n",
      "          feature  weight_norm\n",
      "2         density     0.952769\n",
      "3  density_actual     0.019045\n",
      "4             LDD     0.011365\n",
      "6              MC     0.010588\n",
      "0            size     0.006232\n",
      "7             DLR     0.000000\n",
      "1      num_agents     0.000000\n",
      "5              BN     0.000000\n",
      "\n",
      "=== 线性可解释“蒸馏”公式（输入为标准化后的 z 分数） ===\n",
      "ŷ = 0.849477 + (+0.030566·size_z) + (-0.005275·num_agents_z) + (+0.076298·density_z) + (+0.179387·density_actual_z) + (+0.030260·LDD_z) + (-0.177219·BN_z) + (+0.042741·MC_z) + (+0.000663·DLR_z)\n",
      "\n",
      "=== 输出文件 ===\n",
      "Feature importance: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\xgb_feature_importance.png\n",
      "Permutation importance: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\permutation_importance.png\n",
      "Pred vs Actual: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pred_vs_actual.png\n",
      "Residuals vs Pred: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\residuals_vs_pred.png\n",
      "Residuals histogram: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\residuals_hist.png\n",
      "PDP (top 3): C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_MC.png, C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_density.png, C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_LDD.png\n",
      "Complexity 权重 CSV: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\complexity_weights_from_PI.csv\n",
      "线性系数 CSV: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\linear_surrogate_coeffs.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# XGBoost 特征学习 + 可视化 + 权重提取 + 线性公式\n",
    "# ================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ---------- 0) 参数 ----------\n",
    "CSV_PATH = \"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/logs/train_gray3d-Copy.csv\"   # 改成你的路径\n",
    "OUTDIR   = \"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics1\"                      # 图片和结果输出目录\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# 使用到的特征与目标\n",
    "FEATURES = [\"size\", \"num_agents\", \"density\", \"density_actual\", \"LDD\", \"BN\", \"MC\", \"DLR\"]\n",
    "TARGET   = \"success_rate\"  # 首选目标\n",
    "\n",
    "# ---------- 1) 读数据 & 清洗 ----------\n",
    "df = pd.read_csv(CSV_PATH, engine=\"python\")\n",
    "\n",
    "# 强制转数值\n",
    "for col in FEATURES + [TARGET]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# 选择目标：优先 success_rate；如果全无效，回退到 Complexity proxy\n",
    "use_success = False\n",
    "if TARGET in df.columns:\n",
    "    y_ok = df[TARGET].notna() & np.isfinite(df[TARGET]) & (df[TARGET] >= 0) & (df[TARGET] <= 1)\n",
    "    if int(y_ok.sum()) > 0:\n",
    "        use_success = True\n",
    "        y = df.loc[y_ok, TARGET]\n",
    "        X = df.loc[y_ok, FEATURES]\n",
    "    else:\n",
    "        y = df[[\"LDD\", \"BN\", \"MC\", \"DLR\"]].mean(axis=1)\n",
    "        X = df[FEATURES]\n",
    "else:\n",
    "    y = df[[\"LDD\", \"BN\", \"MC\", \"DLR\"]].mean(axis=1)\n",
    "    X = df[FEATURES]\n",
    "\n",
    "# 特征用中位数补缺，保证稳定绘图\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# ---------- 2) 划分数据 ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- 3) 训练 XGBoost ----------\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ---------- 4) 评估 ----------\n",
    "y_pred_tr = xgb_model.predict(X_train)\n",
    "y_pred_te = xgb_model.predict(X_test)\n",
    "\n",
    "r2_tr = r2_score(y_train, y_pred_tr)\n",
    "r2_te = r2_score(y_test, y_pred_te)\n",
    "rmse_te = mean_squared_error(y_test, y_pred_te, squared=False)\n",
    "mae_te = mean_absolute_error(y_test, y_pred_te)\n",
    "\n",
    "print(f\"Target = {'success_rate' if use_success else 'Complexity proxy (mean of LDD/BN/MC/DLR)'}\")\n",
    "print(f\"R^2 train = {r2_tr:.3f}, R^2 test = {r2_te:.3f}, RMSE test = {rmse_te:.4f}, MAE test = {mae_te:.4f}\")\n",
    "\n",
    "# ---------- 5) 内置特征重要性 ----------\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.barh(imp_df[\"feature\"], imp_df[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"XGBoost Feature Importance\" + (\" (success_rate)\" if use_success else \" (Complexity proxy)\"))\n",
    "plt.tight_layout()\n",
    "fi_path = os.path.join(OUTDIR, \"xgb_feature_importance.png\")\n",
    "plt.savefig(fi_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) Permutation Importance（测试集，更稳健） ----------\n",
    "pi = permutation_importance(\n",
    "    xgb_model, X_test, y_test, n_repeats=15, random_state=42, n_jobs=1\n",
    ")\n",
    "pi_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES,\n",
    "    \"perm_importance\": pi.importances_mean,\n",
    "    \"perm_std\": pi.importances_std\n",
    "}).sort_values(\"perm_importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.barh(pi_df[\"feature\"], pi_df[\"perm_importance\"])\n",
    "plt.xlabel(\"Permutation Importance (mean)\")\n",
    "plt.title(\"Permutation Importance on Test Set\")\n",
    "plt.tight_layout()\n",
    "pi_path = os.path.join(OUTDIR, \"permutation_importance.png\")\n",
    "plt.savefig(pi_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 7) 预测 vs 实际 ----------\n",
    "xy_min = float(min(y_test.min(), y_pred_te.min()))\n",
    "xy_max = float(max(y_test.max(), y_pred_te.max()))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_te, s=16)\n",
    "plt.plot([xy_min, xy_max], [xy_min, xy_max])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "ttl = \"Predicted vs Actual (XGBoost)\"\n",
    "ttl += f\"\\nR²={r2_te:.3f}, RMSE={rmse_te:.3f}, MAE={mae_te:.3f}\"\n",
    "plt.title(ttl)\n",
    "plt.tight_layout()\n",
    "pva_path = os.path.join(OUTDIR, \"pred_vs_actual.png\")\n",
    "plt.savefig(pva_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 8) 残差图 ----------\n",
    "residuals = y_test - y_pred_te\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(y_pred_te, residuals, s=12)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual (Actual - Pred)\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.tight_layout()\n",
    "rvp_path = os.path.join(OUTDIR, \"residuals_vs_pred.png\")\n",
    "plt.savefig(rvp_path)\n",
    "plt.close()\n",
    "\n",
    "# 残差直方图\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.tight_layout()\n",
    "rhist_path = os.path.join(OUTDIR, \"residuals_hist.png\")\n",
    "plt.savefig(rhist_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 9) 前 3 特征的 PDP ----------\n",
    "top3 = list(imp_df.sort_values(\"importance\", ascending=False)[\"feature\"][:3])\n",
    "baseline = X_train.median(numeric_only=True)\n",
    "pdp_paths = []\n",
    "\n",
    "for f in top3:\n",
    "    f_min, f_max = X_train[f].min(), X_train[f].max()\n",
    "    grid = np.linspace(f_min, f_max, 70)\n",
    "\n",
    "    # 基线为各特征中位数，仅改变一个特征 f\n",
    "    X_grid = np.repeat(baseline.values.reshape(1, -1), len(grid), axis=0)\n",
    "    X_grid = pd.DataFrame(X_grid, columns=FEATURES)\n",
    "    X_grid[f] = grid\n",
    "\n",
    "    preds = xgb_model.predict(X_grid)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(grid, preds)\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(\"Predicted \" + (\"success_rate\" if use_success else \"Complexity proxy\"))\n",
    "    plt.title(\"Partial Dependence: \" + f)\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(OUTDIR, f\"pdp_{f}.png\")\n",
    "    plt.savefig(outp)\n",
    "    plt.close()\n",
    "    pdp_paths.append(outp)\n",
    "\n",
    "# ---------- 10) 基于 Permutation Importance 的“Complexity 权重” ----------\n",
    "# 若有负值（噪声造成的），裁剪为0后再归一化\n",
    "pi_pos = pi_df.copy()\n",
    "pi_pos[\"perm_importance_clipped\"] = pi_pos[\"perm_importance\"].clip(lower=0)\n",
    "total = pi_pos[\"perm_importance_clipped\"].sum()\n",
    "if total == 0:\n",
    "    # 如果全是0，退化为等权\n",
    "    pi_pos[\"weight_norm\"] = 1.0 / len(pi_pos)\n",
    "else:\n",
    "    pi_pos[\"weight_norm\"] = pi_pos[\"perm_importance_clipped\"] / total\n",
    "\n",
    "# 为了直观展示“对复杂度的贡献”，我们通常让“成功率的负相关特征”获得正的复杂度权重。\n",
    "# 这里简单做一个符号翻转：如果 XGBoost 的 SHAP 或方向没有估，先给出无方向的正权重（用在加权综合上）。\n",
    "# 你也可以后续用 SHAP 判断方向，再给权重加 +/- 号。\n",
    "weights_csv = os.path.join(OUTDIR, \"complexity_weights_from_PI.csv\")\n",
    "pi_pos[[\"feature\", \"weight_norm\"]].sort_values(\"weight_norm\", ascending=False).to_csv(weights_csv, index=False)\n",
    "\n",
    "print(\"\\n=== Complexity 权重（基于 Permutation Importance 归一化） ===\")\n",
    "print(pi_pos[[\"feature\", \"weight_norm\"]].sort_values(\"weight_norm\", ascending=False))\n",
    "\n",
    "# ---------- 11) 线性可解释公式（模型蒸馏） ----------\n",
    "# 用 XGBoost 的预测作为 teacher target，对标准化特征做线性回归，得到可解释公式\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_train)\n",
    "X_te_scaled = scaler.transform(X_test)\n",
    "\n",
    "teacher_tr = xgb_model.predict(X_train)\n",
    "teacher_te = xgb_model.predict(X_test)\n",
    "\n",
    "lin = LinearRegression().fit(X_tr_scaled, teacher_tr)\n",
    "coef = lin.coef_\n",
    "intercept = lin.intercept_\n",
    "\n",
    "formula = \"ŷ = {:.6f} + \".format(intercept) + \" + \".join(\n",
    "    f\"({w:+.6f}·{name}_z)\" for w, name in zip(coef, FEATURES)\n",
    ")\n",
    "\n",
    "print(\"\\n=== 线性可解释“蒸馏”公式（输入为标准化后的 z 分数） ===\")\n",
    "print(formula)\n",
    "\n",
    "# 保存线性系数\n",
    "coef_df = pd.DataFrame({\"feature\": FEATURES, \"coef_on_z\": coef})\n",
    "coef_path = os.path.join(OUTDIR, \"linear_surrogate_coeffs.csv\")\n",
    "coef_df.sort_values(\"coef_on_z\", ascending=False).to_csv(coef_path, index=False)\n",
    "\n",
    "# ---------- 12) 路径汇总 ----------\n",
    "print(\"\\n=== 输出文件 ===\")\n",
    "print(\"Feature importance:\", fi_path)\n",
    "print(\"Permutation importance:\", pi_path)\n",
    "print(\"Pred vs Actual:\", pva_path)\n",
    "print(\"Residuals vs Pred:\", rvp_path)\n",
    "print(\"Residuals histogram:\", rhist_path)\n",
    "print(\"PDP (top 3):\", \", \".join(pdp_paths))\n",
    "print(\"Complexity 权重 CSV:\", weights_csv)\n",
    "print(\"线性系数 CSV:\", coef_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
