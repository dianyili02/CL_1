{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5efb18e",
   "metadata": {},
   "source": [
    "ä½ çš„æ¨ç†è¿‡ç¨‹ï¼ˆæˆ‘å¸®ä½ ç†æ¸…ä¸€ä¸‹ï¼‰\n",
    "\n",
    "ç›®æ ‡å˜é‡\n",
    "\n",
    "ä½ æœ€ç»ˆå…³å¿ƒçš„æ˜¯ åœ°å›¾å¤æ‚åº¦ Complexity\n",
    "\n",
    "ä½† Complexity ä¸èƒ½ç›´æ¥è§‚æµ‹ï¼ˆå®ƒåªæ˜¯ä¸€ä¸ªæ½œåœ¨æ¦‚å¿µ/æŒ‡æ ‡ï¼‰\n",
    "\n",
    "æˆ‘ä»¬èƒ½è§‚æµ‹åˆ°çš„ç»“æœæ˜¯ success_rateï¼ˆè®­ç»ƒè¡¨ç°ï¼‰\n",
    "\n",
    "æ ¸å¿ƒå‡è®¾\n",
    "\n",
    "success_rate è¶Šä½ â†’ åœ°å›¾è¶Šéš¾ï¼ˆComplexity è¶Šé«˜ï¼‰\n",
    "\n",
    "success_rate è¶Šé«˜ â†’ åœ°å›¾è¶Šç®€å•ï¼ˆComplexity è¶Šä½ï¼‰\n",
    "\n",
    "æ‰€ä»¥ï¼šComplexity æ˜¯ success_rate çš„å‡½æ•°ã€‚\n",
    "\n",
    "å­¦ä¹ æµç¨‹\n",
    "\n",
    "Step 1. ç”¨ XGBoost å­¦ä¹  success_rate ~ ç‰¹å¾(size, agents, density, LDD, BN, MC, DLR, ...) çš„éçº¿æ€§å…³ç³»\n",
    "\n",
    "Step 2. æå–ç‰¹å¾é‡è¦æ€§ï¼ˆè¯´æ˜å“ªäº›ç‰¹å¾æœ€å½±å“ success_rate â†’ å“ªäº›ç‰¹å¾å†³å®šå¤æ‚åº¦ï¼‰\n",
    "\n",
    "Step 3. æŠŠ XGBoost çš„éçº¿æ€§ç»“æœè½¬åŒ–æˆçº¿æ€§æƒé‡å…¬å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d150322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ç‰¹å¾é‡è¦æ€§ (å¯¹ success_rate çš„å½±å“)ï¼š\n",
      "          feature  importance\n",
      "2         density    0.367306\n",
      "6              MC    0.200028\n",
      "3  density_actual    0.128360\n",
      "4             LDD    0.107434\n",
      "5              BN    0.062293\n",
      "0            size    0.046418\n",
      "7             DLR    0.044426\n",
      "1      num_agents    0.043736\n",
      "\n",
      "ğŸ“Š çº¿æ€§æƒé‡ (å¯è§£é‡Šå…¬å¼)ï¼š\n",
      "          feature    weight\n",
      "3  density_actual  0.131938\n",
      "2         density  0.077375\n",
      "6              MC  0.039424\n",
      "4             LDD  0.026888\n",
      "0            size  0.021092\n",
      "7             DLR  0.002133\n",
      "1      num_agents -0.010119\n",
      "5              BN -0.127655\n",
      "\n",
      "å…¬å¼ï¼š\n",
      "Complexity = 0.848 + (0.021 * size) + (-0.010 * num_agents) + (0.077 * density) + (0.132 * density_actual) + (0.027 * LDD) + (-0.128 * BN) + (0.039 * MC) + (0.002 * DLR)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# è¯»æ•°æ®\n",
    "df = pd.read_csv(\"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/logs/train_gray3d-Copy.csv\")\n",
    "\n",
    "# ç‰¹å¾ & ç›®æ ‡\n",
    "features = [\"size\", \"num_agents\", \"density\", \"density_actual\", \"LDD\", \"BN\", \"MC\", \"DLR\"]\n",
    "X = df[features]\n",
    "y = df[\"success_rate\"]\n",
    "\n",
    "# Step 1: ç”¨ XGBoost å­¦ success_rate\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X, y)\n",
    "\n",
    "# Step 2: ç‰¹å¾é‡è¦æ€§\n",
    "importance = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=False)\n",
    "\n",
    "print(\"ğŸ“Š ç‰¹å¾é‡è¦æ€§ (å¯¹ success_rate çš„å½±å“)ï¼š\")\n",
    "print(importance)\n",
    "\n",
    "# Step 3: è½¬åŒ–æˆçº¿æ€§æƒé‡ï¼ˆæ‹Ÿåˆå¯è§£é‡Šå…¬å¼ï¼‰\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "lr = LinearRegression().fit(X_scaled, y)\n",
    "\n",
    "weights = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"weight\": lr.coef_\n",
    "}).sort_values(\"weight\", ascending=False)\n",
    "\n",
    "print(\"\\nğŸ“Š çº¿æ€§æƒé‡ (å¯è§£é‡Šå…¬å¼)ï¼š\")\n",
    "print(weights)\n",
    "\n",
    "# Step 4: Complexity = 1 - success_rate\n",
    "# ä½ ä¹Ÿå¯ä»¥ç›´æ¥ç”¨ 1 - success_rate æˆ–è€…æŠŠ success_rate å›å½’ç»“æœåè½¬\n",
    "df[\"Complexity\"] = 1 - y\n",
    "\n",
    "print(\"\\nå…¬å¼ï¼š\")\n",
    "print(\"Complexity = {:.3f} + \".format(lr.intercept_) +\n",
    "      \" + \".join(f\"({w:.3f} * {f})\" for f, w in zip(features, lr.coef_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde4de7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target = success_rate\n",
      "R^2 train = 0.930, R^2 test = 0.713, RMSE test = 0.0557, MAE test = 0.0455\n",
      "\n",
      "=== Complexity æƒé‡ï¼ˆåŸºäº Permutation Importance å½’ä¸€åŒ–ï¼‰ ===\n",
      "          feature  weight_norm\n",
      "2         density     0.952769\n",
      "3  density_actual     0.019045\n",
      "4             LDD     0.011365\n",
      "6              MC     0.010588\n",
      "0            size     0.006232\n",
      "7             DLR     0.000000\n",
      "1      num_agents     0.000000\n",
      "5              BN     0.000000\n",
      "\n",
      "=== çº¿æ€§å¯è§£é‡Šâ€œè’¸é¦â€å…¬å¼ï¼ˆè¾“å…¥ä¸ºæ ‡å‡†åŒ–åçš„ z åˆ†æ•°ï¼‰ ===\n",
      "Å· = 0.849477 + (+0.030566Â·size_z) + (-0.005275Â·num_agents_z) + (+0.076298Â·density_z) + (+0.179387Â·density_actual_z) + (+0.030260Â·LDD_z) + (-0.177219Â·BN_z) + (+0.042741Â·MC_z) + (+0.000663Â·DLR_z)\n",
      "\n",
      "=== è¾“å‡ºæ–‡ä»¶ ===\n",
      "Feature importance: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\xgb_feature_importance.png\n",
      "Permutation importance: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\permutation_importance.png\n",
      "Pred vs Actual: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pred_vs_actual.png\n",
      "Residuals vs Pred: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\residuals_vs_pred.png\n",
      "Residuals histogram: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\residuals_hist.png\n",
      "PDP (top 3): C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_MC.png, C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_density.png, C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\pdp_LDD.png\n",
      "Complexity æƒé‡ CSV: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\complexity_weights_from_PI.csv\n",
      "çº¿æ€§ç³»æ•° CSV: C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics\\linear_surrogate_coeffs.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================\n",
    "# XGBoost ç‰¹å¾å­¦ä¹  + å¯è§†åŒ– + æƒé‡æå– + çº¿æ€§å…¬å¼\n",
    "# ================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# ---------- 0) å‚æ•° ----------\n",
    "CSV_PATH = \"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/logs/train_gray3d-Copy.csv\"   # æ”¹æˆä½ çš„è·¯å¾„\n",
    "OUTDIR   = \"C:/Users/MSc_SEIoT_1/MAPF_G2RL-main/pics1\"                      # å›¾ç‰‡å’Œç»“æœè¾“å‡ºç›®å½•\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "# ä½¿ç”¨åˆ°çš„ç‰¹å¾ä¸ç›®æ ‡\n",
    "FEATURES = [\"size\", \"num_agents\", \"density\", \"density_actual\", \"LDD\", \"BN\", \"MC\", \"DLR\"]\n",
    "TARGET   = \"success_rate\"  # é¦–é€‰ç›®æ ‡\n",
    "\n",
    "# ---------- 1) è¯»æ•°æ® & æ¸…æ´— ----------\n",
    "df = pd.read_csv(CSV_PATH, engine=\"python\")\n",
    "\n",
    "# å¼ºåˆ¶è½¬æ•°å€¼\n",
    "for col in FEATURES + [TARGET]:\n",
    "    if col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# é€‰æ‹©ç›®æ ‡ï¼šä¼˜å…ˆ success_rateï¼›å¦‚æœå…¨æ— æ•ˆï¼Œå›é€€åˆ° Complexity proxy\n",
    "use_success = False\n",
    "if TARGET in df.columns:\n",
    "    y_ok = df[TARGET].notna() & np.isfinite(df[TARGET]) & (df[TARGET] >= 0) & (df[TARGET] <= 1)\n",
    "    if int(y_ok.sum()) > 0:\n",
    "        use_success = True\n",
    "        y = df.loc[y_ok, TARGET]\n",
    "        X = df.loc[y_ok, FEATURES]\n",
    "    else:\n",
    "        y = df[[\"LDD\", \"BN\", \"MC\", \"DLR\"]].mean(axis=1)\n",
    "        X = df[FEATURES]\n",
    "else:\n",
    "    y = df[[\"LDD\", \"BN\", \"MC\", \"DLR\"]].mean(axis=1)\n",
    "    X = df[FEATURES]\n",
    "\n",
    "# ç‰¹å¾ç”¨ä¸­ä½æ•°è¡¥ç¼ºï¼Œä¿è¯ç¨³å®šç»˜å›¾\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "# ---------- 2) åˆ’åˆ†æ•°æ® ----------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ---------- 3) è®­ç»ƒ XGBoost ----------\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=4,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    tree_method=\"hist\",\n",
    "    random_state=42\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# ---------- 4) è¯„ä¼° ----------\n",
    "y_pred_tr = xgb_model.predict(X_train)\n",
    "y_pred_te = xgb_model.predict(X_test)\n",
    "\n",
    "r2_tr = r2_score(y_train, y_pred_tr)\n",
    "r2_te = r2_score(y_test, y_pred_te)\n",
    "rmse_te = mean_squared_error(y_test, y_pred_te, squared=False)\n",
    "mae_te = mean_absolute_error(y_test, y_pred_te)\n",
    "\n",
    "print(f\"Target = {'success_rate' if use_success else 'Complexity proxy (mean of LDD/BN/MC/DLR)'}\")\n",
    "print(f\"R^2 train = {r2_tr:.3f}, R^2 test = {r2_te:.3f}, RMSE test = {rmse_te:.4f}, MAE test = {mae_te:.4f}\")\n",
    "\n",
    "# ---------- 5) å†…ç½®ç‰¹å¾é‡è¦æ€§ ----------\n",
    "imp_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES,\n",
    "    \"importance\": xgb_model.feature_importances_\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.barh(imp_df[\"feature\"], imp_df[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"XGBoost Feature Importance\" + (\" (success_rate)\" if use_success else \" (Complexity proxy)\"))\n",
    "plt.tight_layout()\n",
    "fi_path = os.path.join(OUTDIR, \"xgb_feature_importance.png\")\n",
    "plt.savefig(fi_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 6) Permutation Importanceï¼ˆæµ‹è¯•é›†ï¼Œæ›´ç¨³å¥ï¼‰ ----------\n",
    "pi = permutation_importance(\n",
    "    xgb_model, X_test, y_test, n_repeats=15, random_state=42, n_jobs=1\n",
    ")\n",
    "pi_df = pd.DataFrame({\n",
    "    \"feature\": FEATURES,\n",
    "    \"perm_importance\": pi.importances_mean,\n",
    "    \"perm_std\": pi.importances_std\n",
    "}).sort_values(\"perm_importance\", ascending=True)\n",
    "\n",
    "plt.figure(figsize=(7, 4.5))\n",
    "plt.barh(pi_df[\"feature\"], pi_df[\"perm_importance\"])\n",
    "plt.xlabel(\"Permutation Importance (mean)\")\n",
    "plt.title(\"Permutation Importance on Test Set\")\n",
    "plt.tight_layout()\n",
    "pi_path = os.path.join(OUTDIR, \"permutation_importance.png\")\n",
    "plt.savefig(pi_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 7) é¢„æµ‹ vs å®é™… ----------\n",
    "xy_min = float(min(y_test.min(), y_pred_te.min()))\n",
    "xy_max = float(max(y_test.max(), y_pred_te.max()))\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(y_test, y_pred_te, s=16)\n",
    "plt.plot([xy_min, xy_max], [xy_min, xy_max])\n",
    "plt.xlabel(\"Actual\")\n",
    "plt.ylabel(\"Predicted\")\n",
    "ttl = \"Predicted vs Actual (XGBoost)\"\n",
    "ttl += f\"\\nRÂ²={r2_te:.3f}, RMSE={rmse_te:.3f}, MAE={mae_te:.3f}\"\n",
    "plt.title(ttl)\n",
    "plt.tight_layout()\n",
    "pva_path = os.path.join(OUTDIR, \"pred_vs_actual.png\")\n",
    "plt.savefig(pva_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 8) æ®‹å·®å›¾ ----------\n",
    "residuals = y_test - y_pred_te\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(y_pred_te, residuals, s=12)\n",
    "plt.axhline(0)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Residual (Actual - Pred)\")\n",
    "plt.title(\"Residuals vs Predicted\")\n",
    "plt.tight_layout()\n",
    "rvp_path = os.path.join(OUTDIR, \"residuals_vs_pred.png\")\n",
    "plt.savefig(rvp_path)\n",
    "plt.close()\n",
    "\n",
    "# æ®‹å·®ç›´æ–¹å›¾\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.hist(residuals, bins=20)\n",
    "plt.xlabel(\"Residual\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residuals Histogram\")\n",
    "plt.tight_layout()\n",
    "rhist_path = os.path.join(OUTDIR, \"residuals_hist.png\")\n",
    "plt.savefig(rhist_path)\n",
    "plt.close()\n",
    "\n",
    "# ---------- 9) å‰ 3 ç‰¹å¾çš„ PDP ----------\n",
    "top3 = list(imp_df.sort_values(\"importance\", ascending=False)[\"feature\"][:3])\n",
    "baseline = X_train.median(numeric_only=True)\n",
    "pdp_paths = []\n",
    "\n",
    "for f in top3:\n",
    "    f_min, f_max = X_train[f].min(), X_train[f].max()\n",
    "    grid = np.linspace(f_min, f_max, 70)\n",
    "\n",
    "    # åŸºçº¿ä¸ºå„ç‰¹å¾ä¸­ä½æ•°ï¼Œä»…æ”¹å˜ä¸€ä¸ªç‰¹å¾ f\n",
    "    X_grid = np.repeat(baseline.values.reshape(1, -1), len(grid), axis=0)\n",
    "    X_grid = pd.DataFrame(X_grid, columns=FEATURES)\n",
    "    X_grid[f] = grid\n",
    "\n",
    "    preds = xgb_model.predict(X_grid)\n",
    "\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    plt.plot(grid, preds)\n",
    "    plt.xlabel(f)\n",
    "    plt.ylabel(\"Predicted \" + (\"success_rate\" if use_success else \"Complexity proxy\"))\n",
    "    plt.title(\"Partial Dependence: \" + f)\n",
    "    plt.tight_layout()\n",
    "    outp = os.path.join(OUTDIR, f\"pdp_{f}.png\")\n",
    "    plt.savefig(outp)\n",
    "    plt.close()\n",
    "    pdp_paths.append(outp)\n",
    "\n",
    "# ---------- 10) åŸºäº Permutation Importance çš„â€œComplexity æƒé‡â€ ----------\n",
    "# è‹¥æœ‰è´Ÿå€¼ï¼ˆå™ªå£°é€ æˆçš„ï¼‰ï¼Œè£å‰ªä¸º0åå†å½’ä¸€åŒ–\n",
    "pi_pos = pi_df.copy()\n",
    "pi_pos[\"perm_importance_clipped\"] = pi_pos[\"perm_importance\"].clip(lower=0)\n",
    "total = pi_pos[\"perm_importance_clipped\"].sum()\n",
    "if total == 0:\n",
    "    # å¦‚æœå…¨æ˜¯0ï¼Œé€€åŒ–ä¸ºç­‰æƒ\n",
    "    pi_pos[\"weight_norm\"] = 1.0 / len(pi_pos)\n",
    "else:\n",
    "    pi_pos[\"weight_norm\"] = pi_pos[\"perm_importance_clipped\"] / total\n",
    "\n",
    "# ä¸ºäº†ç›´è§‚å±•ç¤ºâ€œå¯¹å¤æ‚åº¦çš„è´¡çŒ®â€ï¼Œæˆ‘ä»¬é€šå¸¸è®©â€œæˆåŠŸç‡çš„è´Ÿç›¸å…³ç‰¹å¾â€è·å¾—æ­£çš„å¤æ‚åº¦æƒé‡ã€‚\n",
    "# è¿™é‡Œç®€å•åšä¸€ä¸ªç¬¦å·ç¿»è½¬ï¼šå¦‚æœ XGBoost çš„ SHAP æˆ–æ–¹å‘æ²¡æœ‰ä¼°ï¼Œå…ˆç»™å‡ºæ— æ–¹å‘çš„æ­£æƒé‡ï¼ˆç”¨åœ¨åŠ æƒç»¼åˆä¸Šï¼‰ã€‚\n",
    "# ä½ ä¹Ÿå¯ä»¥åç»­ç”¨ SHAP åˆ¤æ–­æ–¹å‘ï¼Œå†ç»™æƒé‡åŠ  +/- å·ã€‚\n",
    "weights_csv = os.path.join(OUTDIR, \"complexity_weights_from_PI.csv\")\n",
    "pi_pos[[\"feature\", \"weight_norm\"]].sort_values(\"weight_norm\", ascending=False).to_csv(weights_csv, index=False)\n",
    "\n",
    "print(\"\\n=== Complexity æƒé‡ï¼ˆåŸºäº Permutation Importance å½’ä¸€åŒ–ï¼‰ ===\")\n",
    "print(pi_pos[[\"feature\", \"weight_norm\"]].sort_values(\"weight_norm\", ascending=False))\n",
    "\n",
    "# ---------- 11) çº¿æ€§å¯è§£é‡Šå…¬å¼ï¼ˆæ¨¡å‹è’¸é¦ï¼‰ ----------\n",
    "# ç”¨ XGBoost çš„é¢„æµ‹ä½œä¸º teacher targetï¼Œå¯¹æ ‡å‡†åŒ–ç‰¹å¾åšçº¿æ€§å›å½’ï¼Œå¾—åˆ°å¯è§£é‡Šå…¬å¼\n",
    "scaler = StandardScaler()\n",
    "X_tr_scaled = scaler.fit_transform(X_train)\n",
    "X_te_scaled = scaler.transform(X_test)\n",
    "\n",
    "teacher_tr = xgb_model.predict(X_train)\n",
    "teacher_te = xgb_model.predict(X_test)\n",
    "\n",
    "lin = LinearRegression().fit(X_tr_scaled, teacher_tr)\n",
    "coef = lin.coef_\n",
    "intercept = lin.intercept_\n",
    "\n",
    "formula = \"Å· = {:.6f} + \".format(intercept) + \" + \".join(\n",
    "    f\"({w:+.6f}Â·{name}_z)\" for w, name in zip(coef, FEATURES)\n",
    ")\n",
    "\n",
    "print(\"\\n=== çº¿æ€§å¯è§£é‡Šâ€œè’¸é¦â€å…¬å¼ï¼ˆè¾“å…¥ä¸ºæ ‡å‡†åŒ–åçš„ z åˆ†æ•°ï¼‰ ===\")\n",
    "print(formula)\n",
    "\n",
    "# ä¿å­˜çº¿æ€§ç³»æ•°\n",
    "coef_df = pd.DataFrame({\"feature\": FEATURES, \"coef_on_z\": coef})\n",
    "coef_path = os.path.join(OUTDIR, \"linear_surrogate_coeffs.csv\")\n",
    "coef_df.sort_values(\"coef_on_z\", ascending=False).to_csv(coef_path, index=False)\n",
    "\n",
    "# ---------- 12) è·¯å¾„æ±‡æ€» ----------\n",
    "print(\"\\n=== è¾“å‡ºæ–‡ä»¶ ===\")\n",
    "print(\"Feature importance:\", fi_path)\n",
    "print(\"Permutation importance:\", pi_path)\n",
    "print(\"Pred vs Actual:\", pva_path)\n",
    "print(\"Residuals vs Pred:\", rvp_path)\n",
    "print(\"Residuals histogram:\", rhist_path)\n",
    "print(\"PDP (top 3):\", \", \".join(pdp_paths))\n",
    "print(\"Complexity æƒé‡ CSV:\", weights_csv)\n",
    "print(\"çº¿æ€§ç³»æ•° CSV:\", coef_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
